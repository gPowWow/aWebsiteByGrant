[
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "For part 1 of this assignment, an example of Anscombe’s data from 1973 was run through an executable cell, which turned it into scatter plots with a regression line demonstrating the “Line of Best Fit” for all the data.\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nsapply(mods, coef)  # Note the use of this function\nlapply(mods, function(fm) coef(summary(fm)))\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\npar(op)\n\nFor part 2 of this assignment, examples of “generative art” were found through a Google search and these can be found through the links posted on my homepage of this website.\nFor part 3 of this assignment, an example of a spring leaf graphic was generated after running this Fall.R script in an executable cell where the color of the leaf was changed to violet.\n# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\ninstall.packages(\"gsubfn\")\ninstall.packages(\"tidyverse\")\nlibrary(gsubfn)\nlibrary(tidyverse)\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"violet\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\n\n\n\n\n\nFor part 4 of this assignment, my critique of a chart from a published work can be found by visiting my blog page on this website."
  },
  {
    "objectID": "posts/2023-9-11 Chart Critique/index.html",
    "href": "posts/2023-9-11 Chart Critique/index.html",
    "title": "Critique of a Chart",
    "section": "",
    "text": "Up above, is a graph I found from Verizon’s 2023 Data Breach Investigative Report, which can be accessed at: https://www.verizon.com/business/resources/reports/dbir/2023/introduction/ It’s a graph that shows which types of data breaches have become more or less common over a span of six years from 2017-2023. The types of data breaches listed and categorized in the graph are “denial of service,” “privilege misuse,” “system intrusion,” “basic web application attacks,” “social engineering,” “lost and stolen assets,” “miscellaneous errors,” and “everything else.” There are some pros and cons I have discovered about this graph:\nPros\n\nIt looks at the fluctuations in the amount of a particular data breach over a span of months per year based on monthly averages instead of over a span of days or weeks in order to fit all the data into the small space that the graph covers.\nIt ensures that the viewer and reader of this graph is able to distinguish which line refers to a specific data breach category by matching them based on color.\n\nCons\n\nUsing colors is not necessarily the best way to separate the lines from each other and match a specific data breach category with a specific line. It cannot be assumed that everybody has normal color vision. There are too many cool colors, especially green, blue, and different hues of purple that make it hard for the eye to focus and differentiate the lines from each other. On the light spectrum, the human eye can see the colors green pretty well and the color red fairly well, but not the color blue. We have fewer cones in our eyes that pick up the color blue and, as a result, have to focus more on texts and details that are in the color blue or blended in with a blue background.\nThe text of the data breach category “basic web application attacks” is in a bright yellow color that can make it hard to see and read.\n\nThe best solution for presenting this graph is to either incorporate some more warmer and brighter colors to provide more contrast and differentiation between the colors to help the viewer and reader distinguish which line belongs to which data breach category. Or, create different types of lines such as dotted and broken lines, for example, which I recommend, instead of colored lines."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Grant Powell’s Portfolio",
    "section": "",
    "text": "You have stumbled upon Grant Powell’s portfolio website. It features his work from courses he completed at the University of Texas at Dallas through the human computer interaction specialization track of the Applied Cognition and Neuroscience Masters degree program. The courses in which he completed projects and assignments to deepen and showcase his understanding are from:\n\nCognitive Psychology Essentials for Cybersecurity\nData Visualization\nHuman Computer Interaction I, II, & III\nPerception\nSpeech Perception\nUser Experience Design"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Grant Powell is a graduate of the Applied Cognition and Neuroscience Masters program at the University of Texas at Dallas where he pursued the human computer interaction specialization track. He is also a board-certified music therapist who strives to improve the functional skills and quality of life of clients with autism, hearing and visual impairments, physical disabilities, and intellectual and developmental disabilities. His interests include playing classical guitar and banjo, juggling, keeping up with baseball, cooking, and spending time with his wife and his little Boston terrier."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nUniversity of Texas, Dallas | Dallas, TX | MS in Applied Cognition and Neuroscience | Aug. 2020 - Dec. 2023\nUniversity of Louisville | Louisville, KY | Music Therapy Board Certification | Aug. 2008 - June 2012\nAustin Peay State University | Clarksville, TN | BS in Music Performance | Aug. 2004 - May 2008"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Reflection No. 3: Three Optical Illusions That Stand Out to Me\n\n\n\n\n\n\n\nperception\n\n\ncognitive psychology\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nGrant Powell\n\n\n\n\n\n\n  \n\n\n\n\nReflection No. 6: My Favorite Perception Topic\n\n\n\n\n\n\n\nperception\n\n\ncognitive psychology\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nGrant Powell\n\n\n\n\n\n\n  \n\n\n\n\nReflection No. 5: What Purpose Does Pleasant Touch Play?\n\n\n\n\n\n\n\nperception\n\n\ncognitive psychology\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nGrant Powell\n\n\n\n\n\n\n  \n\n\n\n\nReflection No. 4: Why Our Brains Might Light Up Like a Christmas Tree on an fMRI?\n\n\n\n\n\n\n\nperception\n\n\ncognitive psychology\n\n\nmusic perception\n\n\nmusic therapy\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nGrant Powell\n\n\n\n\n\n\n  \n\n\n\n\nReflection No. 2: The Use of Color Perception for Survival Needs in Modern-day Society\n\n\n\n\n\n\n\nperception\n\n\ncognitive psychology\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nGrant Powell\n\n\n\n\n\n\n  \n\n\n\n\nReflection No. 1: Thoughts on the Perception of the Human Eye\n\n\n\n\n\n\n\nperception\n\n\ncognitive psychology\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nGrant Powell\n\n\n\n\n\n\n  \n\n\n\n\nReflection No. 3: Cognitive Essentials for Cybersecurity Topic on Nudging\n\n\n\n\n\n\n\ncybersecurity\n\n\ncognitive psychology\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\nGrant Powell\n\n\n\n\n\n\n  \n\n\n\n\nReflection No. 2: Cognitive Essentials for Cybersecurity Topic on Confirmation Bias\n\n\n\n\n\n\n\ncybersecurity\n\n\ncognitive psychology\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\nGrant Powell\n\n\n\n\n\n\n  \n\n\n\n\nReflection No. 1: Cognitive Essentials for Cybersecurity Topic on Attention\n\n\n\n\n\n\n\ncybersecurity\n\n\ncognitive psychology\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\nGrant Powell\n\n\n\n\n\n\n  \n\n\n\n\nReview of Tufte’s Future of Data Analysis Presentation\n\n\n\n\n\n\n\ndata analysis\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nGrant Powell\n\n\n\n\n\n\n  \n\n\n\n\nCritique of a Chart\n\n\n\n\n\n\n\ndata visualization\n\n\ncybersecurity\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nGrant Powell\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "For parts 1 and 2 of this assignment I have run line by line of each script of R code of Paul Murrell’s basic R programs from his book “R Graphics by including comments in the script of what each line of code does and some comments of some changes I made for each question that was being posed within the comments:\n# Line graph plots points showing where and how the pressure \n# rises when the temperature rises from 0 to infinity. The pch # is the symbol-type of the plotted data-point.\nplot(pressure, pch=10)  \n# The placement of the text within the line graph describing \n# what the graph is showing. \ntext(150, 600, \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n# Scatterplot\n# Note the incremental additions\n# Initializing each object with an array and list of values\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# This sets the margins of the graph and plot\npar(las=1, mar=c(4, 4, 4, 4), cex=.7) \nplot.new() # Completion of plotting in the curent plot and an advance to a new graphics frame\nplot.window(range(x), c(0, 6)) # Sets up the world coordinate system for a graphics window \nlines(x, y1) # Line showing plots for x and y1\nlines(x, y2) # Line showing plots for x and y2\npoints(x, y1, pch=16, cex=3) # Try different cex value? I chose 3. Sets the symbol type for each plotted point on the line \npoints(x, y2, pch=21, bg=\"red\", cex=2)  # Different background color. I changed each plotted point's color to yellow. \npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\") # Sets color for y and x-axis.\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for? It is which side of the plot the axis is to be drawn on. 1=below, 2=left, 3=above, and 4=right. \naxis(2, at=seq(0, 6, 2)) # 2 = y1 axis on left side\naxis(4, at=seq(0, 6, 2)) # 4 = y2 axis on right side\nbox(bty=\"u\") # Shape of the graph featuring an x-axis on the bottom and y-axises on the left and right side in the form of a \"u.\"\n# This inserts text to label the x-axis.\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8) \n# This inserts text to label the y-axis on the left side and I\n# changed the color of the label to match which line is being\n# shown that represents the data points for this label\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8, col=\"black\")\n# This inserts text to label the y-axis on the right side and I\n# changed the color of the label to match which line is being \n# shown that represents the data points for this label\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8, col=\"red\")\n# This inserts text within the graph to show that a specific is bird number 131.\ntext(4, 5, \"Bird 131\")\n# This sets the margins around the graph as it fits the screen and makes the lines of x-axis and y-axis black.\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Generates 50 random numbers\nY &lt;- rnorm(50)\n# Make sure no Y value limit on the x-axis exceeds [-3.5, 3.5]\nY[Y &lt; -1 | Y &gt; 1] &lt;- NA # Selection/set range. I chose -1 and 1\nx &lt;- seq(-3.5, 3.5, .1) # This sets the spacing of the tick marks along the x-axis\ndn &lt;- dnorm(x) # This establishes the numbers for a bell curve line along the \n               # x-axis to be saved in the dn object\npar(mar=c(4.5, 4.1, 3.1, 0)) # This sets the margins of the graph\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE) # This establishes the range of the x- and \n                               # y-axis and colors the histogram bars gray\n# This creates a normal bell curve line across the x-axis. \nlines(x, dnorm(x), lwd=2)\n# This sets the margins of that bell curve line within the graph\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Boxplot\n# This establishes the margins of the graph\n# I changed the bottom margin to push the x-axis up so that the label\n# can be visible\npar(mar=c(4, 4.1, 2, 1))\n# This is the boxplot function and it inserts the white boxes of data\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\n# This label what the x-axis represents\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\n# This is the boxplot function and it inserts the gray boxes of data\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        subset= supp == \"OJ\")\n# This inserts text within the graph to show that the white boxes represent\n# ascorbic acid and that the gray boxes represent orange juice\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\n# This establishes the margins of something.\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\n# Sets the range of the x-axis\nx &lt;- seq(-10, 10, length= 30)\n# The range of the y-axis is the same as the x-axis\ny &lt;- x\n# I am not sure what this does\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\n# I think this sets the height range of the z axis\nz &lt;- outer(x, y, f)\n# I am not sure what this does\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\n# I changed the bottom margin of the graph to 1 to push it up for better viewing\npar(mar=c(1, 0.5, 0, 0), lwd=0.5)\n# This is persp function that creates the 3D graphic inside the box\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\n# This sets the margins for something\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# This is a piechart.\n# The parameters for the margins, mar, on bottom, left, top, right. The xpd is to read data from a dataframe it is set to false to not do anything. The cex is to determine the line thickness of text and symbols for readability.\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\n# The proportion for each pie slice within the circle each, altogether, adding up to 1.\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\n# Each pie-flavor name placement in each order corresponds in respect to the order placement of each number in the list of the pie.sales object.\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\n# This helps color in the pie slice with different shades of the grayscale from the start of the gamma ray of the gray color range to the end of it that you want and there are 6 items in the pie.sales object.\npie(pie.sales, col = gray(seq(0.3,1.0,length=6)))\n \n\nFor part 3 of this assignment, I plotted some of the following functions using data from a data set called “Happy Planet Index” at http://happyplanetindex.org:\n\npar()\nlines()\npoints()\naxis()\nbox()\ntext()\nmtext()\nhist()\nboxplot()\nlegend()\npersp()\nnames()\npie()\n\nx &lt;- c(1, 2, 3)\ny1 &lt;- c(55.8, 62.3, 55.0)\ny2 &lt;- c(77.4, 83, 74.8)\ncountry &lt;- c(\"Colombia\", \"Switzerland\", \"Ecuador\")\n\nplot.new()\nplot.window(xlim = c(0, 4), ylim = c(0, 90)) # Do I need to ensure strings and characters are accepted?\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=21, bg= \"green4\", cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"red\", cex=2)  # Different background color\npar(mar=c(4, 4, 3, 3.5), col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\", cex=\"0.8\")\naxis(1, at = x, labels = country, las = 1)# What is the first number standing for?\naxis(2, at=seq(0, 90, 10))\naxis(4, at=seq(0, 90, 10))\nbox(bty=\"u\")\nmtext(\"Countries\", side=1, line=2, cex=1, col=\"black\")\nmtext(\"Happy Planet Index\", side=2, line=2, las=0, cex=1, col=\"green4\")\nmtext(\"Life Expectancy\", side=4, line=2, las=0, cex=1, col=\"red\")"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "For parts 1 and 2 of this assignment, I have shown comparisons of the regression models from the anscombe01.R document by showing different ways to create the plots by changing the colors, line types, and plot characters:\n\nHere I changed the color of the outline of the plot characters from red to purple, the background color inside the plot characters from orange to pink, and the regression line from blue to green. I also changed the plot characters from the plot character identification number of 21 to 11. Finally, I changed the regression line from a solid line to a dotted-dashed line.\n\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nsapply(mods, coef)  # Note the use of this function\nlapply(mods, function(fm) coef(summary(fm)))\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\n# I changed the color of the outline of the plot character from red to purple\n# I changed the background color of the plot character from orange to pink\n# I changed the line representing the \"line of best fit\" from blue to green\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"purple\", pch = 23, bg = \"pink\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"green\", lty = 4)\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\npar(op)\n\nFor part 3 of this assignment, I have shown comparisons of the regression models from the anscombe01.R document by showing different ways to create the plots with ggplot2:\nFor part 4 of this assignment, I have shown how to replicate a Scatterplot matrix of daily COVID deaths in Europe by month from 2020 to 2023 without using ggplot2 or any packages:\n## Download COVID data from OWID GitHub\nowidall &lt;- read.csv(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\n# Deselect cases/rows with OWID\nowidall &lt;- owidall[!grepl(\"^OWID\", owidall$iso_code), ]\n# Subset by continent: Europe\nowideu &lt;- subset(owidall, continent==\"Europe\")\n\ndate &lt;- as.Date(owideu$date, format = \"%Y-%m-%d\")\ndeaths &lt;- owideu$new_deaths\n\nplot(date, deaths, type = \"p\", ylim = c(0, 7000), xlab = \"Date\",\n     ylab = \"COVID Deaths in Europe (Daily)\", col = \"magenta2\", pch = 16, \n     cex = 0.7)\npar(las = 2)\naxis.Date(1, at = seq(min(date), max(date), by = \"2 months\"), \n          format = \"%m-%Y\")\naxis(2, at = seq(0, max(y), by = 1000))\npar(mar = c(5, 4, 1, 1) + 0.1, cex.axis = 0.6)\ntext(x = as.Date(\"2020-04-15\"), y = 6000, labels = \"Spain\", cex = 0.5)\ntext(x = as.Date(\"2020-04-18\"), y = 5000, labels = \"Spain\", cex = 0.5)\ntext(x = as.Date(\"2020-12-01\"), y = 6500, labels = \"Germany\", cex = 0.5,\n     pos = 3)\ntext(x = as.Date(\"2021-11-01\"), y = 5000, labels = \"Ukraine\", cex = 0.5)\ntext(x = as.Date(\"2023-01-01\"), y = 1500, labels = \"Germany\", cex = 0.5)\ntext(x = as.Date(\"2023-09-20\"), y = 500, labels = \"Italy\", cex = 0.5)"
  },
  {
    "objectID": "posts/2023-11-9 A Chart Critique/index.html",
    "href": "posts/2023-11-9 A Chart Critique/index.html",
    "title": "Critique of a Chart",
    "section": "",
    "text": "Up above, is a graph I found from Verizon’s 2023 Data Breach Investigative Report, which can be accessed at: https://www.verizon.com/business/resources/reports/dbir/2023/introduction/ It’s a graph that shows which types of data breaches have become more or less common over a span of six years from 2017-2023. The types of data breaches listed and categorized in the graph are “denial of service,” “privilege misuse,” “system intrusion,” “basic web application attacks,” “social engineering,” “lost and stolen assets,” “miscellaneous errors,” and “everything else.” There are some pros and cons I have discovered about this graph:\nPros\n\nIt looks at the fluctuations in the amount of a particular data breach over a span of months per year based on monthly averages instead of over a span of days or weeks in order to fit all the data into the small space that the graph covers.\nIt ensures that the viewer and reader of this graph is able to distinguish which line refers to a specific data breach category by matching them based on color.\n\nCons\n\nUsing colors is not necessarily the best way to separate the lines from each other and match a specific data breach category with a specific line. It cannot be assumed that everybody has normal color vision. There are too many cool colors, especially green, blue, and different hues of purple that make it hard for the eye to focus and differentiate the lines from each other. On the light spectrum, the human eye can see the colors green pretty well and the color red fairly well, but not the color blue. We have fewer cones in our eyes that pick up the color blue and, as a result, have to focus more on texts and details that are in the color blue or blended in with a blue background.\nThe text of the data breach category “basic web application attacks” is in a bright yellow color that can make it hard to see and read.\n\nThe best solution for presenting this graph is to either incorporate some more warmer and brighter colors to provide more contrast and differentiation between the colors to help the viewer and reader distinguish which line belongs to which data breach category. Or, create different types of lines such as dotted and broken lines, for example, which I recommend, instead of colored lines."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "This is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "runs &lt;- X2023BravesWinsAndLosses$R\n\nbin_width &lt;- 1\n\nhist(x = runs, main = \"Number of Runs Scored in Games by the Braves in 2023\", xlab = \"Number of Runs\", ylab = \"Number of Games\", col = \"red\", border = \"black\", breaks = seq(min(runs), max(runs) + bin_width, by = bin_width))"
  },
  {
    "objectID": "posts/2023-20-9 The Future of Data Analysis Review/index.html",
    "href": "posts/2023-20-9 The Future of Data Analysis Review/index.html",
    "title": "Review of Tufte’s Future of Data Analysis Presentation",
    "section": "",
    "text": "I watched a recorded video presentation by Dr. Edward Tufte from 2016 titled “The Future of Data Analysis.” It was a very insightful look into the changing landscape of the field of data analysis and data visualization. He provided some key findings of what all of us need and should strive for in the future in this field. Here are some notes and findings I highlighted from this presentation that I found very insightful:\n\nMost of us don’t think about this part of human history very much, but Gallileo, according to Dr. Tufte, was probably considered one of the greatest data thinking eyes to ever live. In his work of the Sidereus Nuncius, Gallileo illustrated the Pleiades cluster of stars in the sky that were visible, not with a telescope, but to the naked eye. Basically, it was a low-fidelity visualization of data points, the stars.\nProbably one of the most eye-opening and incredible data visualization graphics he showed was a graph showing the measles epidemic from a few decades back. It was split in half with the number of cases shown on the left without an available vaccine and the number of cases left after the vaccine. It was awe-inspiring the comparison being shown because there were hardly any data points for measles cases on the right side of the graph after a vaccine was provided. It showed the importance and power of vaccination. Here’s the graph:\n\n\nThe reason the measles incidence rate graphic was mind-blowing is it showed a “smart” comparison. Dr. Tufte points out that what we all need to strive for in the field of data visualization is not to make conclusions and findings, but to make smart comparisons by providing a better way of looking at the evidence that plain words and texts on paper are not able to do.\n\nThe only way to see big data is to see it. This ability in being able to see data has been greatly assisted by technological advances in screen-display quality of monitors. To clarify his point, he showed a map of the Swiss Alps that was put together to be seen on a screen-display and the contents were very clear and detailed. It was very easy on the eyes.\nThe greatest benefit about having musical scores and music being visualized through interactive colors and shapes is that it helps people with no musical abilities to make sense of the music and understand it is by relating elements of the music being heard and seen with tangible, visual patterns.\nMaps like Google Maps need to have light background colors in to for us to see the details and information laid out before us. It provides contrast.\nSometimes, it is best to capture everything in an instant replay video of a sports highlight or sequence of soccer plays, for example, on a one-page display in stop-action. It is a good way for one to learn and analyze another team’s performance, and strengths and weaknesses, if you are the athlete.\nIt is important to have an open mind, but not an empty head when analyzing data. This is because we want to avoid bias. If we do not, then the problem of fake publications, false-positives, and false-alarms will be an issue in our findings and conclusions of the data. The responsibility of data analytics is to get it right. This is why, for example, when analyzing human behavior that it is harder to replicate replication studies on human behavior and, thus, makes the analysis of human behavior harder than rocket science.\n\nAfter sharing all these invaluable pieces of insight, Dr. Tufte concluded his presentation by pointing out that you can have you own point of view, but not your own facts. He points out that we need to muck around in our data to discover what we did not know. He advises us to take a look a look at the “Quartz Guide to Bad Data” and not fall victim to biases such as “survival bias” where as a famous saying goes “Castles are not all made of stone. They are also made of wood.”"
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Here are charts my group was able to generate for the hackathon.\n\n# Sample data frame\ndata &lt;- data.frame(Item = c(\"Item1\", \"Item2\", \"Item3\"),\nVariable1 = c(10, 15, 20),\nVariable2 = c(5, 8, 12))\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Reshape the data from wide to long format using tidyr\nlibrary(tidyr)\ndata_long &lt;- pivot_longer(data, cols = starts_with(\"Variable\"),\nnames_to = \"Variable\",\nvalues_to = \"Value\")\n\n# Create the grouped bar chart\nggplot(data_long, aes(x = Item, y = Value, fill = Variable)) +\ngeom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +\nlabs(title = \"Comparison of Variables for Three Items\",\nx = \"Items\",\ny = \"Value\") +\ntheme_minimal()\n\n\n\n\n\n# Sample data frame\ndata &lt;- data.frame(Item = c(\"Item1\", \"Item2\", \"Item3\", \"Item4\",\"Item5\",\"item6\",\"item7\",\"item8\"),\nValue1 = c(30, 45, 25, 60, 35,40,31,37),\nValue2 = c(20, 40, 50, 30, 45,21,65,35))\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Create the first horizontal bar chart\nchart1 &lt;- ggplot(data, aes(x = Value1, y = Item, fill = Item)) +\ngeom_bar(stat = \"identity\", orientation = \"y\") +\nlabs(title = \"\", x = \"Value\", y = \"Item\") +\ntheme_minimal() +\ntheme(axis.text.y = element_text(hjust = 1)) # Right-align item labels\n\n# Create the second horizontal bar chart\nchart2 &lt;- ggplot(data, aes(x = Value2, y = Item, fill = Item)) +\ngeom_bar(stat = \"identity\", orientation = \"y\") +\nlabs(title = \"\", x = \"Value\", y = \"Item\") +\ntheme_minimal() +\ntheme(axis.text.y = element_text(hjust = 1)) # Right-align item labels\n\n# Display the charts side by side\nlibrary(gridExtra)\n\ngrid.arrange(chart1, chart2, ncol = 2)"
  },
  {
    "objectID": "assignment7.html#examples-of-chart-9-scatter-chart-from-chart-suggestions---a-thought-starter",
    "href": "assignment7.html#examples-of-chart-9-scatter-chart-from-chart-suggestions---a-thought-starter",
    "title": "Assignment 7",
    "section": "Examples of Chart #9 Scatter Chart from “Chart Suggestions - A Thought Starter”",
    "text": "Examples of Chart #9 Scatter Chart from “Chart Suggestions - A Thought Starter”\n# Sample data\nx &lt;- X2023BattingAverages$BA\ny &lt;- X2023BattingAverages$PA\n\n# Create a scatter chart\nplot(x, y, main = \"Batting Average vs. Plate Appearances of 2023 MLB Teams\", xlab = \"Batting Average\", ylab = \"Plate Appearances\", ylim = c(5000, 7000), col = \"blue\", pch = 1\n)"
  },
  {
    "objectID": "assignment7.html#example-of-chart-18-bubble-chart-from-chart-suggestions---a-thought-starter",
    "href": "assignment7.html#example-of-chart-18-bubble-chart-from-chart-suggestions---a-thought-starter",
    "title": "Assignment 7",
    "section": "Example of Chart #18 Bubble Chart from “Chart Suggestions - A Thought Starter”",
    "text": "Example of Chart #18 Bubble Chart from “Chart Suggestions - A Thought Starter”"
  },
  {
    "objectID": "assignment6.html",
    "href": "assignment6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Shiny App User Interface of RStudio Datasets"
  },
  {
    "objectID": "assignment6.html#shiny-app-link-with-datasets-from-the-rstudio-datasets-library",
    "href": "assignment6.html#shiny-app-link-with-datasets-from-the-rstudio-datasets-library",
    "title": "Assignment 6",
    "section": "",
    "text": "Shiny App User Interface of RStudio Datasets"
  },
  {
    "objectID": "assignment5.html#histogram-with-r-graphics",
    "href": "assignment5.html#histogram-with-r-graphics",
    "title": "Assignment 5",
    "section": "",
    "text": "runs &lt;- X2023BravesWinsAndLosses$R\n\nbin_width &lt;- 1\n\nhist(x = runs, main = \"Number of Runs Scored in Games by the Braves in 2023\", xlab = \"Number of Runs\", ylab = \"Number of Games\", col = \"red\", border = \"black\", breaks = seq(min(runs), max(runs) + bin_width, by = bin_width))"
  },
  {
    "objectID": "assignment5.html#bar-chart-with-r-graphics",
    "href": "assignment5.html#bar-chart-with-r-graphics",
    "title": "Assignment 5",
    "section": "Bar Chart with R Graphics",
    "text": "Bar Chart with R Graphics\nmonths &lt;- c(\"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \n           \"October\")\nwins &lt;- c(1, 17, 15, 21, 13, 21, 16, 0)\nlosses &lt;- c(0, 9, 14, 4, 10, 8, 12, 1)\nbarplot(matrix(c(wins, losses), nrow = 2, byrow = TRUE), beside = TRUE, \n        names.arg = months, main = \"Atlanta Brave's 2023 Monthly Wins\",  \n        xlab = \"Months\", ylab = \"Count\", ylim = c(0, 25), \n        col = c(\"navy\", \"red\"), legend.text = c(\"Wins\", \"Losses\"),las = 2)"
  },
  {
    "objectID": "assignment5.html#pie-chart-with-r-graphics",
    "href": "assignment5.html#pie-chart-with-r-graphics",
    "title": "Assignment 5",
    "section": "Pie Chart with R Graphics",
    "text": "Pie Chart with R Graphics\ndata \n\nhome_runs2 &lt;- head(data$HR, 8)\n\nnames &lt;- c(\"Arizona Diamondbacks\", \"Atlanta Braves\", \"Baltimore Orioles\", \"Boston Red Sox\", \"Chicago Cubs\", \"Chicago White Sox\", \"Cincinnati Reds\", \"Cleveland Guardians\")\n\ncolors &lt;- rainbow(length(home_runs2))\n\npie(x = home_runs2, col = colors, labels = home_runs2)\n\ntitle(\"2023 Home Run Totals by Team\")\n\nlegend(\"topright\", legend = names, fill = colors, title = \"Teams\", cex = 0.8)"
  },
  {
    "objectID": "assignment5.html#histograms-with-ggplot2",
    "href": "assignment5.html#histograms-with-ggplot2",
    "title": "Assignment 5",
    "section": "Histograms with ggplot2",
    "text": "Histograms with ggplot2\n# Import ggplot2\nlibrary(ggplot2)\n\n# Use the ggplot function to create a histogram\nggplot(X2023BravesWinsAndLosses, aes(x = R)) + geom_histogram(binwidth = 1, fill = \"red\", color = \"black\") + labs(x = \"Number of Runs\", y = \"Number of Games\", title = \"Number of Runs Scored in Games by the Braves in 2023\") + \nscale_y_continuous(breaks = seq(0, 25, by = 5)) + \nscale_x_continuous(breaks = seq(0, 25, by = 5))\n\n# Import ggplot2\nlibrary(ggplot2)\n\n# Read and initialize data into an object\nbaseball &lt;- read.csv(\"/Users/thefinas1/documents/2023BattingAverages.csv\", \n                     nrows = 8)\n\n# Use the ggplot function to create a histogram\nggplot(baseball, aes(x = HR, fill = Tm, )) + geom_histogram(binwidth = 4) + \n  xlab(\"Total Home Runs for 2023\") + ylab(\"Number of Baseball Teams\") + \n  scale_y_continuous(breaks = c(0, 1, 2)) + labs(fill = \"Baseball Teams\") + \n  scale_x_continuous(breaks = seq(100, 350, by = 50))"
  },
  {
    "objectID": "assignment5.html#bar-chart-with-ggplot2",
    "href": "assignment5.html#bar-chart-with-ggplot2",
    "title": "Assignment 5",
    "section": "Bar Chart with ggplot2",
    "text": "Bar Chart with ggplot2\n# Create a data frame with the desired order of months\ndata &lt;- data.frame(\n  months = factor(c(\"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\"),\n                  levels = c(\"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\")),\n  losses = c(0, 9, 14, 4, 10, 8, 12, 1),\n  wins = c(1, 17, 15, 21, 13, 21, 16, 0)\n)\n\n# Now, create a bar chart using ggplot2\nlibrary(ggplot2)\n\nggplot(data, aes(x = months)) +\n  geom_bar(aes(y = wins, fill = \"Wins\"), stat = \"identity\", position = \"dodge\") +\n  geom_bar(aes(y = losses, fill = \"Losses\"), stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Comparison of Wins and Losses by Month\",\n       x = \"Months\",\n       y = \"Count\") +\n  scale_fill_manual(values = c(\"Wins\" = \"blue\", \"Losses\" = \"red\")) +\n  theme_minimal()"
  },
  {
    "objectID": "assignment5.html#scatter-plot-with-ggplot2",
    "href": "assignment5.html#scatter-plot-with-ggplot2",
    "title": "Assignment 5",
    "section": "Scatter Plot with ggplot2",
    "text": "Scatter Plot with ggplot2\n# Import ggplot2\nlibrary(ggplot2)\n\n# Read and initialize data into an object\nbaseball &lt;- read.csv(\"/Users/thefinas1/documents/2023BattingAverages.csv\", \n                     nrows = 8)\n\n# Use the ggplot function to create a scatterplot\nggplot(baseball, aes(x = Tm, y = HR)) + geom_point() + \n  xlab(\"Baseball Teams\") + ylab(\"Number of Home Runs\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "assignment8.html",
    "href": "assignment8.html",
    "title": "Assignment 8",
    "section": "",
    "text": "Chart 1\n\nlacrime2020 &lt;- lacrime[2:148206, ]\nlacrime2020$Vict.Descent[lacrime2020$Vict.Descent == \"\"] &lt;- \"X\"\nlacrime2020$Vict.Descent[lacrime2020$Vict.Descent == \"C\"] &lt;- \"A\"\nlacrime2020$Vict.Descent[lacrime2020$Vict.Descent == \"F\"] &lt;- \"A\"\nlacrime2020$Vict.Descent[lacrime2020$Vict.Descent == \"J\"] &lt;- \"A\"\nlacrime2020$Vict.Descent[lacrime2020$Vict.Descent == \"K\"] &lt;- \"A\"\nlacrime2020$Vict.Descent[lacrime2020$Vict.Descent == \"V\"] &lt;- \"A\"\n\nfiltered_data &lt;- lacrime2020 %&gt;% filter(Vict.Descent == \"A\")\n\ndates_for_letter_A &lt;- filtered_data$DATE.OCC\n\nfiltered_data$DATE.OCC &lt;- as.Date(filtered_data$DATE.OCC)\n\nsummary_data &lt;- filtered_data %&gt;%\n  group_by(DATE.OCC) %&gt;%\n  summarize(Count = n())\n\nplot(summary_data$DATE.OCC, summary_data$Count, type = \"p\", pch = 1, cex = 0.4,\n     col = \"blue\", xlab = \"Date Occurred\", ylab = \"Number of Victims\", \n     main = \"Number of Asian Victims Daily in Los Angeles 2020\", mgp = c(3, 1, 0),\n     xaxt = \"n\", yaxt = \"n\")\naxis(1, at = seq(min(summary_data$DATE.OCC), max(summary_data$DATE.OCC), by = \"month\"), \n     labels = format(seq(min(summary_data$DATE.OCC), max(summary_data$DATE.OCC), by = \"month\"), \"%b %Y\"), cex.axis = 0.4)\naxis(2, las = 0, cex.axis = 0.4)\n\nlockdown_date &lt;- as.Date(\"2020-03-19\")\ntext(x = lockdown_date, y = max(summary_data$Count), labels = \"Lockdown 3/19\", pos = 1, offset = 1, cex = 0.4)\n\ntravel_ban &lt;- as.Date(\"2020-01-31\")\ntext(x = travel_ban, y = 39, labels = \"Travel Ban 1/31\", pos = 1, offset = 1, cex = 0.4)\n\nmodel &lt;- lm(summary_data$Count ~ as.numeric(summary_data$DATE.OCC))\nabline(model, col = \"red\")\n\n\n\n\n\n\nChart 2\n\nlacrime2021 &lt;- lacrime[148207:303895, ]\nlacrime2021$Vict.Descent[lacrime2021$Vict.Descent == \"\"] &lt;- \"X\"\nlacrime2021$Vict.Descent[lacrime2021$Vict.Descent == \"C\"] &lt;- \"A\"\nlacrime2021$Vict.Descent[lacrime2021$Vict.Descent == \"F\"] &lt;- \"A\"\nlacrime2021$Vict.Descent[lacrime2021$Vict.Descent == \"J\"] &lt;- \"A\"\nlacrime2021$Vict.Descent[lacrime2021$Vict.Descent == \"K\"] &lt;- \"A\"\nlacrime2021$Vict.Descent[lacrime2021$Vict.Descent == \"V\"] &lt;- \"A\"\n\n# Filter the rows with \"A\" in the Vict.Descent column\nfiltered_data_2 &lt;- lacrime2021 %&gt;% filter(Vict.Descent == \"A\")\n\n# Extract the dates from the filtered data\ndates_for_letter_A_2 &lt;- filtered_data$DATE.OCC\n\n# Make sure the \"Date Occurred\" column is in the date format\nfiltered_data_2$DATE.OCC &lt;- as.Date(filtered_data_2$DATE.OCC)\n\nsummary_data_2 &lt;- filtered_data_2 %&gt;%\n  group_by(DATE.OCC) %&gt;%\n  summarize(Count = n())\n\n# Create a time-series scatterplot\nplot(summary_data_2$DATE.OCC, summary_data_2$Count, type = \"p\", pch = 1, cex = 0.4,\n     col = \"blue\", xlab = \"Date Occurred\", ylab = \"Number of Victims\", \n     main = \"Number of Asian Victims Daily in Los Angeles 2021\", mgp = c(3, 1, 0),\n     xaxt = \"n\", yaxt = \"n\")\naxis(1, at = seq(min(summary_data_2$DATE.OCC), max(summary_data_2$DATE.OCC), by = \"month\"), \n     labels = format(seq(min(summary_data_2$DATE.OCC), max(summary_data_2$DATE.OCC), by = \"month\"), \"%b %Y\"), cex.axis = 0.4)\naxis(2, las = 0, cex.axis = 0.4)\n\n\n\n\n\n\nChart 3\n\nlacrime2022 &lt;- lacrime[303896:404565, ]\nlacrime2022$Vict.Descent[lacrime2022$Vict.Descent == \"\"] &lt;- \"X\"\nlacrime2022$Vict.Descent[lacrime2022$Vict.Descent == \"C\"] &lt;- \"A\"\nlacrime2022$Vict.Descent[lacrime2022$Vict.Descent == \"F\"] &lt;- \"A\"\nlacrime2022$Vict.Descent[lacrime2022$Vict.Descent == \"J\"] &lt;- \"A\"\nlacrime2022$Vict.Descent[lacrime2022$Vict.Descent == \"K\"] &lt;- \"A\"\nlacrime2022$Vict.Descent[lacrime2022$Vict.Descent == \"V\"] &lt;- \"A\"\n\nfiltered_data_3 &lt;- lacrime2022 %&gt;% filter(Vict.Descent == \"A\")\n\nfiltered_data_3$DATE.OCC &lt;- as.Date(filtered_data_3$DATE.OCC)\n\nfiltered_data_3 &lt;- filtered_data_3[order(filtered_data_3$DATE.OCC), ]\n\nsummary_data_3 &lt;- filtered_data_3 %&gt;%\n  group_by(DATE.OCC) %&gt;%\n  summarize(Count = n())\n\n# Create a time-series scatterplot with no x-axis labels\nplot(summary_data_3$DATE.OCC, summary_data_3$Count, type = \"p\", pch = 1, cex = 0.4,\n     col = \"blue\", xlab = \"Date Occurred\", ylab = \"Number of Victims\", \n     main = \"Number of Asian Victims Daily in Los Angeles 2022\", mgp = c(3, 1, 0), \n     xaxt = \"n\", yaxt = \"n\")\naxis(1, at = seq(min(summary_data_3$DATE.OCC), max(summary_data_3$DATE.OCC), by = \"month\"), \n     labels = format(seq(min(summary_data_3$DATE.OCC), max(summary_data_3$DATE.OCC), by = \"month\"), \"%b %Y\"), cex.axis = 0.6)\naxis(2, las = 0, cex.axis = 0.6)"
  },
  {
    "objectID": "assignment9.html",
    "href": "assignment9.html",
    "title": "Assignment 9",
    "section": "",
    "text": "library(TSstudio)\nquantmod::getSymbols(\"BATRA\", src=\"yahoo\")\nclass(BATRA)\nts_plot(BATRA$BATRA.Adjusted, \n        title = \"Atlanta Braves Stock Prices\",\n        Ytitle = \"\")\nclass(BATRA) # What class is this object? It is an xts and zoo class.\n\nApart from the huge dip near 2020, the graph looks more like it has a progressive, upward trend that is increasing over time with some small-level spikes and dips along the way. The BATRA class is an “xts” and “zoo” class."
  },
  {
    "objectID": "assignment9.html#time-series-chart-of-the-stock-for-the-atlanta-braves-baseball-team-using-tsstudio01.r",
    "href": "assignment9.html#time-series-chart-of-the-stock-for-the-atlanta-braves-baseball-team-using-tsstudio01.r",
    "title": "Assignment 9",
    "section": "",
    "text": "library(TSstudio)\nquantmod::getSymbols(\"BATRA\", src=\"yahoo\")\nclass(BATRA)\nts_plot(BATRA$BATRA.Adjusted, \n        title = \"Atlanta Braves Stock Prices\",\n        Ytitle = \"\")\nclass(BATRA) # What class is this object? It is an xts and zoo class.\n\nApart from the huge dip near 2020, the graph looks more like it has a progressive, upward trend that is increasing over time with some small-level spikes and dips along the way. The BATRA class is an “xts” and “zoo” class."
  },
  {
    "objectID": "assignment9.html#time-series-chart-of-ukgas-dataset-using-tsstudio01.r",
    "href": "assignment9.html#time-series-chart-of-ukgas-dataset-using-tsstudio01.r",
    "title": "Assignment 9",
    "section": "Time-Series Chart of UKgas Dataset Using tsstudio01.R",
    "text": "Time-Series Chart of UKgas Dataset Using tsstudio01.R\nlibrary(TSstudio)\nquantmod::getSymbols(\"BATRA\", src=\"yahoo\")\nclass(BATRA)\nts_plot(BATRA$BATRA.Adjusted, \n        title = \"Atlanta Braves Stock Prices\",\n        Ytitle = \"\")\nclass(BATRA) # What class is this object? It is an xts and zoo class.\n\n# Some sample dataset from TSstudio\nts_seasonal(UKgas, type = \"\") # month-year matrix data\n\n# What class is USgas? It is a ts class. I'm guessing ts equals time series?\n\n# Sample charts\nts_heatmap(UKgas)\nts_cor(UKgas) # ACF and PACF\nts_lags(UKgas, margin = .02)\nUKgas=data.frame(UKgas)\n\nThe data set “USgas” was replaced with “UKgas.” It shows faceting chart or grid commonly found in ggplot and the data set “USgas” and “UKgas” is of the class “ts.”"
  },
  {
    "objectID": "assignment4.html#chart-1-variable-width-column-chart-attempt-by-team-member-yichao-jin-from-chart-suggestions---a-thought-starter",
    "href": "assignment4.html#chart-1-variable-width-column-chart-attempt-by-team-member-yichao-jin-from-chart-suggestions---a-thought-starter",
    "title": "Assignment 4",
    "section": "Chart #1 Variable Width Column Chart Attempt by Team Member Yichao Jin from “Chart Suggestions - A Thought Starter”",
    "text": "Chart #1 Variable Width Column Chart Attempt by Team Member Yichao Jin from “Chart Suggestions - A Thought Starter”\n\n# Sample data frame\ndata &lt;- data.frame(Item = c(\"Item1\", \"Item2\", \"Item3\"),\nVariable1 = c(10, 15, 20),\nVariable2 = c(5, 8, 12))\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Reshape the data from wide to long format using tidyr\nlibrary(tidyr)\ndata_long &lt;- pivot_longer(data, cols = starts_with(\"Variable\"),\nnames_to = \"Variable\",\nvalues_to = \"Value\")\n\n# Create the grouped bar chart\nggplot(data_long, aes(x = Item, y = Value, fill = Variable)) +\ngeom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +\nlabs(title = \"Comparison of Variables for Three Items\",\nx = \"Items\",\ny = \"Value\") +\ntheme_minimal()"
  },
  {
    "objectID": "assignment4.html#chart-2-table-or-table-with-embedded-charts-from-chart-suggestions---a-thought-starter",
    "href": "assignment4.html#chart-2-table-or-table-with-embedded-charts-from-chart-suggestions---a-thought-starter",
    "title": "Assignment 4",
    "section": "Chart #2 Table or Table with Embedded Charts from “Chart Suggestions - A Thought Starter”",
    "text": "Chart #2 Table or Table with Embedded Charts from “Chart Suggestions - A Thought Starter”\n(Coming soon…)"
  },
  {
    "objectID": "assignment4.html#chart-3-bar-chart-attempt-by-team-member-yichao-jin-from-chart-suggestions---a-thought-starter",
    "href": "assignment4.html#chart-3-bar-chart-attempt-by-team-member-yichao-jin-from-chart-suggestions---a-thought-starter",
    "title": "Assignment 4",
    "section": "Chart #3 Bar Chart Attempt by Team Member Yichao Jin from “Chart Suggestions - A Thought Starter”",
    "text": "Chart #3 Bar Chart Attempt by Team Member Yichao Jin from “Chart Suggestions - A Thought Starter”\n\n# Sample data frame\ndata &lt;- data.frame(Item = c(\"Item1\", \"Item2\", \"Item3\", \"Item4\",\"Item5\",\"item6\",\"item7\",\"item8\"),\nValue1 = c(30, 45, 25, 60, 35,40,31,37),\nValue2 = c(20, 40, 50, 30, 45,21,65,35))\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Create the first horizontal bar chart\nchart1 &lt;- ggplot(data, aes(x = Value1, y = Item, fill = Item)) +\ngeom_bar(stat = \"identity\", orientation = \"y\") +\nlabs(title = \"\", x = \"Value\", y = \"Item\") +\ntheme_minimal() +\ntheme(axis.text.y = element_text(hjust = 1)) # Right-align item labels\n\n# Create the second horizontal bar chart\nchart2 &lt;- ggplot(data, aes(x = Value2, y = Item, fill = Item)) +\ngeom_bar(stat = \"identity\", orientation = \"y\") +\nlabs(title = \"\", x = \"Value\", y = \"Item\") +\ntheme_minimal() +\ntheme(axis.text.y = element_text(hjust = 1)) # Right-align item labels\n\n# Display the charts side by side\nlibrary(gridExtra)\n\ngrid.arrange(chart1, chart2, ncol = 2)"
  },
  {
    "objectID": "assignment4.html#chart-4-column-chart-from-chart-suggestions---a-thought-starter",
    "href": "assignment4.html#chart-4-column-chart-from-chart-suggestions---a-thought-starter",
    "title": "Assignment 4",
    "section": "Chart #4 Column Chart from “Chart Suggestions - A Thought Starter”",
    "text": "Chart #4 Column Chart from “Chart Suggestions - A Thought Starter”\n(Coming soon…)"
  },
  {
    "objectID": "assignment6.html#shiny-app-link-with-major-league-baseball-datasets-from-the-sports-reference-website",
    "href": "assignment6.html#shiny-app-link-with-major-league-baseball-datasets-from-the-sports-reference-website",
    "title": "Assignment 6",
    "section": "Shiny App Link with Major League Baseball Datasets from the Sports Reference Website",
    "text": "Shiny App Link with Major League Baseball Datasets from the Sports Reference Website\nShiny App User Interface of Grant’s Baseball Datasets"
  },
  {
    "objectID": "assignment7.html#examples-of-chart-9-scatter-chart-and-18-bubble-chart-from-chart-suggestions---a-thought-starter",
    "href": "assignment7.html#examples-of-chart-9-scatter-chart-and-18-bubble-chart-from-chart-suggestions---a-thought-starter",
    "title": "Assignment 7",
    "section": "Examples of Chart #9 Scatter Chart and #18 Bubble Chart from “Chart Suggestions - A Thought Starter”",
    "text": "Examples of Chart #9 Scatter Chart and #18 Bubble Chart from “Chart Suggestions - A Thought Starter”\n# Sample data\nx &lt;- X2023BattingAverages$BA\ny &lt;- X2023BattingAverages$PA\n\n# Create a scatter chart\nplot(x, y, main = \"Batting Average vs. Plate Appearances of 2023 MLB Teams\", xlab = \"Batting Average\", ylab = \"Plate Appearances\", ylim = c(5000, 7000), col = \"blue\", pch = 1\n)"
  },
  {
    "objectID": "assignment7.html#example-of-chart-9-scatter-chart-and-chart-18-bubble-chart-from-chart-suggestions---a-thought-starter",
    "href": "assignment7.html#example-of-chart-9-scatter-chart-and-chart-18-bubble-chart-from-chart-suggestions---a-thought-starter",
    "title": "Assignment 7",
    "section": "Example of Chart #9 Scatter Chart and Chart #18 Bubble Chart from “Chart Suggestions - A Thought Starter”",
    "text": "Example of Chart #9 Scatter Chart and Chart #18 Bubble Chart from “Chart Suggestions - A Thought Starter”\n\nFrom my Group Member, Tayaba Saleem:\n\n\nhttps://tayabasaleem123.shinyapps.io/Teamsprojectshinyapp/"
  },
  {
    "objectID": "assignment10.html",
    "href": "assignment10.html",
    "title": "Assignment 10",
    "section": "",
    "text": "https://gpowwow.shinyapps.io/mapp/"
  },
  {
    "objectID": "assignment10.html#a-map-of-my-location",
    "href": "assignment10.html#a-map-of-my-location",
    "title": "Assignment 10",
    "section": "",
    "text": "https://gpowwow.shinyapps.io/mapp/"
  },
  {
    "objectID": "project.html#my-teams-project-proposal",
    "href": "project.html#my-teams-project-proposal",
    "title": "Project",
    "section": "",
    "text": "## My Graphs and Plots"
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "Assignment 1",
    "section": "",
    "text": "For part 1 of this assignment, an example of Anscombe’s data from 1973 was run through an executable cell, which turned it into scatter plots with a regression line demonstrating the “Line of Best Fit” for all the data.\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nsapply(mods, coef)  # Note the use of this function\nlapply(mods, function(fm) coef(summary(fm)))\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\npar(op)\n\nFor part 2 of this assignment, examples of “generative art” were found through a Google search and these can be found through the links posted on my homepage of this website.\nFor part 3 of this assignment, an example of a spring leaf graphic was generated after running this Fall.R script in an executable cell where the color of the leaf was changed to violet.\n# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\ninstall.packages(\"gsubfn\")\ninstall.packages(\"tidyverse\")\nlibrary(gsubfn)\nlibrary(tidyverse)\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"violet\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\n\n\n\n\n\nFor part 4 of this assignment, my critique of a chart from a published work can be found by visiting my blog page on this website."
  },
  {
    "objectID": "data_visualization.html#assignments",
    "href": "data_visualization.html#assignments",
    "title": "Assignment 1",
    "section": "",
    "text": "For part 1 of this assignment, an example of Anscombe’s data from 1973 was run through an executable cell, which turned it into scatter plots with a regression line demonstrating the “Line of Best Fit” for all the data.\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nsapply(mods, coef)  # Note the use of this function\nlapply(mods, function(fm) coef(summary(fm)))\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\npar(op)\n\nFor part 2 of this assignment, examples of “generative art” were found through a Google search and these can be found through the links posted on my homepage of this website.\nFor part 3 of this assignment, an example of a spring leaf graphic was generated after running this Fall.R script in an executable cell where the color of the leaf was changed to violet.\n# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\ninstall.packages(\"gsubfn\")\ninstall.packages(\"tidyverse\")\nlibrary(gsubfn)\nlibrary(tidyverse)\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"violet\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\n\n\n\n\n\nFor part 4 of this assignment, my critique of a chart from a published work can be found by visiting my blog page on this website."
  },
  {
    "objectID": "monthlyBarchartsOfAsianVictims.html",
    "href": "monthlyBarchartsOfAsianVictims.html",
    "title": "Monthly Barcharts of Asian Victims",
    "section": "",
    "text": "Chart 1\n\n\n\n\n\n\n\nChart 2\n\n\n\n\n\n\n\nChart 3"
  },
  {
    "objectID": "yearlyBarchartsOfBlackAndAsianVictims.html",
    "href": "yearlyBarchartsOfBlackAndAsianVictims.html",
    "title": "Yearly Barcharts Of Black and Asian Victims",
    "section": "",
    "text": "Chart 1\n\n\n\n\n\n\n\nChart 2"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Grant Powell’s Portfolio",
    "section": "",
    "text": "You have stumbled upon Grant Powell’s portfolio website. It features his work from courses he completed at the University of Texas at Dallas through the human computer interaction specialization track of the Applied Cognition and Neuroscience Masters degree program. The courses in which he completed projects and assignments to deepen and showcase his understanding are from:\n\nCognitive Psychology Essentials for Cybersecurity\nData Visualization\nHuman Computer Interaction I, II, & III\nPerception\nSpeech Perception\nUser Experience Design"
  },
  {
    "objectID": "cybersecurity.html",
    "href": "cybersecurity.html",
    "title": "Cybersecurity",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "cybersecurity.html#quarto",
    "href": "cybersecurity.html#quarto",
    "title": "Cybersecurity",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "cybersecurity.html#running-code",
    "href": "cybersecurity.html#running-code",
    "title": "Cybersecurity",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "accessibility.html",
    "href": "accessibility.html",
    "title": "Accessibility",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "accessibility.html#quarto",
    "href": "accessibility.html#quarto",
    "title": "Accessibility",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "accessibility.html#running-code",
    "href": "accessibility.html#running-code",
    "title": "Accessibility",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "hotel_website_redesign.html",
    "href": "hotel_website_redesign.html",
    "title": "Hotel Website Redesign",
    "section": "",
    "text": "This is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "hotel_website_redesign.html#quarto",
    "href": "hotel_website_redesign.html#quarto",
    "title": "Hotel Website Redesign",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "hotel_website_redesign.html#running-code",
    "href": "hotel_website_redesign.html#running-code",
    "title": "Hotel Website Redesign",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "ride_saver_app.html",
    "href": "ride_saver_app.html",
    "title": "RideSaver App",
    "section": "",
    "text": "This is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "ride_saver_app.html#quarto",
    "href": "ride_saver_app.html#quarto",
    "title": "RideSaver App",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "ride_saver_app.html#running-code",
    "href": "ride_saver_app.html#running-code",
    "title": "RideSaver App",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "easy_internet_app.html",
    "href": "easy_internet_app.html",
    "title": "Easy Internet App",
    "section": "",
    "text": "This is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "easy_internet_app.html#quarto",
    "href": "easy_internet_app.html#quarto",
    "title": "Easy Internet App",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "easy_internet_app.html#running-code",
    "href": "easy_internet_app.html#running-code",
    "title": "Easy Internet App",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "closed_caption_1.html",
    "href": "closed_caption_1.html",
    "title": "HCI 1 Research",
    "section": "",
    "text": "Grant Powell\nSchool of Brain and Behavioral Sciences, University of Texas at Dallas\nACN 6341: Human Computer Interaction I\nProfessor Ericka Orrick\nNovember 24, 2020\n\n\nEver since closed captioning was first debuted in 1971 and first available for limited use in 1972, it has not gone through as much of a transformation to improve upon its ability to further address the needs of the deaf and hard-of-hearing (HOH) (Hunter, 2017). Closed captioning means that text showing what someone on the television is saying is not available the moment one turns on the television, which would be open captioning. It is available only when that person goes to the television settings and turns it on if they realize the need to improve their viewing experience (“What is the Difference Between Open and Closed Captioning,” 2020). Closed captioning was not available to the entire viewing public until 1980 when it was provided for widespread access by allowing deaf and HOH viewers the opportunity to purchase special decoder boxes that could be attached to the television in order to access the closed captioning capabilities. Thanks to the United States (US) Television Decoder Circuitry Act of 1993, decoders are embedded in all televisions that customers purchase in the U.S. (Branje et al., 2005, p. 2330).\nIn the US Hanson (2009) pointed out that the National Institute on Deafness and Other Communication Disorder’s (NIDCD) estimated statistical figures from 2004 showed that 28 million people in the US had some degree of hearing loss. Now, that figure for the US has increased to 48 million people with some degree of hearing loss including 477 million people worldwide according to the most recent estimated statistics available to date (Hearing Health Foundation, 2020).\nClosed captioning is still presented as white text, in a single mono-spaced font and single font size, against a black background on most video platforms with some changes (see Appendix). For example, it has changed some from only all capital letters to mixed case letters, and it sometimes shows a small set of text colors and uses special characters such as music notes to inform the user when noise, sound, or music is being played (Branje et al., 2005, p. 2330). These minor changes are the result of advancements in encoder and decoder technologies from when it first appeared on analogue televisions (Fels et al., 2008, p. 506). With these changes, along with the fact that current closed captioning guidelines in the captioning industry have remained unchanged in the US since captioning was mandated by the Federal Communications Commission, captioners sometimes make creative choices that may not be obvious to the user such as assigning descriptive text to sound effects, paraphrasing long speeches, and omitting information deemed unnecessary to try to improve the user’s viewing experience (Hanson, 2009, p. 128 and Fels et al., 2008, p. 506). However, these changes are not enough to meet the needs of the deaf and HOH population because its needs and range of hearing loss is very diverse.\nThe major issue that separates people with hearing loss on the spectrum of hearing loss ranging from relatively little hearing loss to profound hearing loss is language acquisition skills. Because experiences of difficulty with pitch, timbre, and loudness will occur with hearing loss, speech perception difficulties will not only be visible, but reading and writing skills will inevitably be affected (Hanson, 2009, p. 126). Being able to understand this has implications for interface designers as it relates to closed captioning for the purpose of improving the end users’ video viewing experience either for entertainment, education, or communication purposes. For example, users who are deaf and HOH may have difficulty with reading because of speech perception difficulties. This is especially the case if they lost their hearing at birth or during the pre-lingual stage of child development because reading is based on the overall phonetic structure and understanding of grammar and text comprehension of a spoken language (Hanson, 2009, p. 127). Instead of presenting the text in closed captioning verbatim and making a captioner’s job harder to try to improve the user’s viewing experience, one of the ways designers can make closed captioning more user-friendly is to consider incorporating graphics and animations or animated texts to convey the displayed emotion and sound information (Fels et al., 2008, p. 507).\n\n\n\nThere are two ideas present in the research literature that were discovered on the topic of closed captioning and the deaf and HOH that use graphics and animations or animated texts. The first one is emotive captioning and the second one is kinetic typography. The first idea called emotive captioning is a type of captioning that Branje et al. (2005) describes in their study as the use of a combination of graphics, color, icons and animations, and text to illustrate sound information. It involved using graphics to represent emotion and sound effects and by utilizing a production team to deploy the designed graphical captions by showing very short segments of two different television vignettes from an eight-vignette series titled Burnt Toast: Forever and Ever and Traffic Jammed (Branje et al., 2005, p. 2331). To decide on which graphics to convey sound information, the researchers had to decide what information to convey and how to best express them through graphics, and which graphics are the most appropriate (Branje et al., 2005, p. 2331). The most common emotions they found according to the research literature in order to figure out which graphics and icons to display in the captions were fear, anger, sadness, happiness, disgust, and surprise including two more that members of the production team wanted to add, which were sexy and love (Branje et al., 2005, p. 2331). The production team marked up their script using a Caption Markup tool to tag it with the different emotions and emotional intensities (Branje et al., 2005, 2332). The purpose of doing that is for the script to display a text-based file (Branje et al., 2005, 2332). Then some members of the production team marked up the script some more with their interpretation of the emotive characteristics of the show (Branje et al., 2005, 2332). What doing this does is it allows for their Rendering Engine tool to automatically create graphical pop-on captions using pre-designed image files that are associated with each different emotion selected for the study and allow for captions to be edited with image and text tools consisting of font style, size, and text color (Branje et al., 2005, 2332; see Appendix).\nBranje et al.’s study had eleven participants consisting of six deaf, American Sign Language (ASL) users and five HOH users. To compare data, the researchers had the users complete a pre-test and post-test questionnaire and interview including discussions that took place amongst the users who were recorded watching the two video segments (Branje et al., 2005, p. 2332). Basically, the reactions to the three different versions of both video segments with conventional captions, emotive captions located in one location of the screen, and emotive captions located in different locations on the screen designed to show speaker identification were being compared between the group of deaf users with ASL knowledge and the group of HOH users without ASL knowledge (Branje et al., 2005, 2332).\n\n\n\nThe second idea called kinetic typography is a type of captioning that may use, for example, trembling letters with scratchy typeface to convey a sense of terror, large font sizes to express the emotion associated with screaming, and a small font size to communicate the emotion associated with whispering (Fels et al., 2008, p. 507). Fels et al. (2008) did a study where they experimented with the usage of kinetic typography on twenty-five participants split into two groups, ten hearing (H) users and fifteen HOH users. The kinetic texts used in this study corresponded to five basic emotions: anger, fear, happiness, sadness, and disgust. These emotions were later combined to be turned into a range of fifteen total emotions that were depicted through kinetic texts with an emotional intensity modifier defined as high, medium, or low (Fels et al., p. 508). The different emotions expressed by kinetic texts were presented in two one-minute video segments from two episodes of a children’s show called Deaf Planet titled “Bad Vibes” and “To Air is Human (Fels et al., 2008, p. 509).” The episodes were presented to the participants in three versions: closed caption, enhanced animated text, and extreme animated text with static text shown at the bottom and animated text animated dynamically around the screen (Fels et al., 2008, p. 509; see Appendix). To get the results, the comparison of the data from the three versions of both shows were obtained by having the users complete a pre and post-test questionnaire, having the users complete a checklist, and videotaping the users comments and reactions to the viewing experience (Fels et al., 2008, p. 509).\nBranje et al. (2005) found no significant differences in the users’ responses between the two video segments in their study on emotive captions. But, they did find that emotive captions tended to benefit the HOH users more than the deaf users because HOH users preferred using graphics, icons, and color to represent sound information, whereas, deaf users did not (Branje et al., 2005, 2336). The emotive captions were received positively by HOH users than deaf users in the areas of willingness to engage in conversation with hearing friends about the viewing content, appearance of the face icons in the graphics of the emotive captions, graphical representation of the emotions, and color of the emotive captions (Branje et al., 2005, 2336).\nFels et al. (2008) found a significant difference between the three viewing presentations of captioning in the study on kinetic typography in terms of likeability. Both H and HOH viewers preferred the enhanced animated captions instead of the conventional captions and extreme animated captions. The ratio of positive to negative comments for enhanced captions was higher on the positive side and positive results were higher with enhanced captions concerning the movement of text and portrayal of emotions by the moving text (Fels et al., 2008, p. 516). In terms of identifying the correct emotion being presented through the three different captioning presentations, based on the number of tries one uses in identifying the correct emotion, HOH viewers missed fewer emotions than H viewers (Fels et al., 2008, p.517). Significant differences amongst HOH viewers were found concerning text descriptions for sound effects where most viewers suggested that symbols be used for sound effects instead of text (Fels et al., 2008, p.517). That finding reiterates the findings from Branje et al. (2005) that reported that deaf and HOH viewers like animated symbols for representing sound effects (Fels et al., 2008, p.517). Basically, the results from this first study on the usage of kinetic texts suggests that animated texts is progressing as being the best path forward in helping HOH viewers who are indicating that they want more sound information in closed captioning (Fels et al., 2008, p.517). Despite some of the positive findings on the usage of emotive captioning and kinetic typography as closed captioning alternatives, closed captioning in either its current form or in the mentioned adaptations, especially from the results of the study by Branje et al. (2005), may still not be effective in meeting the needs of deaf users who only sign.\n\n\n\nDeaf and HOH users who only sign need an adaptable form of closed captioning that involves communicating and describing what is being said, what the emotional content is, and whether music, sound, and noise is being played by providing a visual representation of a sign language signer communicating that information. Communicating all that information through sign interfaces has always come with a disadvantage. They have always required a live signer (Hanson, 2009, p. 129). Now, there is video phone technology that exists for deaf and HOH users that allows a signed phone conversation to occur that is as close as it gets for a sign interface to resemble automated closed caption derived from speech recognition technology (Hanson, 2009, p. 131). However, when a live signer is involved everything must be pre-recorded and there is no automatic signed translation of software and web content available for one to rely on if one is looking for signed versions of audio or multimedia that can be created in real time (Hanson, 2009, p. 129).\nOne of the solutions to dealing with this limitation on sign interfaces that has been created is providing an automatic sign presentation called “concatenated signing” where a software program is created by stringing words together in a sentence in order to concatenate signs for each word that were produced by a live signer (Hanson, 2009, p. 129). To ensure a seamless transition from one signed word to the next, since the transition from one video of a live signer to another video of a different live signer can be disjointed, computer algorithms have been created to help smoothen the flow (Hanson, 2009, p.129). Other disadvantages of providing an automatic sign presentation in this manner include the high production cost of creating a professional quality video, the increasing cost of making new videos each time content detail to a website or software changes, the large size of videos for storing and downloading, and the need for a sophisticated server architecture to accommodate streaming content that is making changes at runtime (Glauert et al., 2007). Despite its disadvantages, the automatic sign presentation as described is not necessarily a bad concept, but there is one automatic sign interface concept that is even more promising.\n\n\n\nThe automatic sign interface concept that has the greatest potential to benefit deaf users who only sign is the use of virtual signing avatars (see Appendix). Signing avatars are operated by animation software in which motion data is generated in real time from a scripting notation designed for describing the avatar’s signing (Glauert et al., 2007). The use of signing avatars has six distinct advantages that make it an attractive alternative to sign interfaces on the internet, which have always been presented through video.\nOne is that a user can define the signs through scripting notation and compose sequences of signs on a desktop computer without the need for video or motion capture equipment (Glauert et al., 2007). Second, the continuous flow from sign to sign is very smooth because any piece of signing can be sequenced for any avatar (Glauert et al., 2007). Third, the details of the signing content can be edited without having to rerecord any sequences of signs (Glauert et al., 2007). Fourth, the concern for bandwidth and disk space is very low because, after the software is installed on one’s computer to translate scripting notation to motion data and create the avatar, scripting notation is all that is needed to be transmitted and its data can be much smaller than highly compressed videos or motion data of live signers (Glauert et al., 2007). Fifth, the frame rate concerning graphics and images is less of a concern because data transmitted by the end user can be set to whatever speed the user’s computer is capable of creating the avatar and avatar’s signing (Glauert et al., 2007). Finally, the user has more control over the avatar than with videos of live signers by adjusting the viewing angle of an avatar and switching amongst different types of avatars during playback (Glauert et al., 2007).\nThe use of signing avatars have been used primarily in different countries in the European Union (EU) as part of various projects and initiatives to provide counter clerks with a tool for communicating with deaf customers in post offices, to convert the stream of captioning from the EU’s version of closed captioning called teletext into signing, and to improve usage of broadcast and face-to-face interaction applications on the web for deaf users (Glauert et al., 2007). Although there are always improvements needing to be made to the implementation of signing avatars, based on the evaluations of deaf participants’ comprehension of single signs, signed sentences, and text chunks, the evaluation scores have been high amongst some deaf participants, thus, indicating that the concept including the modifications and adjustments that have been made to improve the presentation of signing avatars is working (Glauert et al., 2007). This is also reflected in the participants appreciation for the existence of signing avatars because they cited the avatars as helpful to those with severe reading difficulties, as offering them more user control of how to use the avatars to meet their needs, and how they could be helpful in giving information to them at train stations, airports, townhalls, hospitals, and travel agencies (Glauert et al., 2007).\n\n\n\nOverall, the summarized results and conclusions from the evaluations of the signing avatars used in the EU show that they are benefitting deaf users who only sign and that the use of emotive captioning and kinetic typography in the studies mentioned show that they benefit HOH users. Although these closed captioning adaptations and alternatives do not benefit their respective users in every area, they still benefit them in certain noticeable areas. The presentation and introduction of closed captioning adaptations and alternatives in this review are not necessarily the only true way to enact them. More research and creative problem-solving should continue to take place to either further validate the findings from these closed captioning adaptations and alternatives, improve upon them, and find more newer ones. Continuing this may help change and loosen some of the strict captioning guidelines set by the US federal government that do not allow much flexibility in creating and providing more closed captioning adaptations and alternatives (Fels et al., 2008, p.506). Meanwhile, countries in the EU have been implementing and providing more closed captioning adaptations and alternatives for a while now (Fels et al., 2008, p. 506).  \n\n\n\nBranje, C., Fels, D. I., Hornburg, M. & Lee, D. G. (2005). Emotive Captioning and Access to\nTelevision. AMCIS 2005 Proceedings, 2330-2337. http://aisel.aisnet.org/amcis2005/300\nFels, D. I., Hunt, R., Vy, Q., & Rashid, R. (2008). Dancing with Words: Using Animated Text\nfor Captioning. International Journal of Human-Computer Interaction, 24(5), 505-519.\nhttps://doi.org/10.1080/10447310802142342\nGlauert, J. R. W. & J. R. Kennaway (2007). Providing Signed Content on the Internet by\nSynthesized Animation. ACM Transactions on Computer-Human Interaction, 14(3).  http://doi.acm.org/10.1145/1279700.1279705\nHanson, V.L. (2009). Computing Technologies for Deaf and Hard of Hearing Users. In A. Sears\n& J. A. Jacko (Eds.), Human-Computer Interaction: Designing for Diverse Users and\nDomains (pp. 125-133). Taylor & Francis Group.\nHearing Loss and Tinnitus Statistics. (2020). Hearing Health Foundation. Retrieved from\nhttps://hearinghealthfoundation.org/hearing-loss-tinnitus statistics#:~:text=48%20million%20people%20in%20America,born%20with%20a%20hearing%20loss.\nHunter, E. (2017, December). A Brief History of Closed Captioning. The Sign Language\nCompany. Retrieved fromhttps://signlanguageco.com/a-brief-history-of-closed-captioning/\nWhat is the Difference Between Open and Closed Captioning. (2019). DO∙IT. Retrieved\nfrom https://www.washington.edu/doit/what-difference-between-open-and-closed-captioning#:~:text=Open%20captions%20always%20are%20in,order%20to%20view%20closed%20captions."
  },
  {
    "objectID": "closed_caption_1.html#closed-captioning-presentation-methods-to-improve-the-viewing-and-communication-experience-of-deaf-and-hard-of-hearing-users",
    "href": "closed_caption_1.html#closed-captioning-presentation-methods-to-improve-the-viewing-and-communication-experience-of-deaf-and-hard-of-hearing-users",
    "title": "HCI 1 Research",
    "section": "",
    "text": "Grant Powell\nSchool of Brain and Behavioral Sciences, University of Texas at Dallas\nACN 6341: Human Computer Interaction I\nProfessor Ericka Orrick\nNovember 24, 2020\n\n\nEver since closed captioning was first debuted in 1971 and first available for limited use in 1972, it has not gone through as much of a transformation to improve upon its ability to further address the needs of the deaf and hard-of-hearing (HOH) (Hunter, 2017). Closed captioning means that text showing what someone on the television is saying is not available the moment one turns on the television, which would be open captioning. It is available only when that person goes to the television settings and turns it on if they realize the need to improve their viewing experience (“What is the Difference Between Open and Closed Captioning,” 2020). Closed captioning was not available to the entire viewing public until 1980 when it was provided for widespread access by allowing deaf and HOH viewers the opportunity to purchase special decoder boxes that could be attached to the television in order to access the closed captioning capabilities. Thanks to the United States (US) Television Decoder Circuitry Act of 1993, decoders are embedded in all televisions that customers purchase in the U.S. (Branje et al., 2005, p. 2330).\nIn the US Hanson (2009) pointed out that the National Institute on Deafness and Other Communication Disorder’s (NIDCD) estimated statistical figures from 2004 showed that 28 million people in the US had some degree of hearing loss. Now, that figure for the US has increased to 48 million people with some degree of hearing loss including 477 million people worldwide according to the most recent estimated statistics available to date (Hearing Health Foundation, 2020).\nClosed captioning is still presented as white text, in a single mono-spaced font and single font size, against a black background on most video platforms with some changes (see Appendix). For example, it has changed some from only all capital letters to mixed case letters, and it sometimes shows a small set of text colors and uses special characters such as music notes to inform the user when noise, sound, or music is being played (Branje et al., 2005, p. 2330). These minor changes are the result of advancements in encoder and decoder technologies from when it first appeared on analogue televisions (Fels et al., 2008, p. 506). With these changes, along with the fact that current closed captioning guidelines in the captioning industry have remained unchanged in the US since captioning was mandated by the Federal Communications Commission, captioners sometimes make creative choices that may not be obvious to the user such as assigning descriptive text to sound effects, paraphrasing long speeches, and omitting information deemed unnecessary to try to improve the user’s viewing experience (Hanson, 2009, p. 128 and Fels et al., 2008, p. 506). However, these changes are not enough to meet the needs of the deaf and HOH population because its needs and range of hearing loss is very diverse.\nThe major issue that separates people with hearing loss on the spectrum of hearing loss ranging from relatively little hearing loss to profound hearing loss is language acquisition skills. Because experiences of difficulty with pitch, timbre, and loudness will occur with hearing loss, speech perception difficulties will not only be visible, but reading and writing skills will inevitably be affected (Hanson, 2009, p. 126). Being able to understand this has implications for interface designers as it relates to closed captioning for the purpose of improving the end users’ video viewing experience either for entertainment, education, or communication purposes. For example, users who are deaf and HOH may have difficulty with reading because of speech perception difficulties. This is especially the case if they lost their hearing at birth or during the pre-lingual stage of child development because reading is based on the overall phonetic structure and understanding of grammar and text comprehension of a spoken language (Hanson, 2009, p. 127). Instead of presenting the text in closed captioning verbatim and making a captioner’s job harder to try to improve the user’s viewing experience, one of the ways designers can make closed captioning more user-friendly is to consider incorporating graphics and animations or animated texts to convey the displayed emotion and sound information (Fels et al., 2008, p. 507).\n\n\n\nThere are two ideas present in the research literature that were discovered on the topic of closed captioning and the deaf and HOH that use graphics and animations or animated texts. The first one is emotive captioning and the second one is kinetic typography. The first idea called emotive captioning is a type of captioning that Branje et al. (2005) describes in their study as the use of a combination of graphics, color, icons and animations, and text to illustrate sound information. It involved using graphics to represent emotion and sound effects and by utilizing a production team to deploy the designed graphical captions by showing very short segments of two different television vignettes from an eight-vignette series titled Burnt Toast: Forever and Ever and Traffic Jammed (Branje et al., 2005, p. 2331). To decide on which graphics to convey sound information, the researchers had to decide what information to convey and how to best express them through graphics, and which graphics are the most appropriate (Branje et al., 2005, p. 2331). The most common emotions they found according to the research literature in order to figure out which graphics and icons to display in the captions were fear, anger, sadness, happiness, disgust, and surprise including two more that members of the production team wanted to add, which were sexy and love (Branje et al., 2005, p. 2331). The production team marked up their script using a Caption Markup tool to tag it with the different emotions and emotional intensities (Branje et al., 2005, 2332). The purpose of doing that is for the script to display a text-based file (Branje et al., 2005, 2332). Then some members of the production team marked up the script some more with their interpretation of the emotive characteristics of the show (Branje et al., 2005, 2332). What doing this does is it allows for their Rendering Engine tool to automatically create graphical pop-on captions using pre-designed image files that are associated with each different emotion selected for the study and allow for captions to be edited with image and text tools consisting of font style, size, and text color (Branje et al., 2005, 2332; see Appendix).\nBranje et al.’s study had eleven participants consisting of six deaf, American Sign Language (ASL) users and five HOH users. To compare data, the researchers had the users complete a pre-test and post-test questionnaire and interview including discussions that took place amongst the users who were recorded watching the two video segments (Branje et al., 2005, p. 2332). Basically, the reactions to the three different versions of both video segments with conventional captions, emotive captions located in one location of the screen, and emotive captions located in different locations on the screen designed to show speaker identification were being compared between the group of deaf users with ASL knowledge and the group of HOH users without ASL knowledge (Branje et al., 2005, 2332).\n\n\n\nThe second idea called kinetic typography is a type of captioning that may use, for example, trembling letters with scratchy typeface to convey a sense of terror, large font sizes to express the emotion associated with screaming, and a small font size to communicate the emotion associated with whispering (Fels et al., 2008, p. 507). Fels et al. (2008) did a study where they experimented with the usage of kinetic typography on twenty-five participants split into two groups, ten hearing (H) users and fifteen HOH users. The kinetic texts used in this study corresponded to five basic emotions: anger, fear, happiness, sadness, and disgust. These emotions were later combined to be turned into a range of fifteen total emotions that were depicted through kinetic texts with an emotional intensity modifier defined as high, medium, or low (Fels et al., p. 508). The different emotions expressed by kinetic texts were presented in two one-minute video segments from two episodes of a children’s show called Deaf Planet titled “Bad Vibes” and “To Air is Human (Fels et al., 2008, p. 509).” The episodes were presented to the participants in three versions: closed caption, enhanced animated text, and extreme animated text with static text shown at the bottom and animated text animated dynamically around the screen (Fels et al., 2008, p. 509; see Appendix). To get the results, the comparison of the data from the three versions of both shows were obtained by having the users complete a pre and post-test questionnaire, having the users complete a checklist, and videotaping the users comments and reactions to the viewing experience (Fels et al., 2008, p. 509).\nBranje et al. (2005) found no significant differences in the users’ responses between the two video segments in their study on emotive captions. But, they did find that emotive captions tended to benefit the HOH users more than the deaf users because HOH users preferred using graphics, icons, and color to represent sound information, whereas, deaf users did not (Branje et al., 2005, 2336). The emotive captions were received positively by HOH users than deaf users in the areas of willingness to engage in conversation with hearing friends about the viewing content, appearance of the face icons in the graphics of the emotive captions, graphical representation of the emotions, and color of the emotive captions (Branje et al., 2005, 2336).\nFels et al. (2008) found a significant difference between the three viewing presentations of captioning in the study on kinetic typography in terms of likeability. Both H and HOH viewers preferred the enhanced animated captions instead of the conventional captions and extreme animated captions. The ratio of positive to negative comments for enhanced captions was higher on the positive side and positive results were higher with enhanced captions concerning the movement of text and portrayal of emotions by the moving text (Fels et al., 2008, p. 516). In terms of identifying the correct emotion being presented through the three different captioning presentations, based on the number of tries one uses in identifying the correct emotion, HOH viewers missed fewer emotions than H viewers (Fels et al., 2008, p.517). Significant differences amongst HOH viewers were found concerning text descriptions for sound effects where most viewers suggested that symbols be used for sound effects instead of text (Fels et al., 2008, p.517). That finding reiterates the findings from Branje et al. (2005) that reported that deaf and HOH viewers like animated symbols for representing sound effects (Fels et al., 2008, p.517). Basically, the results from this first study on the usage of kinetic texts suggests that animated texts is progressing as being the best path forward in helping HOH viewers who are indicating that they want more sound information in closed captioning (Fels et al., 2008, p.517). Despite some of the positive findings on the usage of emotive captioning and kinetic typography as closed captioning alternatives, closed captioning in either its current form or in the mentioned adaptations, especially from the results of the study by Branje et al. (2005), may still not be effective in meeting the needs of deaf users who only sign.\n\n\n\nDeaf and HOH users who only sign need an adaptable form of closed captioning that involves communicating and describing what is being said, what the emotional content is, and whether music, sound, and noise is being played by providing a visual representation of a sign language signer communicating that information. Communicating all that information through sign interfaces has always come with a disadvantage. They have always required a live signer (Hanson, 2009, p. 129). Now, there is video phone technology that exists for deaf and HOH users that allows a signed phone conversation to occur that is as close as it gets for a sign interface to resemble automated closed caption derived from speech recognition technology (Hanson, 2009, p. 131). However, when a live signer is involved everything must be pre-recorded and there is no automatic signed translation of software and web content available for one to rely on if one is looking for signed versions of audio or multimedia that can be created in real time (Hanson, 2009, p. 129).\nOne of the solutions to dealing with this limitation on sign interfaces that has been created is providing an automatic sign presentation called “concatenated signing” where a software program is created by stringing words together in a sentence in order to concatenate signs for each word that were produced by a live signer (Hanson, 2009, p. 129). To ensure a seamless transition from one signed word to the next, since the transition from one video of a live signer to another video of a different live signer can be disjointed, computer algorithms have been created to help smoothen the flow (Hanson, 2009, p.129). Other disadvantages of providing an automatic sign presentation in this manner include the high production cost of creating a professional quality video, the increasing cost of making new videos each time content detail to a website or software changes, the large size of videos for storing and downloading, and the need for a sophisticated server architecture to accommodate streaming content that is making changes at runtime (Glauert et al., 2007). Despite its disadvantages, the automatic sign presentation as described is not necessarily a bad concept, but there is one automatic sign interface concept that is even more promising.\n\n\n\nThe automatic sign interface concept that has the greatest potential to benefit deaf users who only sign is the use of virtual signing avatars (see Appendix). Signing avatars are operated by animation software in which motion data is generated in real time from a scripting notation designed for describing the avatar’s signing (Glauert et al., 2007). The use of signing avatars has six distinct advantages that make it an attractive alternative to sign interfaces on the internet, which have always been presented through video.\nOne is that a user can define the signs through scripting notation and compose sequences of signs on a desktop computer without the need for video or motion capture equipment (Glauert et al., 2007). Second, the continuous flow from sign to sign is very smooth because any piece of signing can be sequenced for any avatar (Glauert et al., 2007). Third, the details of the signing content can be edited without having to rerecord any sequences of signs (Glauert et al., 2007). Fourth, the concern for bandwidth and disk space is very low because, after the software is installed on one’s computer to translate scripting notation to motion data and create the avatar, scripting notation is all that is needed to be transmitted and its data can be much smaller than highly compressed videos or motion data of live signers (Glauert et al., 2007). Fifth, the frame rate concerning graphics and images is less of a concern because data transmitted by the end user can be set to whatever speed the user’s computer is capable of creating the avatar and avatar’s signing (Glauert et al., 2007). Finally, the user has more control over the avatar than with videos of live signers by adjusting the viewing angle of an avatar and switching amongst different types of avatars during playback (Glauert et al., 2007).\nThe use of signing avatars have been used primarily in different countries in the European Union (EU) as part of various projects and initiatives to provide counter clerks with a tool for communicating with deaf customers in post offices, to convert the stream of captioning from the EU’s version of closed captioning called teletext into signing, and to improve usage of broadcast and face-to-face interaction applications on the web for deaf users (Glauert et al., 2007). Although there are always improvements needing to be made to the implementation of signing avatars, based on the evaluations of deaf participants’ comprehension of single signs, signed sentences, and text chunks, the evaluation scores have been high amongst some deaf participants, thus, indicating that the concept including the modifications and adjustments that have been made to improve the presentation of signing avatars is working (Glauert et al., 2007). This is also reflected in the participants appreciation for the existence of signing avatars because they cited the avatars as helpful to those with severe reading difficulties, as offering them more user control of how to use the avatars to meet their needs, and how they could be helpful in giving information to them at train stations, airports, townhalls, hospitals, and travel agencies (Glauert et al., 2007).\n\n\n\nOverall, the summarized results and conclusions from the evaluations of the signing avatars used in the EU show that they are benefitting deaf users who only sign and that the use of emotive captioning and kinetic typography in the studies mentioned show that they benefit HOH users. Although these closed captioning adaptations and alternatives do not benefit their respective users in every area, they still benefit them in certain noticeable areas. The presentation and introduction of closed captioning adaptations and alternatives in this review are not necessarily the only true way to enact them. More research and creative problem-solving should continue to take place to either further validate the findings from these closed captioning adaptations and alternatives, improve upon them, and find more newer ones. Continuing this may help change and loosen some of the strict captioning guidelines set by the US federal government that do not allow much flexibility in creating and providing more closed captioning adaptations and alternatives (Fels et al., 2008, p.506). Meanwhile, countries in the EU have been implementing and providing more closed captioning adaptations and alternatives for a while now (Fels et al., 2008, p. 506).  \n\n\n\nBranje, C., Fels, D. I., Hornburg, M. & Lee, D. G. (2005). Emotive Captioning and Access to\nTelevision. AMCIS 2005 Proceedings, 2330-2337. http://aisel.aisnet.org/amcis2005/300\nFels, D. I., Hunt, R., Vy, Q., & Rashid, R. (2008). Dancing with Words: Using Animated Text\nfor Captioning. International Journal of Human-Computer Interaction, 24(5), 505-519.\nhttps://doi.org/10.1080/10447310802142342\nGlauert, J. R. W. & J. R. Kennaway (2007). Providing Signed Content on the Internet by\nSynthesized Animation. ACM Transactions on Computer-Human Interaction, 14(3).  http://doi.acm.org/10.1145/1279700.1279705\nHanson, V.L. (2009). Computing Technologies for Deaf and Hard of Hearing Users. In A. Sears\n& J. A. Jacko (Eds.), Human-Computer Interaction: Designing for Diverse Users and\nDomains (pp. 125-133). Taylor & Francis Group.\nHearing Loss and Tinnitus Statistics. (2020). Hearing Health Foundation. Retrieved from\nhttps://hearinghealthfoundation.org/hearing-loss-tinnitus statistics#:~:text=48%20million%20people%20in%20America,born%20with%20a%20hearing%20loss.\nHunter, E. (2017, December). A Brief History of Closed Captioning. The Sign Language\nCompany. Retrieved fromhttps://signlanguageco.com/a-brief-history-of-closed-captioning/\nWhat is the Difference Between Open and Closed Captioning. (2019). DO∙IT. Retrieved\nfrom https://www.washington.edu/doit/what-difference-between-open-and-closed-captioning#:~:text=Open%20captions%20always%20are%20in,order%20to%20view%20closed%20captions."
  },
  {
    "objectID": "project.html#powerpoint-of-our-project",
    "href": "project.html#powerpoint-of-our-project",
    "title": "Project",
    "section": "Powerpoint of Our Project",
    "text": "Powerpoint of Our Project"
  },
  {
    "objectID": "closed_caption_2.html",
    "href": "closed_caption_2.html",
    "title": "Speech Perception Research",
    "section": "",
    "text": "Grant Powell\nSchool of Behavioral and Brain Sciences, University of Texas at Dallas\nACN 6763 Speech Perception\nProfessor Peter Assmann\nDecember 13, 2022\n\n\nWhen most people think of the words “closed captioning” they may automatically think that it mainly benefits listeners who are Deaf and hard-of-hearing (HOH). Surprisingly, this could not be further from the truth. While it is true that closed captioning was initially designed for listeners who are Deaf and HOH to improve their listening experience while viewing shows on television, its usage is gradually becoming utilized by listeners who are not Deaf and HOH (Gernsbacher, M. A., 2015).\nThere are many situations where the usage of closed captioning by listeners who are, or not, Deaf and HOH has been implemented. One has been for educational purposes where children and adults are learning a foreign language. A second one has been for healthcare settings where a patient must decide whether to provide informed consent for medical privacy or approval to proceed with some form of treatment. A third one has been for air travel where pilots must listen to the radio for important instructions for air traffic control. Finally, a fourth one has been for telecommunication purposes where people are having to converse through a phone or teleconference medium or view recorded media.\nIn most of these situations, closed captioning is there to serve the purpose of alleviating problems with speech intelligibility, which may, broadly, refer to various levels of intelligibility involving comprehension, recognition, identification, and recollection of speech. Although, in the case of language learning, speech intelligibility through the help of closed captioning may aid in the production of speech such as pronunciation or articulation (Gernsbacher, M. A., 2015). Lately, closed captioning has become, somewhat, more accessible in adverse speech situations to not only Deaf and HOH listeners.\nThere has been an increase in the implementation of captioning software, which have broadly been identified under various names such as speech or voice recognition technology or real-time speech-to-text software. This technology has revealed itself quite a bit on many telecommunication devices such as laptops, mobile phones, and electronic tablets for teleconferencing, recording lectures and meetings for notetaking, or viewing recorded media. Closed captioning is not as enclosed and hidden from public visibility, hence the name closed captioning, as it often was when watching television back in the 1980s and 1990s.\nIt used to be, during those times, that one would have to maneuver a convoluted set of menu options on the remote control or television to find access to it and turn it on. Furthermore, when closed captioning was first introduced, and before the availability and advancement of captioning software, it usually required a person who was a captioner to transcribe and edit the text that was to be provided for television shows, beforehand. Nowadays, it can be found easily on recorded media if the video someone is watching has closed captioning included, or closed captioning technology embedded within it, by searching for the “CC” icon on the screen of the video and clicking it. According to captioning companies such as 3PlayMedia, providing closed captioning through captioning software has been considered more cost effective amongst most companies than having to hire an outside captioning service where a team of captioners do extensive editing and transcribing (Klein, R., 2022). Despite some of the progress that has been made in ensuring accessibility to closed captioning, it is still not as accessible to everyone as it should be.\nBecause most people still consider closed captioning to be intended mainly for listeners who are Deaf and HOH, the benefits of closed captioning for non-Deaf and HOH listeners are not as well understood because empirical evidence has often been published across separate topics such as Deaf education, second-language learning, adult literacy, and reading acquisition instead of being standardized for better public visibility from a publication perspective (Gernsbacher, M. A., 2015). This is one reason why access to closed captioning is still lacking. Another reason is that many video and content creators are still naïve about the legal government mandates requiring closed captioning and the empirical benefits that come with closed captioning for listeners who are, or not, Deaf and HOH (Gernsbacher, M. A., 2015). Since most closed captioning, as mentioned earlier, is generated automatically through captioning software such as speech recognition technology (SRT) instead of being transcribed, solely, through an outside captioning service, the gap between inaccessibility and accessibility to closed captioning has lessened to an extent.\nDue to that convenience of using captioning software at the expense of saving costs, the quality of closed captioning has been lacking because, while captioning software has made significant gains and has been viewed as a more cost-effective option, it is still not on the same level as captions transcribed and edited by outside captioning services in terms of presentation accuracy and clarity (Gernsbacher, M. A., 2015). However, the captioning software, Real-Time Text (RTT), has made enough gains to where it has been fully embraced by the Federal Communications Commission (FCC) as a permanent replacement for teletypewriter (TTY) (Tinio, R. F., 2018).\n\n\n\nBecause the FCC has decided to transition from TTY to RTT on a permanent basis, it has been the hope of deaf pilots and pilots of the Deaf Pilots Association that RTT will make its way into everyday radio communications between pilots and air traffic controllers (ATCs) (Tinio, R. F., 2018). This has been the hope for quite a long time because when deaf pilots get their pilot certificate, granting them legal permission to fly, privately, they can only land and takeoff from airports that do not have control towers or require radio communication (Tinio, R. F., 2018). On their certificate, it states it is “Not Valid for Flights Requiring the Use of Radio” (Tinio, R. F., 2018). Before control towers were built and radio communication was required in the early 1930s, due to significant increase in air traffic and growing unreliability of alternative communication methods, deaf pilots were able to fly, freely, by relying on visual flying rules (VFR) (Tinio, R. F., 2018). VFR required that pilots fly in appropriate weather conditions where there is sufficient daylight clear of clouds and to stay clear of clouds at a specific distance to ensure they can be able to see and avoid other airplanes (Tinio, R. F., 2018). Along with VFR, deaf pilots were also able to fly, freely, by communicating with ATCs using colored flags and light gun signals (Tinio, R. F., 2018). The limitation placed on their certificates is not entirely discouraging as there are more than 18,000 airports in the United States that are uncontrolled and 512 airports that have control towers, but it still makes it a little bit difficult to plan their flights, ahead of time (Tinio, R. F., 2018).\nThey must make sure to avoid certain airports, areas, or airspaces that require radio communication, but if they want to land or take off from an airport that has a control tower and requires radio communication, they must either have a co-pilot or certified flight instructor (CFI) who can handle radio communications or use light gun signals by planning ahead with an ATC (Tinio, R. F., 2018). Despite certified deaf pilots being unable to fly into airports with control towers requiring radio communication without the assistance of a co-pilot and CFI, there are changes being made to the future mode of radio communication that may soon change their circumstances.\n The Federal Aviation Administration (FAA) has been making, or has made, a transition from the old, most recent, mode of radio communication to a newer mode of radio communication with a Data Communications (Data Comm) system that is part of the Next Generation Air Transportation System (NextGen) program (Tinio, R. F., 2018). NextGen was reported in the news by the National Public Radio (NPR) back in 2016 (Naylor, B., 2016). Data Comm is expected to rectify communication issues between pilots and ATCs such as those being lengthy due to procedures and causing comprehension difficulties and errors from miscommunication (Tinio, R. F., 2018). The procedures for communication between pilots and ATCs often involve identifying the sender and receiver of the message, comprehending what was said and received in the instructions, and reading back the instructions to ensure they were understood and received (Tinio, R. F., 2018). Sometimes, errors can occur from the communication procedures through miscommunication due to language barriers, various accents, and cultural differences in the presentation of the instructions and, thus, be detrimental to flight safety (Tinio, R. F., 2018). It is unsure if all air traffic control instructions, traffic advisories, and weather information will be transmitted to the cockpit, digitally, with technological projects such as an aeronautical datalink and ADS-B (automatic dependent surveillance broadcast), which have been tested in commercial jets and expected to be included in general aviation aircraft within the next decade (Tinio, R. F., 2018). But when NPR first reported about NextGen and talked about Data Comm, it reported on one specific feature of Data Comm.\nIt reported on a text messaging system for all air traffic communication to improve communication between hearing pilots and ATCs (Naylor, B., 2016). How it works is that ATCs and pilots exchange information, electronically, by texting each other (Naylor, B., 2016). This is like closed captioning, but with differences in the method of presentation and delivery. While most pilots are not deaf and HOH, this was a very essential policy measure to implement, especially, considering air traffic control towers at most major metropolitan airports can be very chatty places.\nThe chatter on the radio can affect an aircraft pilot’s speech intelligibility, as mentioned earlier, because there are various numbers, phrases, and words commonly found in pilot-speak contained within the instructions received from ATCs designed to efficiently spell out the routes pilots need to get to their destinations (Naylor, B., 2016). What makes a text messaging system even more vital for situations like this is that if a pilot mishears the instructions, they received from the ATC after carefully reading back what was heard, then the pilot must go through the process all over again (Naylor, B., 2016). Route information must always be communicated to pilots by ATCs while a plane is on the taxiway because changing weather patterns involving, say, thunderstorms immediately showing up on the radar can cause the anticipated route to be changed before receiving clearance to takeoff (Naylor, B., 2016). Apart from flight safety concerns due to miscommunication, this can also end up taking several minutes by delaying departures, burning fuel, and emitting carbon dioxide emissions while sitting on the taxiway waiting to takeoff (Naylor, B., 2016). Pilots and administrators have noted the benefits of this text messaging system.\nAccording to NPR, a United Parcel Service (UPS) pilot of a Boeing 767 has claimed that he is able to process route clearances, route changes, and frequency changes through the text messaging system (Naylor, B., 2016). Since him and other pilots normally have gotten most of their messaging through voice on a radio frequency while taxiing towards the runway, they are now able to get their messages in a text-message-like format on a screen near the center of the console of the cockpit (Naylor, B., 2016). This allows him and his colleagues to view the message and reprogram their computers in seconds, which used to take minutes to process when they received messages by voice (Naylor, B., 2016). This benefit is very important because he pointed out that when receiving messages by voice it would get to a point where at a busy metropolitan airport there would be 30 or 40 airplanes waiting in line during a weather event (Naylor, B., 2016). Add in a couple minutes comprehending the messages multiplied times 40 aircraft and it, easily, led to over an hour in delays (Naylor, B., 2016). The text messaging system is expected to cut down on traveling delays because an Assistant FAA Administrator said it should cut down on situations where passengers have missed their connecting flight or the shoes someone ordered did not arrive on time by making the whole system feel more stable and predictable and not have to sit on a plane wondering what is going on and when someone will get off the plane upon arrival (Naylor, B., 2016). Another area where difficulty communicating occurs not only among Deaf and HOH listeners, but also non-Deaf and HOH listeners, is in medical settings.\n\n\n\nSpehar et al. (2016) conducted a study that looked at how effective speech-to-text (STT) software could be in patient-to-physician encounters and what benefits it could provide in helping patients when making informed consent decisions. This was done by determining if STT could improve patients’ recall of their informed consent decision-making and whether this approach could improve physicians’ communication skills, professionally, and for training purposes with patients, by conducting a post-assessment analysis of the transcripts of their conversations with them (Spehar et al, 2016). One of the reasons they tested this is because enough advancements in STT have been made for it to be used in physician-to-patient encounters, especially, in anesthesiology where regional anesthesia and its associated risks that comes with certain anesthesia techniques are not well understood by patients (Spehar et al, 2016). A second reason is that the real-time captioning that STT provides has the potential capability of improving Deaf and HOH listeners’ abilities to comprehend and later recall conversations with physicians in other medical disciplines besides anesthesiology (Spehar et al, 2016). Finally, a third reason is that speech-to-text devices that incorporate STT could be used to improve comprehension in Deaf and HOH listeners, in general, and that its benefits are probably most readily available to be implemented in medical settings from a policy perspective (Spehar et al, 2016). The results of the researchers’ study led to some insightful conclusions they made about STT as it applies to patient-to-physician encounters in medical settings.\nThe study’s small sample consisted of twelve older adult subjects with hearing loss, and without STT, they performed just as worse as younger subjects in previous studies in retrieving the information presented to them by their physicians (Spehar et al, 2016).  But, with STT, the researchers found that the subjects performed better at recalling the information presented by their physicians (Spehar et al, 2016).  Despite recall being much poorer than recognition, the patients were able to recall nearly half again as many key items from the information presented to them with the assistance of STT than without it (Spehar et al, 2016).\nThis shows that STT can substantially benefit Deaf and HOH listeners’ ability to recall information such as the kind presented to them by a physician in a medical setting by acting as a valuable supplement to the spoken word (Spehar et al, 2016).  The reasoning is that for listeners who are, or not, Deaf and HOH the text of a physician’s most recent utterance can function as a short-term memory store when trying to sort through the confusion that comes with understanding unfamiliar medical terms and concepts (Spehar et al, 2016). The researchers believed that this function is the reason why the Deaf and HOH subjects in their study benefitted, significantly, from STT (Spehar et al, 2016). Although the study had a small sample size and fewer patient-to-physician encounters, a controlled, simulated environment instead of a clinical setting, and subjects who volunteered who may not necessarily be fully representative of Deaf and HOH patients encountered in a clinical setting, the researchers still stood by their findings including other findings they made that could be applied to other areas of patient-to-physician encounters that do not necessarily have to involve Deaf and HOH listeners (Spehar et al, 2016).\nThere are two other areas where the application of STT could be beneficial in patient-to-physician encounters. One area is with physicians from a physician education perspective (Spehar et al, 2016). The transcripts that are automatically produced by STT could be used by physicians to evaluate how well they communicated with patients. This is considering that from the results of the study the physicians were more enthusiastic about the use of captioning than the subjects were in patient-to-physician encounters with average scores of 5.1 and 5.5 by the physicians and 4.1 and 3.5 by the subjects on post-encounter questionnaire items (Spehar et al, 2016). The physicians’ ability to read what they said on the screen of the device utilizing STT not only allowed them to correct any errors made in the transcriptions by the device, but it also allowed opportunities for them to reflect upon what they said and how they said it to help guide their conversations with the patients and hone their communication skills (Spehar et al, 2016). A second area is with patients who speak different languages and have difficulty communicating with a physician (Spehar et al, 2016). Language translation software has improved enough to where it could be used between two different speakers, the patient and physician, who each speak a different language by providing simultaneous translation between speakers of different languages through the utilization of STT (Spehar et al, 2016). However, this potential benefit to speakers with different languages, or communication barriers, does not necessarily have to be specific to medical settings it could also extend into another area, educational settings.   \n\n\n\nIn educational settings, captions can be used to benefit language learners, children, or adults, who are learning either a primary or secondary language (Gernsbacher, M. A., 2015). Captions provide opportunities for language learners to analyze the words that appear on the screen as they are aurally processing the language by using the visualization of the words to determine linguistic units such as phonemes, morphemes, lexemes, syntax, and context along with grammar, semantics, and pragmatics (Gass et al, 2019). What this does is it helps the learners map the aural speech stream to individual, or meaningful, words and phrases (Gass et al, 2019). Gass et al (2019) investigated in their past studies whether second language (L2) learners utilized captions as a tool for listening comprehension in a classroom setting.\nThey found that captions helped them with listening comprehension by allowing them to segment speech streams into meaningful components (Gass et al, 2019). They extended the findings from those studies through a new study titled “How Captions Help People Learn Languages: A Working Memory, Eye-tracking Study” to investigate how captions helped with the L2 learners’ listening comprehension by investigating how individual differences influence the use of captions through an in-depth look into their working memory (WM) capacities (Gass et al, 2019).  It investigated two populations of learners, English as a second language (ESL) and English speakers learning Spanish (Spanish L2 learners), by observing for common patterns that emerged in their language-learning (Gass et al, 2019). Its results provide insights into the benefits that captions for language learning may provide and best practices and recommendations when implementing them for that purpose.\nGass et al (2019) found that the results answered the first research question of the study - “Does captioning aid comprehension?” - by showing that captioning did promote L2 learners’ video comprehension, thus, supporting the researchers’ past work. They also found that in answering the second research questions of the study - “What is the relationship between WM and L2 video comprehension? Do learners with high WM capacity comprehend more than learners with low WM capacity?” - the results showed that there was little effect of WM and L2 video comprehension for the Spanish L2 learners, but a medium effect of WM and L2 video comprehension for the ESL learners.\nThis may have been influenced by the possibility that the proficiency levels of the ESL learners were higher than the Spanish L2 learners because the recall scores of the ESL learners from a free-recall task following the watching of a short, captioned video were higher than the Spanish L2 learners (Gass et al, 2019). Gass et al (2019) theorized that there is a certain level of proficiency, or proficiency threshold, that must be reached before WM can play a factor in influencing the video comprehension of L2 learners. Basically, WM capacity does not seem to differ among certain types of learners when their proficiency level is lower because the effort in comprehending is going to be lower at a very basic level consisting of a word-to-word interpretation and it may be difficult to put individual words together to create a meaningful stream of speech (Gass et al, 2019). In examining the results to answer their third research questions - “What is the relationship between WM and caption-reading behavior? Are there differences in caption-reading behavior between learners with high WM and learners with low WM?” - Gass et al (2019) found interesting differences based on first-time (T1) viewing versus second time (T2) viewing of watching a video in a L2.\nDuring T2 viewing, the reading behavior of the Spanish L2 and ESL learners was similar, however, the high WM groups reduced their caption reading time and the low-WM groups increased their reading time (Gass et al, 2019). To clarify, the researchers identified those learners in the Spanish L2 learners’ group for experiment one and ESL learners’ group for experiment two who performed better on the free-recall test and assigned them to the high WM group, whereas the rest of the learners were assigned to the low WM group of each experiment (Gass et al, 2019). Gass et al (2019) decided that they could not conclude with confidence of any correlation from this finding, but, instead, suggested that this was most likely the result of individual differences accounting for variation in information-processing during multimedia learning. For them to conclude with confidence that group differences were occurring, they would have needed a much larger sample size to help reveal greater differences between those with low and high WM capacity (Gass et al, 2019). In examining their results to answer the fourth research question - “What is the relationship between caption-reading behavior and L2 video comprehension? Are there differences in caption-reading behaviors between learners who demonstrate high video-comprehension and learners who demonstrate low video-comprehension?” - Gass et al (2019) found that language immersion may have played a role in the differences in results between the two learning groups.\nThey found that language immersion may have been the deciding factor in why the ESL learning group had fewer differences in how much time was spent reading the captions when watching the video between the high – and low – comprehension groups during T1 viewing versus T2 viewing (Gass et al, 2019).  Mainly, because the ESL learning group had spent more time gaining experience learning how to speak English in an English-speaking country and had developed a certain level of proficiency (Gass et al, 2019). Whereas, on the other hand, the Spanish L2 learning group had not and showed differences in time spent reading the captions amongst the two comprehension learners. The high-comprehension learners spent less time, overall, reading captions during T1 viewing and less time rereading than the low-comprehension learners (Gass et al, 2019). It was most likely that the high-comprehension learners were possibly depending more on the audio or from the visuals of the video than from the captions (Gass et al, 2019). As a result, the findings from the results that helped answer this fourth research question including the other three led to more findings that give insights into best practices and recommendations for implementing captions for L2 learning.\nBased on the results, Gass et al (2019) recommends that it is best to expose L2 learners to captions when watching videos in a L2 because giving them more experience watching videos with the captions allows them to develop the ability to efficiently split their attention among multiple modes of input consisting of captions, audio, and other visual input. Having this balanced approach to using multiple sources of input will help enable them to parse and understand the incoming speech stream and aid in their learning (Gass et al, 2019). However, it is still important to consider a learner’s WM capacity and L2 proficiency level relative to a video’s content when implementing captions (Gass et al, 2019).\nWhile L2 learners regardless of L2 proficiency used captions and those with higher WM, proficiency, and comprehension still read them, L2 learners with lower WM, proficiency, and comprehension tended to benefit more from using the captions because it acted more as a helpful processing aid relative to the difficulty level of the video (Gass et al, 2019). Captions acted in this way by helping those types of L2 learners with attentional control by acting as a useful salient, attention-grabbing piece of written information when aural information became too difficult and inaccessible for learning (Gass et al, 2019). L2 learners with higher WM capacity used the captions less than those with lower WM capacity, most notably, during T2 viewing (Gass et al, 2019). The reason is that they may have been able to hold more key information effectively in the episodic buffer store of their WM during T1 viewing and, thus, already were able to glean enough needed information from the captions the first time around (Gass et al, 2019). This is important to take note of when implementing captions as a L2 learning aid because, while captions most certainly aid comprehension, they must be chosen carefully by an educator to make sure they match the learners’ proficiency level (Gass et al, 2019). Otherwise, if a learner’s proficiency, comprehension, and WM is very high, then providing captions could end up, eventually, serving more as a nuisance (Gass et al, 2019). Besides learning languages in educational settings, another area where the implementation of captions is vital for Deaf and HOH and, occasionally, non-Deaf and HOH listeners is daily communication.    \n\n\n\nThe implementation and availability of closed captioning is essential to Deaf and HOH listeners for daily communication because communicating through technology has become more normalized (Zhong et al, 2022). As a result, the challenges of participating in telecommunication involving conversing on the telephone and teleconference platforms, and viewing recorded media, are growing because conversing on the telephone has become important for staying in contact with friends and family, for scheduling medical appointments, and participating in telehealth (Zhong et al, 2022). The novel coronavirus (COVID-19) pandemic has exacerbated these growing challenges, even more, through the increase in demand for teleconferencing by enforcing orders and recommendations to reduce in-person gatherings (Zhong et al, 2022). While it is very important to reduce the risk of infection and improve productivity by making work and communication between people more efficient, it may present communication difficulties for Deaf and HOH listeners, in general, and non-Deaf and HOH listeners in noisy environments. This presents a need for the implementation and availability of closed captioning including improvements in its delivery and presentation for daily communication (Zhong et al, 2022).\nIn a systematic review by Zhong et al (2002), it was found that the results of ten studies showed that text captions added to auditory signals has the potential to help listeners who are, or not, Deaf and HOH understand speech, which was consistent with the demonstrated benefits of providing text captions as asynchronous feedback for recognition of distorted speech. This supports the need to implement and provide availability and access to closed captioning including the findings that the benefits of text captions are both greater when the auditory signal integrity is low and in lower auditory conditions when the auditory integrity was manipulated (Zhong et al, 2022). However, the benefits of closed captioning for daily communication have been found to be very beneficial to older adults with hearing loss who are representing a growing population of listeners with hearing loss because the benefits of text captions do not seem to vary based on listener age (Zhong et al, 2022).\nAlthough older adults with hearing loss are less able to benefit from visual speech information, also known as speech-or lip-reading, than younger adults, older adults benefit from text captions because it is believed that older adults benefit from the additional context that text captions provide (Zhong et al, 2022). Existing evidence has shown that older adults can benefit from context more so than younger adults (Zhong et al, 2022). Since older adults tend to visit and consult with medical personnel more often as they get older, the benefits of text captions that have been found for older adults is important. This is because it is suggested that the implementation of text captions in teleconferencing applications for telehealth appointments could, significantly, benefit those who have hearing loss who might have more trouble communicating in a telephone appointment or video-conferencing session due to reduced audibility (Zhong et al, 2022). However, there is still room for improvement in maximizing the benefits of closed captioning for daily communication in listeners who are, or not, Deaf and HOH through improvements in its delivery and presentation.\nWhen Deaf and HOH listeners are communicating through telecommunication with the help of text captioning, certain text characteristics such as text integrity and timing, when presented and delivered efficiently, may enhance the benefits of text captions (Zhong et al, 2022). It has been found that as the delay time increased while text captions were being presented in response to a speech signal, the text caption benefit decreased (Zhong et al, 2022). This highlights the importance of fast automated speech-to-text algorithms when implementing text captions for telecommunication including policy changes in governmental guidelines that would help provide better guidance for creators of these algorithms (Zhong et al, 2022). Mainly, because with the FCC’s guidelines for text captions set at a recommended 125 words per minute for telephones with 98% accuracy, there have been some reports that the estimated speaking rate of conversations has been able to reach as high as 320 words per minute (Zhong et al, 2022). If speech-to-text algorithms are, mainly, set to meet the FCC guidelines by presenting text captions at 125 words per minute, then it would result in text caption delays relative to the auditory signal in those speaking situations that have a speaking rate higher than 125 words per minute, thus, reducing the benefits of text captions (Zhong et al, 2022).  \n\n\n\nOverall, it is the hope that this review gives a general idea of how closed captioning and its variants such as RTT and STT has been and can be implemented in air travel, medical, educational, and daily communication settings to benefit listeners who are, or not, Deaf and HOH by alleviating problems with speech intelligibility. Moreover, it is also the hope that it provides readers in medical, educational, and daily communication settings with a guideline on some best practices and recommendations for implementing closed captioning and its variants in those settings. Before ending this review, some final notes need to be addressed on some of the details in the review’s content.\nThere either needs to be experimental research, or more of it, that is more accessible to the public that shows the benefits of the text messaging system of NextGen’s Data Comm system. There is a good possibility that there has been quite a bit of research on various features of the Data Comm system because, otherwise, this system as part of the NextGen program probably would not have been approved for use as a newer, more improved mode of radio communication by the FAA. After doing some digging on the FAA’s website and coming up empty, mostly, with a focus on looking for research studies done on the text messaging system of the Data Comm system, most of the evidence on the benefits of the text messaging system that could be found, so far, were mostly anecdotal evidence from reputable news sources such as NPR. Because this is all related to air travel, it would also be interesting to see experimental research, or more of it, if it exists, on the benefits of closed captioning, not necessarily, on television for watching entertainment in airports or airplanes because we have enough of that research as it applies to other settings, but on television or live screen devices that are geared towards helping listeners who are, or not, Deaf and HOH be informed about various changes to their flights such as gate changes, sudden emergencies, or other important announcements at airports that can make air travel less stressful (Collaborative for Communication Access via Captioning, 2016). The reason for this is because, according to 3PlayMedia, the United States Department of Transportation (DOT) issued a ruling that went into effect in 2015, which was an amendment to Section 504 of the Rehabilitation Act of 1973, requiring closed captioning on all televisions in US airports that receive federal funding and experience at least 10,000 flights annually (Griffin, E., 2015). Besides air travel, experimental research is also needed on the benefits of closed captioning in another area of medical settings, the operating room.\nThere needs to be experimental research, or more of it, if it exists, on the benefits of closed captioning in operating rooms of medical settings where surgeries or operations are planned. This is because medical personnel must wear masks in these situations and helpful audiovisual cues such as speech-and lip-reading that can, sometimes, be helpful for deciphering speech when communicating are non-existent. There have been instances appear in the news or news articles, occasionally, where Deaf and HOH medical students or personnel fulfill their medical training or perform their job duties in the operating room and utilize text captions on a television screen showing what each person in the operating room is saying. After all, communication in operating rooms is critical and necessary to avoid medical errors and mistakes. Finally, more final clarification is needed to address how research on closed captioning benefits learners learning their primary language, as mentioned in the section on closed captioning in educational settings, especially, in regard to reading skills.\nThe implementation of closed caption has been found to serve the same purpose in benefitting children’s reading skills in their primary language in much the same way as it does for L2 learners learning a L2 because learning to read also involves mapping sound and meaning onto text (Gernsbacher, M. A., 2015). It has, evidently, been found that watching videos with both audio and text captions can improve reading skills by helping non-Deaf and HOH children define content words heard in the videos, pronounce novel words, recognize vocabulary items, and make sense of what is happening in the videos (Gernsbacher, M. A., 2015). This has the potential benefit of growing and expanding the vocabulary of those young listeners who are not Deaf and HOH. Lastly, in India, after researchers encouraged India’s national television to begin captioning Bollywood music videos, it was found in a study assessed over a certain period that the literacy of adults who watched the videos increased more than those who very rarely or did not watch them (Gernsbacher, M. A., 2015).   \n\n\n\nCollaborative for Communication Access via Captioning (2016). Captioning in transportation –\nAir travel and more. http://ccacaptioning.org/captioning-transportation/\nGass, S., Winke, P., Isbell, D. R., & Ahn, J. (2019). How captions help people learn languages:\nA working-memory, eye-tracking study. Language Learning & Technology, 23(2), 84-\n104. https://doi.org/10125/44684\nGernsbacher, M. A. (2015) Video captions benefit everyone. Policy Insights Behav. Brain Sci.,\n2(1), 195-202. https://doi.org/10.1177/2372732215602130\nGriffin, E. (2015, August 10). US DOT officially requires closed captioning on airport TVs.\n3PLAYMEDIA. https://www.3playmedia.com/blog/us-dot-officially-requires-closed-\ncaptioning-on-airport-tvs/\nKlein, R. (2022, July 25). What’s the true price of closed captioning services? 3PLAYMEDIA.\nhttps://www.3playmedia.com/blog/how-much-does-closed-captioning-service-cost/\nZhong, L., Noud, B. P., Pruitt, H., Marcrum, S. C., & Picou, E. M. (2022) Effects of text\nsupplementation on speech intelligibility for listeners with normal and impaired hearing:\nA systematic review with implications for telecommunication. International Journal of\nAudiology, 61(1), 1-11. https://doi.org/10.1080/14992027.2021.1937346\nNaylor, B. (2016, October 3). Air traffic controllers and pilots can now communicate\nelectronically. NPR. https://www.npr.org/sections/alltechconsidered/2016/10/03/4963937\n87/air-traffic-controllers-and-pilots-can-now-communicate-electronically\nPayne, B. R., Silcox, J. W., Crandell, H. A., Lash, A., Ferguson, S. H., & Lohani, M. (2022) Text\ncaptioning buffers against the effects of background noise and hearing loss on memory\nfor speech. Ear & Hearing, 43(1), 115-127. https://doi.org/10.1097/AUD.000000000000    1079\nSpehar, B., Tye-Murray, N., Myerson, J., & Murray D. J. (2016) Real-time captioning for\nimproving informed consent: Patient and physician benefits. Reg. Anesth. Pain Med.,\n41(1), 65-68. https://doi.org/10.1097/AAP.0000000000000347\nTinio, R. F. (2018). Perceiving the communication methods between deaf pilots and air traffic\ncontrol (Publication No. 1464) [Doctoral dissertation, Purdue University]. Open Access\nTheses"
  },
  {
    "objectID": "closed_caption_2.html#introduction",
    "href": "closed_caption_2.html#introduction",
    "title": "Speech Perception Research",
    "section": "",
    "text": "When most people think of the words “closed captioning” they may automatically think that it mainly benefits listeners who are Deaf and hard-of-hearing (HOH). Surprisingly, this could not be further from the truth. While it is true that closed captioning was initially designed for listeners who are Deaf and HOH to improve their listening experience while viewing shows on television, its usage is gradually becoming utilized by listeners who are not Deaf and HOH (Gernsbacher, M. A., 2015).\nThere are many situations where the usage of closed captioning by listeners who are, or not, Deaf and HOH has been implemented. One has been for educational purposes where children and adults are learning a foreign language. A second one has been for healthcare settings where a patient must decide whether to provide informed consent for medical privacy or approval to proceed with some form of treatment. A third one has been for air travel where pilots must listen to the radio for important instructions for air traffic control. Finally, a fourth one has been for telecommunication purposes where people are having to converse through a phone or teleconference medium or view recorded media.\nIn most of these situations, closed captioning is there to serve the purpose of alleviating problems with speech intelligibility, which may, broadly, refer to various levels of intelligibility involving comprehension, recognition, identification, and recollection of speech. Although, in the case of language learning, speech intelligibility through the help of closed captioning may aid in the production of speech such as pronunciation or articulation (Gernsbacher, M. A., 2015). Lately, closed captioning has become, somewhat, more accessible in adverse speech situations to not only Deaf and HOH listeners.\nThere has been an increase in the implementation of captioning software, which have broadly been identified under various names such as speech or voice recognition technology or real-time speech-to-text software. This technology has revealed itself quite a bit on many telecommunication devices such as laptops, mobile phones, and electronic tablets for teleconferencing, recording lectures and meetings for notetaking, or viewing recorded media. Closed captioning is not as enclosed and hidden from public visibility, hence the name closed captioning, as it often was when watching television back in the 1980s and 1990s.\nIt used to be, during those times, that one would have to maneuver a convoluted set of menu options on the remote control or television to find access to it and turn it on. Furthermore, when closed captioning was first introduced, and before the availability and advancement of captioning software, it usually required a person who was a captioner to transcribe and edit the text that was to be provided for television shows, beforehand. Nowadays, it can be found easily on recorded media if the video someone is watching has closed captioning included, or closed captioning technology embedded within it, by searching for the “CC” icon on the screen of the video and clicking it. According to captioning companies such as 3PlayMedia, providing closed captioning through captioning software has been considered more cost effective amongst most companies than having to hire an outside captioning service where a team of captioners do extensive editing and transcribing (Klein, R., 2022). Despite some of the progress that has been made in ensuring accessibility to closed captioning, it is still not as accessible to everyone as it should be.\nBecause most people still consider closed captioning to be intended mainly for listeners who are Deaf and HOH, the benefits of closed captioning for non-Deaf and HOH listeners are not as well understood because empirical evidence has often been published across separate topics such as Deaf education, second-language learning, adult literacy, and reading acquisition instead of being standardized for better public visibility from a publication perspective (Gernsbacher, M. A., 2015). This is one reason why access to closed captioning is still lacking. Another reason is that many video and content creators are still naïve about the legal government mandates requiring closed captioning and the empirical benefits that come with closed captioning for listeners who are, or not, Deaf and HOH (Gernsbacher, M. A., 2015). Since most closed captioning, as mentioned earlier, is generated automatically through captioning software such as speech recognition technology (SRT) instead of being transcribed, solely, through an outside captioning service, the gap between inaccessibility and accessibility to closed captioning has lessened to an extent.\nDue to that convenience of using captioning software at the expense of saving costs, the quality of closed captioning has been lacking because, while captioning software has made significant gains and has been viewed as a more cost-effective option, it is still not on the same level as captions transcribed and edited by outside captioning services in terms of presentation accuracy and clarity (Gernsbacher, M. A., 2015). However, the captioning software, Real-Time Text (RTT), has made enough gains to where it has been fully embraced by the Federal Communications Commission (FCC) as a permanent replacement for teletypewriter (TTY) (Tinio, R. F., 2018)."
  },
  {
    "objectID": "closed_caption_2.html#closed-captioning-usage-by-airplane-pilots",
    "href": "closed_caption_2.html#closed-captioning-usage-by-airplane-pilots",
    "title": "Speech Perception Research",
    "section": "",
    "text": "Because the FCC has decided to transition from TTY to RTT on a permanent basis, it has been the hope of deaf pilots and pilots of the Deaf Pilots Association that RTT will make its way into everyday radio communications between pilots and air traffic controllers (ATCs) (Tinio, R. F., 2018). This has been the hope for quite a long time because when deaf pilots get their pilot certificate, granting them legal permission to fly, privately, they can only land and takeoff from airports that do not have control towers or require radio communication (Tinio, R. F., 2018). On their certificate, it states it is “Not Valid for Flights Requiring the Use of Radio” (Tinio, R. F., 2018). Before control towers were built and radio communication was required in the early 1930s, due to significant increase in air traffic and growing unreliability of alternative communication methods, deaf pilots were able to fly, freely, by relying on visual flying rules (VFR) (Tinio, R. F., 2018). VFR required that pilots fly in appropriate weather conditions where there is sufficient daylight clear of clouds and to stay clear of clouds at a specific distance to ensure they can be able to see and avoid other airplanes (Tinio, R. F., 2018). Along with VFR, deaf pilots were also able to fly, freely, by communicating with ATCs using colored flags and light gun signals (Tinio, R. F., 2018). The limitation placed on their certificates is not entirely discouraging as there are more than 18,000 airports in the United States that are uncontrolled and 512 airports that have control towers, but it still makes it a little bit difficult to plan their flights, ahead of time (Tinio, R. F., 2018).\nThey must make sure to avoid certain airports, areas, or airspaces that require radio communication, but if they want to land or take off from an airport that has a control tower and requires radio communication, they must either have a co-pilot or certified flight instructor (CFI) who can handle radio communications or use light gun signals by planning ahead with an ATC (Tinio, R. F., 2018). Despite certified deaf pilots being unable to fly into airports with control towers requiring radio communication without the assistance of a co-pilot and CFI, there are changes being made to the future mode of radio communication that may soon change their circumstances.\n The Federal Aviation Administration (FAA) has been making, or has made, a transition from the old, most recent, mode of radio communication to a newer mode of radio communication with a Data Communications (Data Comm) system that is part of the Next Generation Air Transportation System (NextGen) program (Tinio, R. F., 2018). NextGen was reported in the news by the National Public Radio (NPR) back in 2016 (Naylor, B., 2016). Data Comm is expected to rectify communication issues between pilots and ATCs such as those being lengthy due to procedures and causing comprehension difficulties and errors from miscommunication (Tinio, R. F., 2018). The procedures for communication between pilots and ATCs often involve identifying the sender and receiver of the message, comprehending what was said and received in the instructions, and reading back the instructions to ensure they were understood and received (Tinio, R. F., 2018). Sometimes, errors can occur from the communication procedures through miscommunication due to language barriers, various accents, and cultural differences in the presentation of the instructions and, thus, be detrimental to flight safety (Tinio, R. F., 2018). It is unsure if all air traffic control instructions, traffic advisories, and weather information will be transmitted to the cockpit, digitally, with technological projects such as an aeronautical datalink and ADS-B (automatic dependent surveillance broadcast), which have been tested in commercial jets and expected to be included in general aviation aircraft within the next decade (Tinio, R. F., 2018). But when NPR first reported about NextGen and talked about Data Comm, it reported on one specific feature of Data Comm.\nIt reported on a text messaging system for all air traffic communication to improve communication between hearing pilots and ATCs (Naylor, B., 2016). How it works is that ATCs and pilots exchange information, electronically, by texting each other (Naylor, B., 2016). This is like closed captioning, but with differences in the method of presentation and delivery. While most pilots are not deaf and HOH, this was a very essential policy measure to implement, especially, considering air traffic control towers at most major metropolitan airports can be very chatty places.\nThe chatter on the radio can affect an aircraft pilot’s speech intelligibility, as mentioned earlier, because there are various numbers, phrases, and words commonly found in pilot-speak contained within the instructions received from ATCs designed to efficiently spell out the routes pilots need to get to their destinations (Naylor, B., 2016). What makes a text messaging system even more vital for situations like this is that if a pilot mishears the instructions, they received from the ATC after carefully reading back what was heard, then the pilot must go through the process all over again (Naylor, B., 2016). Route information must always be communicated to pilots by ATCs while a plane is on the taxiway because changing weather patterns involving, say, thunderstorms immediately showing up on the radar can cause the anticipated route to be changed before receiving clearance to takeoff (Naylor, B., 2016). Apart from flight safety concerns due to miscommunication, this can also end up taking several minutes by delaying departures, burning fuel, and emitting carbon dioxide emissions while sitting on the taxiway waiting to takeoff (Naylor, B., 2016). Pilots and administrators have noted the benefits of this text messaging system.\nAccording to NPR, a United Parcel Service (UPS) pilot of a Boeing 767 has claimed that he is able to process route clearances, route changes, and frequency changes through the text messaging system (Naylor, B., 2016). Since him and other pilots normally have gotten most of their messaging through voice on a radio frequency while taxiing towards the runway, they are now able to get their messages in a text-message-like format on a screen near the center of the console of the cockpit (Naylor, B., 2016). This allows him and his colleagues to view the message and reprogram their computers in seconds, which used to take minutes to process when they received messages by voice (Naylor, B., 2016). This benefit is very important because he pointed out that when receiving messages by voice it would get to a point where at a busy metropolitan airport there would be 30 or 40 airplanes waiting in line during a weather event (Naylor, B., 2016). Add in a couple minutes comprehending the messages multiplied times 40 aircraft and it, easily, led to over an hour in delays (Naylor, B., 2016). The text messaging system is expected to cut down on traveling delays because an Assistant FAA Administrator said it should cut down on situations where passengers have missed their connecting flight or the shoes someone ordered did not arrive on time by making the whole system feel more stable and predictable and not have to sit on a plane wondering what is going on and when someone will get off the plane upon arrival (Naylor, B., 2016). Another area where difficulty communicating occurs not only among Deaf and HOH listeners, but also non-Deaf and HOH listeners, is in medical settings."
  },
  {
    "objectID": "closed_caption_2.html#closed-captioning-usage-in-medical-settings",
    "href": "closed_caption_2.html#closed-captioning-usage-in-medical-settings",
    "title": "Speech Perception Research",
    "section": "",
    "text": "Spehar et al. (2016) conducted a study that looked at how effective speech-to-text (STT) software could be in patient-to-physician encounters and what benefits it could provide in helping patients when making informed consent decisions. This was done by determining if STT could improve patients’ recall of their informed consent decision-making and whether this approach could improve physicians’ communication skills, professionally, and for training purposes with patients, by conducting a post-assessment analysis of the transcripts of their conversations with them (Spehar et al, 2016). One of the reasons they tested this is because enough advancements in STT have been made for it to be used in physician-to-patient encounters, especially, in anesthesiology where regional anesthesia and its associated risks that comes with certain anesthesia techniques are not well understood by patients (Spehar et al, 2016). A second reason is that the real-time captioning that STT provides has the potential capability of improving Deaf and HOH listeners’ abilities to comprehend and later recall conversations with physicians in other medical disciplines besides anesthesiology (Spehar et al, 2016). Finally, a third reason is that speech-to-text devices that incorporate STT could be used to improve comprehension in Deaf and HOH listeners, in general, and that its benefits are probably most readily available to be implemented in medical settings from a policy perspective (Spehar et al, 2016). The results of the researchers’ study led to some insightful conclusions they made about STT as it applies to patient-to-physician encounters in medical settings.\nThe study’s small sample consisted of twelve older adult subjects with hearing loss, and without STT, they performed just as worse as younger subjects in previous studies in retrieving the information presented to them by their physicians (Spehar et al, 2016).  But, with STT, the researchers found that the subjects performed better at recalling the information presented by their physicians (Spehar et al, 2016).  Despite recall being much poorer than recognition, the patients were able to recall nearly half again as many key items from the information presented to them with the assistance of STT than without it (Spehar et al, 2016).\nThis shows that STT can substantially benefit Deaf and HOH listeners’ ability to recall information such as the kind presented to them by a physician in a medical setting by acting as a valuable supplement to the spoken word (Spehar et al, 2016).  The reasoning is that for listeners who are, or not, Deaf and HOH the text of a physician’s most recent utterance can function as a short-term memory store when trying to sort through the confusion that comes with understanding unfamiliar medical terms and concepts (Spehar et al, 2016). The researchers believed that this function is the reason why the Deaf and HOH subjects in their study benefitted, significantly, from STT (Spehar et al, 2016). Although the study had a small sample size and fewer patient-to-physician encounters, a controlled, simulated environment instead of a clinical setting, and subjects who volunteered who may not necessarily be fully representative of Deaf and HOH patients encountered in a clinical setting, the researchers still stood by their findings including other findings they made that could be applied to other areas of patient-to-physician encounters that do not necessarily have to involve Deaf and HOH listeners (Spehar et al, 2016).\nThere are two other areas where the application of STT could be beneficial in patient-to-physician encounters. One area is with physicians from a physician education perspective (Spehar et al, 2016). The transcripts that are automatically produced by STT could be used by physicians to evaluate how well they communicated with patients. This is considering that from the results of the study the physicians were more enthusiastic about the use of captioning than the subjects were in patient-to-physician encounters with average scores of 5.1 and 5.5 by the physicians and 4.1 and 3.5 by the subjects on post-encounter questionnaire items (Spehar et al, 2016). The physicians’ ability to read what they said on the screen of the device utilizing STT not only allowed them to correct any errors made in the transcriptions by the device, but it also allowed opportunities for them to reflect upon what they said and how they said it to help guide their conversations with the patients and hone their communication skills (Spehar et al, 2016). A second area is with patients who speak different languages and have difficulty communicating with a physician (Spehar et al, 2016). Language translation software has improved enough to where it could be used between two different speakers, the patient and physician, who each speak a different language by providing simultaneous translation between speakers of different languages through the utilization of STT (Spehar et al, 2016). However, this potential benefit to speakers with different languages, or communication barriers, does not necessarily have to be specific to medical settings it could also extend into another area, educational settings."
  },
  {
    "objectID": "closed_caption_2.html#closed-captioning-usage-in-educational-settings",
    "href": "closed_caption_2.html#closed-captioning-usage-in-educational-settings",
    "title": "Speech Perception Research",
    "section": "",
    "text": "In educational settings, captions can be used to benefit language learners, children, or adults, who are learning either a primary or secondary language (Gernsbacher, M. A., 2015). Captions provide opportunities for language learners to analyze the words that appear on the screen as they are aurally processing the language by using the visualization of the words to determine linguistic units such as phonemes, morphemes, lexemes, syntax, and context along with grammar, semantics, and pragmatics (Gass et al, 2019). What this does is it helps the learners map the aural speech stream to individual, or meaningful, words and phrases (Gass et al, 2019). Gass et al (2019) investigated in their past studies whether second language (L2) learners utilized captions as a tool for listening comprehension in a classroom setting.\nThey found that captions helped them with listening comprehension by allowing them to segment speech streams into meaningful components (Gass et al, 2019). They extended the findings from those studies through a new study titled “How Captions Help People Learn Languages: A Working Memory, Eye-tracking Study” to investigate how captions helped with the L2 learners’ listening comprehension by investigating how individual differences influence the use of captions through an in-depth look into their working memory (WM) capacities (Gass et al, 2019).  It investigated two populations of learners, English as a second language (ESL) and English speakers learning Spanish (Spanish L2 learners), by observing for common patterns that emerged in their language-learning (Gass et al, 2019). Its results provide insights into the benefits that captions for language learning may provide and best practices and recommendations when implementing them for that purpose.\nGass et al (2019) found that the results answered the first research question of the study - “Does captioning aid comprehension?” - by showing that captioning did promote L2 learners’ video comprehension, thus, supporting the researchers’ past work. They also found that in answering the second research questions of the study - “What is the relationship between WM and L2 video comprehension? Do learners with high WM capacity comprehend more than learners with low WM capacity?” - the results showed that there was little effect of WM and L2 video comprehension for the Spanish L2 learners, but a medium effect of WM and L2 video comprehension for the ESL learners.\nThis may have been influenced by the possibility that the proficiency levels of the ESL learners were higher than the Spanish L2 learners because the recall scores of the ESL learners from a free-recall task following the watching of a short, captioned video were higher than the Spanish L2 learners (Gass et al, 2019). Gass et al (2019) theorized that there is a certain level of proficiency, or proficiency threshold, that must be reached before WM can play a factor in influencing the video comprehension of L2 learners. Basically, WM capacity does not seem to differ among certain types of learners when their proficiency level is lower because the effort in comprehending is going to be lower at a very basic level consisting of a word-to-word interpretation and it may be difficult to put individual words together to create a meaningful stream of speech (Gass et al, 2019). In examining the results to answer their third research questions - “What is the relationship between WM and caption-reading behavior? Are there differences in caption-reading behavior between learners with high WM and learners with low WM?” - Gass et al (2019) found interesting differences based on first-time (T1) viewing versus second time (T2) viewing of watching a video in a L2.\nDuring T2 viewing, the reading behavior of the Spanish L2 and ESL learners was similar, however, the high WM groups reduced their caption reading time and the low-WM groups increased their reading time (Gass et al, 2019). To clarify, the researchers identified those learners in the Spanish L2 learners’ group for experiment one and ESL learners’ group for experiment two who performed better on the free-recall test and assigned them to the high WM group, whereas the rest of the learners were assigned to the low WM group of each experiment (Gass et al, 2019). Gass et al (2019) decided that they could not conclude with confidence of any correlation from this finding, but, instead, suggested that this was most likely the result of individual differences accounting for variation in information-processing during multimedia learning. For them to conclude with confidence that group differences were occurring, they would have needed a much larger sample size to help reveal greater differences between those with low and high WM capacity (Gass et al, 2019). In examining their results to answer the fourth research question - “What is the relationship between caption-reading behavior and L2 video comprehension? Are there differences in caption-reading behaviors between learners who demonstrate high video-comprehension and learners who demonstrate low video-comprehension?” - Gass et al (2019) found that language immersion may have played a role in the differences in results between the two learning groups.\nThey found that language immersion may have been the deciding factor in why the ESL learning group had fewer differences in how much time was spent reading the captions when watching the video between the high – and low – comprehension groups during T1 viewing versus T2 viewing (Gass et al, 2019).  Mainly, because the ESL learning group had spent more time gaining experience learning how to speak English in an English-speaking country and had developed a certain level of proficiency (Gass et al, 2019). Whereas, on the other hand, the Spanish L2 learning group had not and showed differences in time spent reading the captions amongst the two comprehension learners. The high-comprehension learners spent less time, overall, reading captions during T1 viewing and less time rereading than the low-comprehension learners (Gass et al, 2019). It was most likely that the high-comprehension learners were possibly depending more on the audio or from the visuals of the video than from the captions (Gass et al, 2019). As a result, the findings from the results that helped answer this fourth research question including the other three led to more findings that give insights into best practices and recommendations for implementing captions for L2 learning.\nBased on the results, Gass et al (2019) recommends that it is best to expose L2 learners to captions when watching videos in a L2 because giving them more experience watching videos with the captions allows them to develop the ability to efficiently split their attention among multiple modes of input consisting of captions, audio, and other visual input. Having this balanced approach to using multiple sources of input will help enable them to parse and understand the incoming speech stream and aid in their learning (Gass et al, 2019). However, it is still important to consider a learner’s WM capacity and L2 proficiency level relative to a video’s content when implementing captions (Gass et al, 2019).\nWhile L2 learners regardless of L2 proficiency used captions and those with higher WM, proficiency, and comprehension still read them, L2 learners with lower WM, proficiency, and comprehension tended to benefit more from using the captions because it acted more as a helpful processing aid relative to the difficulty level of the video (Gass et al, 2019). Captions acted in this way by helping those types of L2 learners with attentional control by acting as a useful salient, attention-grabbing piece of written information when aural information became too difficult and inaccessible for learning (Gass et al, 2019). L2 learners with higher WM capacity used the captions less than those with lower WM capacity, most notably, during T2 viewing (Gass et al, 2019). The reason is that they may have been able to hold more key information effectively in the episodic buffer store of their WM during T1 viewing and, thus, already were able to glean enough needed information from the captions the first time around (Gass et al, 2019). This is important to take note of when implementing captions as a L2 learning aid because, while captions most certainly aid comprehension, they must be chosen carefully by an educator to make sure they match the learners’ proficiency level (Gass et al, 2019). Otherwise, if a learner’s proficiency, comprehension, and WM is very high, then providing captions could end up, eventually, serving more as a nuisance (Gass et al, 2019). Besides learning languages in educational settings, another area where the implementation of captions is vital for Deaf and HOH and, occasionally, non-Deaf and HOH listeners is daily communication."
  },
  {
    "objectID": "closed_caption_2.html#closed-captioning-usage-in-daily-communication",
    "href": "closed_caption_2.html#closed-captioning-usage-in-daily-communication",
    "title": "Speech Perception Research",
    "section": "",
    "text": "The implementation and availability of closed captioning is essential to Deaf and HOH listeners for daily communication because communicating through technology has become more normalized (Zhong et al, 2022). As a result, the challenges of participating in telecommunication involving conversing on the telephone and teleconference platforms, and viewing recorded media, are growing because conversing on the telephone has become important for staying in contact with friends and family, for scheduling medical appointments, and participating in telehealth (Zhong et al, 2022). The novel coronavirus (COVID-19) pandemic has exacerbated these growing challenges, even more, through the increase in demand for teleconferencing by enforcing orders and recommendations to reduce in-person gatherings (Zhong et al, 2022). While it is very important to reduce the risk of infection and improve productivity by making work and communication between people more efficient, it may present communication difficulties for Deaf and HOH listeners, in general, and non-Deaf and HOH listeners in noisy environments. This presents a need for the implementation and availability of closed captioning including improvements in its delivery and presentation for daily communication (Zhong et al, 2022).\nIn a systematic review by Zhong et al (2002), it was found that the results of ten studies showed that text captions added to auditory signals has the potential to help listeners who are, or not, Deaf and HOH understand speech, which was consistent with the demonstrated benefits of providing text captions as asynchronous feedback for recognition of distorted speech. This supports the need to implement and provide availability and access to closed captioning including the findings that the benefits of text captions are both greater when the auditory signal integrity is low and in lower auditory conditions when the auditory integrity was manipulated (Zhong et al, 2022). However, the benefits of closed captioning for daily communication have been found to be very beneficial to older adults with hearing loss who are representing a growing population of listeners with hearing loss because the benefits of text captions do not seem to vary based on listener age (Zhong et al, 2022).\nAlthough older adults with hearing loss are less able to benefit from visual speech information, also known as speech-or lip-reading, than younger adults, older adults benefit from text captions because it is believed that older adults benefit from the additional context that text captions provide (Zhong et al, 2022). Existing evidence has shown that older adults can benefit from context more so than younger adults (Zhong et al, 2022). Since older adults tend to visit and consult with medical personnel more often as they get older, the benefits of text captions that have been found for older adults is important. This is because it is suggested that the implementation of text captions in teleconferencing applications for telehealth appointments could, significantly, benefit those who have hearing loss who might have more trouble communicating in a telephone appointment or video-conferencing session due to reduced audibility (Zhong et al, 2022). However, there is still room for improvement in maximizing the benefits of closed captioning for daily communication in listeners who are, or not, Deaf and HOH through improvements in its delivery and presentation.\nWhen Deaf and HOH listeners are communicating through telecommunication with the help of text captioning, certain text characteristics such as text integrity and timing, when presented and delivered efficiently, may enhance the benefits of text captions (Zhong et al, 2022). It has been found that as the delay time increased while text captions were being presented in response to a speech signal, the text caption benefit decreased (Zhong et al, 2022). This highlights the importance of fast automated speech-to-text algorithms when implementing text captions for telecommunication including policy changes in governmental guidelines that would help provide better guidance for creators of these algorithms (Zhong et al, 2022). Mainly, because with the FCC’s guidelines for text captions set at a recommended 125 words per minute for telephones with 98% accuracy, there have been some reports that the estimated speaking rate of conversations has been able to reach as high as 320 words per minute (Zhong et al, 2022). If speech-to-text algorithms are, mainly, set to meet the FCC guidelines by presenting text captions at 125 words per minute, then it would result in text caption delays relative to the auditory signal in those speaking situations that have a speaking rate higher than 125 words per minute, thus, reducing the benefits of text captions (Zhong et al, 2022)."
  },
  {
    "objectID": "closed_caption_2.html#conclusion",
    "href": "closed_caption_2.html#conclusion",
    "title": "Speech Perception Research",
    "section": "",
    "text": "Overall, it is the hope that this review gives a general idea of how closed captioning and its variants such as RTT and STT has been and can be implemented in air travel, medical, educational, and daily communication settings to benefit listeners who are, or not, Deaf and HOH by alleviating problems with speech intelligibility. Moreover, it is also the hope that it provides readers in medical, educational, and daily communication settings with a guideline on some best practices and recommendations for implementing closed captioning and its variants in those settings. Before ending this review, some final notes need to be addressed on some of the details in the review’s content.\nThere either needs to be experimental research, or more of it, that is more accessible to the public that shows the benefits of the text messaging system of NextGen’s Data Comm system. There is a good possibility that there has been quite a bit of research on various features of the Data Comm system because, otherwise, this system as part of the NextGen program probably would not have been approved for use as a newer, more improved mode of radio communication by the FAA. After doing some digging on the FAA’s website and coming up empty, mostly, with a focus on looking for research studies done on the text messaging system of the Data Comm system, most of the evidence on the benefits of the text messaging system that could be found, so far, were mostly anecdotal evidence from reputable news sources such as NPR. Because this is all related to air travel, it would also be interesting to see experimental research, or more of it, if it exists, on the benefits of closed captioning, not necessarily, on television for watching entertainment in airports or airplanes because we have enough of that research as it applies to other settings, but on television or live screen devices that are geared towards helping listeners who are, or not, Deaf and HOH be informed about various changes to their flights such as gate changes, sudden emergencies, or other important announcements at airports that can make air travel less stressful (Collaborative for Communication Access via Captioning, 2016). The reason for this is because, according to 3PlayMedia, the United States Department of Transportation (DOT) issued a ruling that went into effect in 2015, which was an amendment to Section 504 of the Rehabilitation Act of 1973, requiring closed captioning on all televisions in US airports that receive federal funding and experience at least 10,000 flights annually (Griffin, E., 2015). Besides air travel, experimental research is also needed on the benefits of closed captioning in another area of medical settings, the operating room.\nThere needs to be experimental research, or more of it, if it exists, on the benefits of closed captioning in operating rooms of medical settings where surgeries or operations are planned. This is because medical personnel must wear masks in these situations and helpful audiovisual cues such as speech-and lip-reading that can, sometimes, be helpful for deciphering speech when communicating are non-existent. There have been instances appear in the news or news articles, occasionally, where Deaf and HOH medical students or personnel fulfill their medical training or perform their job duties in the operating room and utilize text captions on a television screen showing what each person in the operating room is saying. After all, communication in operating rooms is critical and necessary to avoid medical errors and mistakes. Finally, more final clarification is needed to address how research on closed captioning benefits learners learning their primary language, as mentioned in the section on closed captioning in educational settings, especially, in regard to reading skills.\nThe implementation of closed caption has been found to serve the same purpose in benefitting children’s reading skills in their primary language in much the same way as it does for L2 learners learning a L2 because learning to read also involves mapping sound and meaning onto text (Gernsbacher, M. A., 2015). It has, evidently, been found that watching videos with both audio and text captions can improve reading skills by helping non-Deaf and HOH children define content words heard in the videos, pronounce novel words, recognize vocabulary items, and make sense of what is happening in the videos (Gernsbacher, M. A., 2015). This has the potential benefit of growing and expanding the vocabulary of those young listeners who are not Deaf and HOH. Lastly, in India, after researchers encouraged India’s national television to begin captioning Bollywood music videos, it was found in a study assessed over a certain period that the literacy of adults who watched the videos increased more than those who very rarely or did not watch them (Gernsbacher, M. A., 2015)."
  },
  {
    "objectID": "closed_caption_1.html#introduction",
    "href": "closed_caption_1.html#introduction",
    "title": "HCI 1 Research",
    "section": "",
    "text": "Ever since closed captioning was first debuted in 1971 and first available for limited use in 1972, it has not gone through as much of a transformation to improve upon its ability to further address the needs of the deaf and hard-of-hearing (HOH) (Hunter, 2017). Closed captioning means that text showing what someone on the television is saying is not available the moment one turns on the television, which would be open captioning. It is available only when that person goes to the television settings and turns it on if they realize the need to improve their viewing experience (“What is the Difference Between Open and Closed Captioning,” 2020). Closed captioning was not available to the entire viewing public until 1980 when it was provided for widespread access by allowing deaf and HOH viewers the opportunity to purchase special decoder boxes that could be attached to the television in order to access the closed captioning capabilities. Thanks to the United States (US) Television Decoder Circuitry Act of 1993, decoders are embedded in all televisions that customers purchase in the U.S. (Branje et al., 2005, p. 2330).\nIn the US Hanson (2009) pointed out that the National Institute on Deafness and Other Communication Disorder’s (NIDCD) estimated statistical figures from 2004 showed that 28 million people in the US had some degree of hearing loss. Now, that figure for the US has increased to 48 million people with some degree of hearing loss including 477 million people worldwide according to the most recent estimated statistics available to date (Hearing Health Foundation, 2020).\nClosed captioning is still presented as white text, in a single mono-spaced font and single font size, against a black background on most video platforms with some changes (see Appendix). For example, it has changed some from only all capital letters to mixed case letters, and it sometimes shows a small set of text colors and uses special characters such as music notes to inform the user when noise, sound, or music is being played (Branje et al., 2005, p. 2330). These minor changes are the result of advancements in encoder and decoder technologies from when it first appeared on analogue televisions (Fels et al., 2008, p. 506). With these changes, along with the fact that current closed captioning guidelines in the captioning industry have remained unchanged in the US since captioning was mandated by the Federal Communications Commission, captioners sometimes make creative choices that may not be obvious to the user such as assigning descriptive text to sound effects, paraphrasing long speeches, and omitting information deemed unnecessary to try to improve the user’s viewing experience (Hanson, 2009, p. 128 and Fels et al., 2008, p. 506). However, these changes are not enough to meet the needs of the deaf and HOH population because its needs and range of hearing loss is very diverse.\nThe major issue that separates people with hearing loss on the spectrum of hearing loss ranging from relatively little hearing loss to profound hearing loss is language acquisition skills. Because experiences of difficulty with pitch, timbre, and loudness will occur with hearing loss, speech perception difficulties will not only be visible, but reading and writing skills will inevitably be affected (Hanson, 2009, p. 126). Being able to understand this has implications for interface designers as it relates to closed captioning for the purpose of improving the end users’ video viewing experience either for entertainment, education, or communication purposes. For example, users who are deaf and HOH may have difficulty with reading because of speech perception difficulties. This is especially the case if they lost their hearing at birth or during the pre-lingual stage of child development because reading is based on the overall phonetic structure and understanding of grammar and text comprehension of a spoken language (Hanson, 2009, p. 127). Instead of presenting the text in closed captioning verbatim and making a captioner’s job harder to try to improve the user’s viewing experience, one of the ways designers can make closed captioning more user-friendly is to consider incorporating graphics and animations or animated texts to convey the displayed emotion and sound information (Fels et al., 2008, p. 507)."
  },
  {
    "objectID": "closed_caption_1.html#emotive-captioning",
    "href": "closed_caption_1.html#emotive-captioning",
    "title": "HCI 1 Research",
    "section": "",
    "text": "There are two ideas present in the research literature that were discovered on the topic of closed captioning and the deaf and HOH that use graphics and animations or animated texts. The first one is emotive captioning and the second one is kinetic typography. The first idea called emotive captioning is a type of captioning that Branje et al. (2005) describes in their study as the use of a combination of graphics, color, icons and animations, and text to illustrate sound information. It involved using graphics to represent emotion and sound effects and by utilizing a production team to deploy the designed graphical captions by showing very short segments of two different television vignettes from an eight-vignette series titled Burnt Toast: Forever and Ever and Traffic Jammed (Branje et al., 2005, p. 2331). To decide on which graphics to convey sound information, the researchers had to decide what information to convey and how to best express them through graphics, and which graphics are the most appropriate (Branje et al., 2005, p. 2331). The most common emotions they found according to the research literature in order to figure out which graphics and icons to display in the captions were fear, anger, sadness, happiness, disgust, and surprise including two more that members of the production team wanted to add, which were sexy and love (Branje et al., 2005, p. 2331). The production team marked up their script using a Caption Markup tool to tag it with the different emotions and emotional intensities (Branje et al., 2005, 2332). The purpose of doing that is for the script to display a text-based file (Branje et al., 2005, 2332). Then some members of the production team marked up the script some more with their interpretation of the emotive characteristics of the show (Branje et al., 2005, 2332). What doing this does is it allows for their Rendering Engine tool to automatically create graphical pop-on captions using pre-designed image files that are associated with each different emotion selected for the study and allow for captions to be edited with image and text tools consisting of font style, size, and text color (Branje et al., 2005, 2332; see Appendix).\nBranje et al.’s study had eleven participants consisting of six deaf, American Sign Language (ASL) users and five HOH users. To compare data, the researchers had the users complete a pre-test and post-test questionnaire and interview including discussions that took place amongst the users who were recorded watching the two video segments (Branje et al., 2005, p. 2332). Basically, the reactions to the three different versions of both video segments with conventional captions, emotive captions located in one location of the screen, and emotive captions located in different locations on the screen designed to show speaker identification were being compared between the group of deaf users with ASL knowledge and the group of HOH users without ASL knowledge (Branje et al., 2005, 2332)."
  },
  {
    "objectID": "closed_caption_1.html#kinetic-typography",
    "href": "closed_caption_1.html#kinetic-typography",
    "title": "HCI 1 Research",
    "section": "",
    "text": "The second idea called kinetic typography is a type of captioning that may use, for example, trembling letters with scratchy typeface to convey a sense of terror, large font sizes to express the emotion associated with screaming, and a small font size to communicate the emotion associated with whispering (Fels et al., 2008, p. 507). Fels et al. (2008) did a study where they experimented with the usage of kinetic typography on twenty-five participants split into two groups, ten hearing (H) users and fifteen HOH users. The kinetic texts used in this study corresponded to five basic emotions: anger, fear, happiness, sadness, and disgust. These emotions were later combined to be turned into a range of fifteen total emotions that were depicted through kinetic texts with an emotional intensity modifier defined as high, medium, or low (Fels et al., p. 508). The different emotions expressed by kinetic texts were presented in two one-minute video segments from two episodes of a children’s show called Deaf Planet titled “Bad Vibes” and “To Air is Human (Fels et al., 2008, p. 509).” The episodes were presented to the participants in three versions: closed caption, enhanced animated text, and extreme animated text with static text shown at the bottom and animated text animated dynamically around the screen (Fels et al., 2008, p. 509; see Appendix). To get the results, the comparison of the data from the three versions of both shows were obtained by having the users complete a pre and post-test questionnaire, having the users complete a checklist, and videotaping the users comments and reactions to the viewing experience (Fels et al., 2008, p. 509).\nBranje et al. (2005) found no significant differences in the users’ responses between the two video segments in their study on emotive captions. But, they did find that emotive captions tended to benefit the HOH users more than the deaf users because HOH users preferred using graphics, icons, and color to represent sound information, whereas, deaf users did not (Branje et al., 2005, 2336). The emotive captions were received positively by HOH users than deaf users in the areas of willingness to engage in conversation with hearing friends about the viewing content, appearance of the face icons in the graphics of the emotive captions, graphical representation of the emotions, and color of the emotive captions (Branje et al., 2005, 2336).\nFels et al. (2008) found a significant difference between the three viewing presentations of captioning in the study on kinetic typography in terms of likeability. Both H and HOH viewers preferred the enhanced animated captions instead of the conventional captions and extreme animated captions. The ratio of positive to negative comments for enhanced captions was higher on the positive side and positive results were higher with enhanced captions concerning the movement of text and portrayal of emotions by the moving text (Fels et al., 2008, p. 516). In terms of identifying the correct emotion being presented through the three different captioning presentations, based on the number of tries one uses in identifying the correct emotion, HOH viewers missed fewer emotions than H viewers (Fels et al., 2008, p.517). Significant differences amongst HOH viewers were found concerning text descriptions for sound effects where most viewers suggested that symbols be used for sound effects instead of text (Fels et al., 2008, p.517). That finding reiterates the findings from Branje et al. (2005) that reported that deaf and HOH viewers like animated symbols for representing sound effects (Fels et al., 2008, p.517). Basically, the results from this first study on the usage of kinetic texts suggests that animated texts is progressing as being the best path forward in helping HOH viewers who are indicating that they want more sound information in closed captioning (Fels et al., 2008, p.517). Despite some of the positive findings on the usage of emotive captioning and kinetic typography as closed captioning alternatives, closed captioning in either its current form or in the mentioned adaptations, especially from the results of the study by Branje et al. (2005), may still not be effective in meeting the needs of deaf users who only sign."
  },
  {
    "objectID": "closed_caption_1.html#sign-language-signing-interfaces",
    "href": "closed_caption_1.html#sign-language-signing-interfaces",
    "title": "HCI 1 Research",
    "section": "",
    "text": "Deaf and HOH users who only sign need an adaptable form of closed captioning that involves communicating and describing what is being said, what the emotional content is, and whether music, sound, and noise is being played by providing a visual representation of a sign language signer communicating that information. Communicating all that information through sign interfaces has always come with a disadvantage. They have always required a live signer (Hanson, 2009, p. 129). Now, there is video phone technology that exists for deaf and HOH users that allows a signed phone conversation to occur that is as close as it gets for a sign interface to resemble automated closed caption derived from speech recognition technology (Hanson, 2009, p. 131). However, when a live signer is involved everything must be pre-recorded and there is no automatic signed translation of software and web content available for one to rely on if one is looking for signed versions of audio or multimedia that can be created in real time (Hanson, 2009, p. 129).\nOne of the solutions to dealing with this limitation on sign interfaces that has been created is providing an automatic sign presentation called “concatenated signing” where a software program is created by stringing words together in a sentence in order to concatenate signs for each word that were produced by a live signer (Hanson, 2009, p. 129). To ensure a seamless transition from one signed word to the next, since the transition from one video of a live signer to another video of a different live signer can be disjointed, computer algorithms have been created to help smoothen the flow (Hanson, 2009, p.129). Other disadvantages of providing an automatic sign presentation in this manner include the high production cost of creating a professional quality video, the increasing cost of making new videos each time content detail to a website or software changes, the large size of videos for storing and downloading, and the need for a sophisticated server architecture to accommodate streaming content that is making changes at runtime (Glauert et al., 2007). Despite its disadvantages, the automatic sign presentation as described is not necessarily a bad concept, but there is one automatic sign interface concept that is even more promising."
  },
  {
    "objectID": "closed_caption_1.html#virtual-sign-language-signing-avatars",
    "href": "closed_caption_1.html#virtual-sign-language-signing-avatars",
    "title": "HCI 1 Research",
    "section": "",
    "text": "The automatic sign interface concept that has the greatest potential to benefit deaf users who only sign is the use of virtual signing avatars (see Appendix). Signing avatars are operated by animation software in which motion data is generated in real time from a scripting notation designed for describing the avatar’s signing (Glauert et al., 2007). The use of signing avatars has six distinct advantages that make it an attractive alternative to sign interfaces on the internet, which have always been presented through video.\nOne is that a user can define the signs through scripting notation and compose sequences of signs on a desktop computer without the need for video or motion capture equipment (Glauert et al., 2007). Second, the continuous flow from sign to sign is very smooth because any piece of signing can be sequenced for any avatar (Glauert et al., 2007). Third, the details of the signing content can be edited without having to rerecord any sequences of signs (Glauert et al., 2007). Fourth, the concern for bandwidth and disk space is very low because, after the software is installed on one’s computer to translate scripting notation to motion data and create the avatar, scripting notation is all that is needed to be transmitted and its data can be much smaller than highly compressed videos or motion data of live signers (Glauert et al., 2007). Fifth, the frame rate concerning graphics and images is less of a concern because data transmitted by the end user can be set to whatever speed the user’s computer is capable of creating the avatar and avatar’s signing (Glauert et al., 2007). Finally, the user has more control over the avatar than with videos of live signers by adjusting the viewing angle of an avatar and switching amongst different types of avatars during playback (Glauert et al., 2007).\nThe use of signing avatars have been used primarily in different countries in the European Union (EU) as part of various projects and initiatives to provide counter clerks with a tool for communicating with deaf customers in post offices, to convert the stream of captioning from the EU’s version of closed captioning called teletext into signing, and to improve usage of broadcast and face-to-face interaction applications on the web for deaf users (Glauert et al., 2007). Although there are always improvements needing to be made to the implementation of signing avatars, based on the evaluations of deaf participants’ comprehension of single signs, signed sentences, and text chunks, the evaluation scores have been high amongst some deaf participants, thus, indicating that the concept including the modifications and adjustments that have been made to improve the presentation of signing avatars is working (Glauert et al., 2007). This is also reflected in the participants appreciation for the existence of signing avatars because they cited the avatars as helpful to those with severe reading difficulties, as offering them more user control of how to use the avatars to meet their needs, and how they could be helpful in giving information to them at train stations, airports, townhalls, hospitals, and travel agencies (Glauert et al., 2007)."
  },
  {
    "objectID": "closed_caption_1.html#conclusion",
    "href": "closed_caption_1.html#conclusion",
    "title": "HCI 1 Research",
    "section": "",
    "text": "Overall, the summarized results and conclusions from the evaluations of the signing avatars used in the EU show that they are benefitting deaf users who only sign and that the use of emotive captioning and kinetic typography in the studies mentioned show that they benefit HOH users. Although these closed captioning adaptations and alternatives do not benefit their respective users in every area, they still benefit them in certain noticeable areas. The presentation and introduction of closed captioning adaptations and alternatives in this review are not necessarily the only true way to enact them. More research and creative problem-solving should continue to take place to either further validate the findings from these closed captioning adaptations and alternatives, improve upon them, and find more newer ones. Continuing this may help change and loosen some of the strict captioning guidelines set by the US federal government that do not allow much flexibility in creating and providing more closed captioning adaptations and alternatives (Fels et al., 2008, p.506). Meanwhile, countries in the EU have been implementing and providing more closed captioning adaptations and alternatives for a while now (Fels et al., 2008, p. 506)."
  },
  {
    "objectID": "closed_caption_1.html#references",
    "href": "closed_caption_1.html#references",
    "title": "HCI 1 Research",
    "section": "",
    "text": "Branje, C., Fels, D. I., Hornburg, M. & Lee, D. G. (2005). Emotive Captioning and Access to\nTelevision. AMCIS 2005 Proceedings, 2330-2337. http://aisel.aisnet.org/amcis2005/300\nFels, D. I., Hunt, R., Vy, Q., & Rashid, R. (2008). Dancing with Words: Using Animated Text\nfor Captioning. International Journal of Human-Computer Interaction, 24(5), 505-519.\nhttps://doi.org/10.1080/10447310802142342\nGlauert, J. R. W. & J. R. Kennaway (2007). Providing Signed Content on the Internet by\nSynthesized Animation. ACM Transactions on Computer-Human Interaction, 14(3).  http://doi.acm.org/10.1145/1279700.1279705\nHanson, V.L. (2009). Computing Technologies for Deaf and Hard of Hearing Users. In A. Sears\n& J. A. Jacko (Eds.), Human-Computer Interaction: Designing for Diverse Users and\nDomains (pp. 125-133). Taylor & Francis Group.\nHearing Loss and Tinnitus Statistics. (2020). Hearing Health Foundation. Retrieved from\nhttps://hearinghealthfoundation.org/hearing-loss-tinnitus statistics#:~:text=48%20million%20people%20in%20America,born%20with%20a%20hearing%20loss.\nHunter, E. (2017, December). A Brief History of Closed Captioning. The Sign Language\nCompany. Retrieved fromhttps://signlanguageco.com/a-brief-history-of-closed-captioning/\nWhat is the Difference Between Open and Closed Captioning. (2019). DO∙IT. Retrieved\nfrom https://www.washington.edu/doit/what-difference-between-open-and-closed-captioning#:~:text=Open%20captions%20always%20are%20in,order%20to%20view%20closed%20captions."
  },
  {
    "objectID": "closed_caption_2.html#references",
    "href": "closed_caption_2.html#references",
    "title": "Speech Perception Research",
    "section": "",
    "text": "Collaborative for Communication Access via Captioning (2016). Captioning in transportation –\nAir travel and more. http://ccacaptioning.org/captioning-transportation/\nGass, S., Winke, P., Isbell, D. R., & Ahn, J. (2019). How captions help people learn languages:\nA working-memory, eye-tracking study. Language Learning & Technology, 23(2), 84-\n104. https://doi.org/10125/44684\nGernsbacher, M. A. (2015) Video captions benefit everyone. Policy Insights Behav. Brain Sci.,\n2(1), 195-202. https://doi.org/10.1177/2372732215602130\nGriffin, E. (2015, August 10). US DOT officially requires closed captioning on airport TVs.\n3PLAYMEDIA. https://www.3playmedia.com/blog/us-dot-officially-requires-closed-\ncaptioning-on-airport-tvs/\nKlein, R. (2022, July 25). What’s the true price of closed captioning services? 3PLAYMEDIA.\nhttps://www.3playmedia.com/blog/how-much-does-closed-captioning-service-cost/\nZhong, L., Noud, B. P., Pruitt, H., Marcrum, S. C., & Picou, E. M. (2022) Effects of text\nsupplementation on speech intelligibility for listeners with normal and impaired hearing:\nA systematic review with implications for telecommunication. International Journal of\nAudiology, 61(1), 1-11. https://doi.org/10.1080/14992027.2021.1937346\nNaylor, B. (2016, October 3). Air traffic controllers and pilots can now communicate\nelectronically. NPR. https://www.npr.org/sections/alltechconsidered/2016/10/03/4963937\n87/air-traffic-controllers-and-pilots-can-now-communicate-electronically\nPayne, B. R., Silcox, J. W., Crandell, H. A., Lash, A., Ferguson, S. H., & Lohani, M. (2022) Text\ncaptioning buffers against the effects of background noise and hearing loss on memory\nfor speech. Ear & Hearing, 43(1), 115-127. https://doi.org/10.1097/AUD.000000000000    1079\nSpehar, B., Tye-Murray, N., Myerson, J., & Murray D. J. (2016) Real-time captioning for\nimproving informed consent: Patient and physician benefits. Reg. Anesth. Pain Med.,\n41(1), 65-68. https://doi.org/10.1097/AAP.0000000000000347\nTinio, R. F. (2018). Perceiving the communication methods between deaf pilots and air traffic\ncontrol (Publication No. 1464) [Doctoral dissertation, Purdue University]. Open Access\nTheses"
  },
  {
    "objectID": "project.html#project-proposal",
    "href": "project.html#project-proposal",
    "title": "Project",
    "section": "",
    "text": "This is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "project.html#project-presentation",
    "href": "project.html#project-presentation",
    "title": "Project",
    "section": "Project Presentation",
    "text": "Project Presentation\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "project.html#project-report",
    "href": "project.html#project-report",
    "title": "Project",
    "section": "Project Report",
    "text": "Project Report\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; document, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "ride_saver_app.html#ridesaver-app-design-presentation",
    "href": "ride_saver_app.html#ridesaver-app-design-presentation",
    "title": "RideSaver App",
    "section": "",
    "text": "This is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "ride_saver_app.html#ridesaver-app-design-report",
    "href": "ride_saver_app.html#ridesaver-app-design-report",
    "title": "RideSaver App",
    "section": "RideSaver App Design Report",
    "text": "RideSaver App Design Report\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; document, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "ride_saver_app.html#ridesaver-app-design-prototype",
    "href": "ride_saver_app.html#ridesaver-app-design-prototype",
    "title": "RideSaver App",
    "section": "RideSaver App Design Prototype",
    "text": "RideSaver App Design Prototype"
  },
  {
    "objectID": "policy_paper.html",
    "href": "policy_paper.html",
    "title": "Policy Paper on Oversharing",
    "section": "",
    "text": "Grant Powell\nSchool of Behavioral and Brain Sciences, University of Texas at Dallas\nPSYC 6532 Cognitive Psychology Essentials for Cybersecurity\nProfessor Daniel Krawczyk\nMay 4, 2022\n\n\nOversharing, also called self-disclosure, is a common behavior that has been observed on social media sites (SMSs) such as Facebook and Twitter. Sharing information, in and of itself, is one of the foundational objectives that SMSs were created to achieve. It allows for users to create accounts called profiles to allow users to get in touch with long lost childhood friends, classmates, distant family members, like-minded people, etc. by sharing personal information about themselves to search for and connect with certain people or make it easier for those people to search for them. The more users interact with SMSs and socialize with others, the more information they share. This is a boon to SMSs because the more information users share, the more revenue they generate through advertising. Not only will more relationships between users become strengthened and attract more primary users, but it will also attract more secondary users, the advertisers. This raises concerns over privacy and cybersecurity.\nOne concern is becoming victims of cybercrime (Ziegeldorf, 2016, p. 226).  There is always the risk of information abuse that comes with increased sharing on SMSs (Kroll, 2021, p. 3). One’s information could be shared with and collected by third parties such as advertisers, for example, and used for malicious purposes such as cyber stalking and identity theft (Kroll, 2021, p. 3). Cyber stalking can involve users experiencing threats, sexual harassment, and conflicts (Kroll, 2021, p. 4). Identity theft could be the vulnerability of falling prey to a cybercrime called spear-phishing. Spear-phishing is where emails are targeted to a specific individual based on personal information that the attacker knows about that individual. The attacker uses that information to coax that individual into clicking on an email that looks very legitimate in the hopes of getting that individual to divulge more sensitive personal information. One example is login information to gain access to bank accounts. One of the ways one’s personal information can fall into the hands of an attacker is through oversharing on SMSs. Attackers can use any information one shares on SMSs and figure out a pattern that helps them determine one’s lifestyle and habits. Ways those attackers can collect information about you is by either acting as a troll or employing social bots.\nTrolls are real human users on SMSs with fake accounts. Social bots are computer programs posing as real human users on SMSs with fake accounts. They can take advantage of algorithms on SMSs that advertisers use to help them figure out what advertisements to share with certain users that they think a specific user would most likely be interested in seeing. This is determined based on information that users share, post, like, click, and divulge. This helps trolls and social bots figure out patterns about a specific user’s lifestyle and habits. It also gives them opportunities to attempt swaying the outcome of elections during election season by spreading inflammatory political content to users who are most likely to believe it, true or not. Thus, creating “echo chambers” and “us versus them” confrontations on SMSs by preying on our cognitive biases. This can cause users to experience missed career opportunities and embarrass themselves towards either never getting hired for a job or fired from a job if such content pulls them into using harmful language on SMSs that they otherwise would never use in face-to-face interactions (Ziegeldorf, 2016, p. 226). Researchers have come up with different reasons why users overshare on SMSs and put themselves at risk.\nDespite privacy concerns they may have, users do not always observe privacy-protecting behaviors that reflect their concerns (Wisniewski, 2017, p. 95). This could be a result of limited motivation to control their privacy by taking the route that is good enough instead of one that is more optimal (Wisniewski, 2017, p. 95). Or biases may be playing a role against users’ better judgement because users may be experiencing information overload or feel they have nothing to hide (Bergram, 2020, p. 2). Bergram et al. (2020) pointed out that 62% of a sample of users surveyed in the U.S. believed that if a website had a privacy policy that it meant it could not share their data with other companies. Not surprising because it is hard to get a sense of any SMSs position on the problem of over-sharing since they are still businesses (Kroll, 2021, p. 2). They are still going to do what is necessary with whatever security system they put in place to increase user interaction and sharing (Kroll, 2021, p. 2). It is hard to know if the feeling of a secure and trustworthy platform they are promoting is all just security theater or a genuine attempt to toughen security measures (Kroll, 2021, p. 2). This is because the desire to share is influenced by trust in the SMS and a feeling of control (Kroll, 2021, p. 4). Even if privacy controls are available to users, they may also be either unwilling or unable to take the time to learn and educate themselves on the controls that they can use to their advantage (Wisniewski, 2017, p. 95).\nThis could be the result of users having trouble navigating elaborate privacy options that are available to them (Wisniewski, 2017, p. 95). Wisniewski, et al. (2017) found that 48% of users on SMSs have expressed difficulty understanding how to manage their SMSs privacy settings. Most Facebook users do not seem to know the consequences of setting up their own privacy settings in a certain way based on the fact common in most users, they share content that goes against the disclosure intentions they uphold (Wisniewski, 2017, p. 96). Because of these behaviors, users do not always use the privacy controls available to them to take control over their data. By staying with the permissive privacy control defaults and having a lack of knowledge or motivation to control their own privacy, oversharing becomes predominant among users (Wisniewski, 2017, p. 96). To curb the problem of oversharing, researchers should consider or continue considering some research possibilities.\n\n\n\nOne research possibility could be to investigate how many users have added random strangers and by investigating further how often users add random strangers on a certain scale (i.e., all the time, most of the time, neutral, sometimes, never). However, to advance this further as it relates to the basis of why users overshare, researchers should consider understanding why users add random strangers as friends or request to be their friends in the first place (Kroll, 2021, p. 4). There is probably a psychological basis to this that involves the issue of low self-esteem and the feeling of being wanted and accepted by others. Involvement on social media for some users may be more of a popularity contest since there are users who have been successful as social media influencers and made a living out of that lifestyle. This could bring about potential privacy concerns regarding oversharing because a user may not be aware of whether a random stranger is a potential troll, social bot, hacker, or attacker who could prey upon that user’s oversharing behavior to commit crimes such as cyber-stalking or identity theft. Research in this area could help lead towards ways of designing privacy nudges, or better ones, to help users avoid or reconsider adding certain users to their circle of friends that they do not know. Yet, it could also reveal coping strategies users utilize that could be helpful towards the design of privacy nudges.\nResearch is needed to investigate what coping strategies users use to limit their own self-disclosure (Kroll, 2021, p. 4). Researchers could ask users if they have stopped disclosing information by not creating further posts, deleting past posts, deleting their account, changing the visibility of new and old posts, or deleting friendships with remote strangers (Kroll, 2021, p. 4). From there, researchers can glean some more coping strategies from users that users have used to deal with difficult users online that they do not know and have regretted adding as friends or have used after regretting sharing certain online content. This could help create a classification system of coping strategies to help design or improve designs of privacy nudges used to nudge users towards consideration of certain coping strategies to help limit oversharing.\nAnother research possibility could be to investigate if privacy campaigns lead to an increased awareness amongst users of privacy and the privacy settings available to them to help them make informed decisions on how to control their sharing (Kroll, 2021, p. 5). There is a need to discover which privacy settings are more desirable to users and which ones SMSs are creating are the desired or actual kinds of privacy settings that researchers and policymakers are looking for or are needed to be created and presented to users (Kroll, 2021, p. 5). However, it might be a good idea to investigate the role that attention plays on users’ privacy awareness in terms of their awareness of privacy settings and features.\nResearch is needed to investigate the role attention plays in most users displaying privacy behavior such as oversharing on SMSs that does not align with the privacy concerns that they have. The amount of information laid out on the screen of SMSs is so great. Each piece of information is competing for our attention constantly. It could be the reason why some users claim to not notice or remember seeing a privacy notification about their current privacy setting to control oversharing (Kroll, 2021, p.13). Eye tracking data of users through research could be used to provide insights into where on the screen of certain SMSs that users fixate their attention on the most. The data could serve as a guide towards better placement of privacy nudges to where they are in the correct area of the screen in a user’s visual focus of attention.\nAnother research possibility could be to continue investigating how well personalized privacy nudges tailored to a specific user to limit oversharing is better than privacy nudges that are designed to be standardized and a one-size-fits-all model (Wisniewski, 2017, p. 96). Further research in this area could help continue to create or build upon existing classification systems of specific user types. This could be done according to privacy management strategies based on a user’s privacy behavior and awareness of privacy features on a SMS and then grouping users from there into privacy proficiency levels (Wisniewski, 2017, p. 106). It needs to be investigated not only with one SMS such as Facebook but with other SMSs such as Twitter, Instagram, Reddit, etc. that exist. The reason is that privacy behaviors amongst users is going to be different with each SMS because they are all going to have different privacy features and settings. But let’s look at Facebook as an example, anyway.\nThe classification system for Facebook that Wisniewski et al. (2017) has created is very helpful for potentially creating machine learning applications and programs, specifically, for Facebook that can help Facebook sort out what type of users on their platform falls into a specific categorical group based on the privacy management strategies they utilize. That way Facebook can use that to help them create personalized privacy nudges for each specific user based on where a user falls in the classification system.\nTo understand how this works, the group of Facebook users that Wisniewski et al (2017) analyzed were grouped into the categories of “Privacy Minimalists,” “Self-Censors,” Time Savers/Consumers,” “Privacy Balancers,” and “Privacy Maximizers” based on which 11 privacy behaviors they commonly utilized, and which 6 privacy features they were commonly aware of. The privacy management strategies utilized by each group based on their privacy behavior of using Facebook and privacy feature awareness of utilizing Facebook’s privacy features were also used to further classify them as being either “Novice,” “Near-Novice,” “Mostly Novice,” “Some Expertise,” “Near-Experts,” and “Experts” (Wisniewski, 2017, p. 103). To see how this came together, Wisniewski et al (2017) classified “Privacy Maximizers” as either “Experts” or “Near-Experts;” “Privacy Balancers” as  either “Experts,” “Near-Experts,” “Some Expertise,” or “Novice;” “Privacy Minimalists” as either “Mostly Novices” or “Near-Novices;” “Selective Sharers” as either “Experts” or “Near Experts;” and both “Time Sharers/Consumers” and “Self-Censors” as either “Mostly Novice” or “Some Expertise.”\nAs one can see, such a classification system like this still needs more refinement through more research because there is not really a one-to-one linear relationship between these categories (Wisniewski, 2017, p. 103). However, it is a step in the right direction towards helping make privacy nudges more effective at limiting oversharing.Another idea that has shown promising results, but still needs more research to help refine it is a system devised by Ziegeldorf et al, (2020) called Comparison-based Privacy (CbP).\nIt is built on the same concept as the classification system created by Wisniewski et al (2017) in that the goal is not to create standardized, one-size-fits-all privacy nudges, but to personalized them for a specific type of user. The CbP approach is not really a classification system but more of a comparison-based system that seems to be designed towards being programmed into a computer application that utilizes aspects of machine learning. Basically, CbP compares a user’s sharing behavior to the sharing behavior of different groups of users within a SMS (Ziegeldorf, 2016, p. 227). That user’s sharing behavior is not just being compared to random groups of users but based on certain comparison metrics and comparison groups (Ziegeldorf, 2016, p. 227).\nHow this works is, first, the sharing behavior of a user is determined based upon certain comparison metrics such as amount of shared content, usage patterns, etc. (Ziegeldorf, 2016, p. 227). Second, the overall profile of the user such as family, friends, colleagues, profession, age, etc. is collected to determine certain user groups that the user can relate to that can be used as the comparison group (Ziegeldorf, 2016, p. 227). Third, the averaged overall sharing behavior of the comparison group is compared to the sharing behavior of the user (Ziegeldorf, 2016, p. 227). Finally, personalized privacy nudges are generated and provided to the user to limit any sharing behavior that may not have been commonly found in the comparison group (Ziegeldorf, 2016, p. 227). The CbP approach found some relative success when it was used with a collection of tweets from the SMS, Twitter (Ziegeldorf, 2016, p. 232).\nBased on half a million tweets from 1,839 Twitter users with 659 users being teachers, 542 users being nurses, 559 users being journalists, and 79 users being U.S. senators, it was discovered that all groups were very restrictive about their location by keeping their tweets from being tagged with a geo-location (Ziegeldorf, 2016, p. 232). Above 90% of users disclosed their location in less than 7.8% of their tweets (Ziegeldorf, 2016, p. 232). Ziegeldorf et al (2016) found that the CbP approach was able to reflect that fact. For the “abusive language” metric, it was discovered that journalists and politicians use very little abusive language, whereas nurses and teachers used quite a bit of it (Ziegeldorf, 2016, p. 232). However, Ziegeldorf et al (2016) found that the CbP approach would rather nudge the politician more so than the nurse due to the relative norm of privacy and lack of user-specific comparison groups because some amount of abusive language was tolerable in certain groups.\nFurthermore, with the collection of the top 300 job-haters from a site called “FireMe!”as a contrast group by using the metrics such as disclosing more locations than others, having significantly higher rates of abusive language, and tweeting with clearly more negative sentiment, the CbP approach was not able to reveal to individual users that they were at risk of losing their job, but it was still able to nudge them away from those harmful sharing behaviors. This shows that although this approach has the potential to effectively nudge users towards limiting their sharing behavior, its functionality still needs to be polished through more study.\nFinally, another research possibility could investigate which privacy features and settings from each SMS performs better in helping users display more privacy awareness. This could help researchers and policymakers figure out which types of features and settings could be used as privacy nudges to help improve the design of interfaces for privacy and security purposes across all SMSs. Now, let’s take what is known about the problem of oversharing, its research, and the research possibilities being considered and think of ways a possible applied cybersecurity policy could be implemented.\n\n\n\nOne idea that can be implemented into an applied cybersecurity policy is to encourage users through privacy nudges to have them reconsider avoiding accepting friend requests from users that they have weak ties to. Users with weak ties are those who are random strangers or volatile acquaintances with low intimacy whereas users with strong ties are those who are good friends or relatives that a user is already familiar with and has a close friendship (Kroll, 2021, p. 4). Encouraging users to either reconsider being friends or deleting friendships with remote strangers could protect them from being victims of potential cyber-stalking or identity theft (Kroll, 2021, p. 4).\nA second idea would be to organize privacy controls and settings in such a way that they are more transparent, consistent, and user-friendly to the user because just the availability and presence of privacy settings, themselves, can help increase the willingness amongst users to protect their personal data (Kroll, 2021, p. 4). The reason for this is that we must consider the possibility that the reason most users’ privacy behavior does not reflect their privacy concerns is that privacy nudges designed to limit oversharing are not always placed in their visual focus of attention. This is because SMSs have such a high volume of content on the screen and it is hard to wade through all the pieces of information competing for our attention (Kroll, 2021, p. 13). With this in mind, an important privacy nudge that could be placed in the visual focus of attention of first-time SMS users would be redirecting users to set up their privacy preferences if they have not already done so. That way when they are making their first post, they are not staying with the default settings (Kroll, 2021, p. 5).\nA third idea would be informing the user of where their post is going and who it is going to reach to help users be aware of where the content being shared is going. This may help prevent users from sharing unwanted posts (Kroll, 2021, p. 5). Mainly, because experiments with Facebook users according to Kroll, et al (2021) have shown that showing the post’s reach or delaying it to give the user a chance to reconsider sending it was valued by users and reduced regret over sharing content, especially amongst users who have employed certain coping mechanisms to counter the feelings of regret after sharing content they felt they should not have shared. Reducing regret over sharing content amongst users can help make their social networking experience more enjoyable.   \nA fourth and final idea would be to provide privacy nudges that are personalized for a specific user, not a standardized, one-size-fits-all nudge (Wisniewski, 2017, p. 95). Mainly, because research has shown personalized nudges may be more effective since if a nudge is contrary to a user’s own privacy management strategy, then it could be viewed as a hindrance (Wisniewski, 2017, p. 96). Every default setting, feedback, or optimal hint may need to be different for each user (Wisniewski, 2017, p. 96). It is important to remember that privacy education designed to help users learn how to use an elaborate set of privacy controls fails to take a user’s existing proficiency into account most of the time (Wisniewski, 2017, p. 96). Every user is different and complex.\nThe classification system laid out by Wisniewski et al (2017), although it may need some refinement, is a good framework from which to work from to help design and provide personalized privacy nudges to limit oversharing. The CbP approach by Ziegeldorf et al (2016), which also needs some refinement, is also a good system to work from to help design and provide personalized privacy nudges for limiting oversharing. However, the best practice recommendation in employing an application that utilizes the CbP approach is to leave it out of the hands of third-party operators. This avoids requiring the user to trust an additional entity for privacy purposes (Ziegeldorf, 2016, p. 231).\nTwo ways to do this is to, first, allow the site operator of the SMS, itself, to run the application or, second, allow the user to run the application as a browser plugin (Ziegeldorf, 2016, p. 231). A second-best practice recommendation is to avoid information leaks to keep a user’s information private by applying differential privacy (Ziegeldorf, 2016, p. 231). What this means is that the user’s information is withheld from the site operator when publicly sharing information about a dataset used to describe the behavioral patterns of groups within it (Ziegeldorf, 2016, p. 231). The final best practice recommendation, considering the site operator of an SMS may not always be trustworthy, is to ensure the application with the CbP system filters out outliers. This is to either prevent an attacker from manipulating the aggregated behavior of a comparison group or prevent the same data, itself, from unintentionally steering the user towards the wrong direction of making ill-advised privacy decisions.\nOverall, it is the hope that delineating the problem of oversharing on SMSs in this era helps show how it was begotten by the concept of what SMSs are all about. Moreover, understanding this, hopefully, has brought about an awareness of the potential consequences of engaging in this behavior on SMSs. Further understanding of the psychological underpinnings of oversharing, hopefully, makes it clear why we as users need to take heed of those potential consequences because anyone could be at risk of becoming victims of cybercrime from engaging in this behavior. Finally, it is the hope that presenting potential research and applied cybersecurity policy ideas can help advance our understanding of oversharing on SMSs and continue developing better solutions.  \n\n\n\nBergram, K., Gjerlufsen, T., Maingot, P., Bezençon, V., Holzer, A. (2020). Digital nudges for\nprivacy awareness: From consent to informed consent?. Twenty-Eighth European\nConference on Information Systems. 1-13.\nKroll, T. & Stieglitz, S. (2021). Digital nudging and privacy: Improving decisions about self-\ndisclosure in social networks. Behaviour & Information Technology, 40(1), 1-19.\nhttps://doi.org/10.1080/0144929X.2019.1584644\nWisniewski, P. J.,  Knijnenburg, B. P., & Lipford, H. R. (2017). Making privacy personal:\nProfilingsocial network users to inform privacy education and nudging. Int. J. Human-\nComputer Studies, 98, 95-108. http://dx.doi.org/10.1016/j.ijhcs.2016.09.006\nZiegeldorf, J. H., Henze, M., Hummen, R., & Wehrle, K. (2016). Comparison-based privacy:\nNudging privacy in social media. Springer International Publishing Switzerland, 226-\n234. 10.1007/978-3-319-29883-2.15"
  },
  {
    "objectID": "policy_paper.html#i.-the-behavior-of-oversharing",
    "href": "policy_paper.html#i.-the-behavior-of-oversharing",
    "title": "Policy Paper on Oversharing",
    "section": "",
    "text": "Oversharing, also called self-disclosure, is a common behavior that has been observed on social media sites (SMSs) such as Facebook and Twitter. Sharing information, in and of itself, is one of the foundational objectives that SMSs were created to achieve. It allows for users to create accounts called profiles to allow users to get in touch with long lost childhood friends, classmates, distant family members, like-minded people, etc. by sharing personal information about themselves to search for and connect with certain people or make it easier for those people to search for them. The more users interact with SMSs and socialize with others, the more information they share. This is a boon to SMSs because the more information users share, the more revenue they generate through advertising. Not only will more relationships between users become strengthened and attract more primary users, but it will also attract more secondary users, the advertisers. This raises concerns over privacy and cybersecurity.\nOne concern is becoming victims of cybercrime (Ziegeldorf, 2016, p. 226).  There is always the risk of information abuse that comes with increased sharing on SMSs (Kroll, 2021, p. 3). One’s information could be shared with and collected by third parties such as advertisers, for example, and used for malicious purposes such as cyber stalking and identity theft (Kroll, 2021, p. 3). Cyber stalking can involve users experiencing threats, sexual harassment, and conflicts (Kroll, 2021, p. 4). Identity theft could be the vulnerability of falling prey to a cybercrime called spear-phishing. Spear-phishing is where emails are targeted to a specific individual based on personal information that the attacker knows about that individual. The attacker uses that information to coax that individual into clicking on an email that looks very legitimate in the hopes of getting that individual to divulge more sensitive personal information. One example is login information to gain access to bank accounts. One of the ways one’s personal information can fall into the hands of an attacker is through oversharing on SMSs. Attackers can use any information one shares on SMSs and figure out a pattern that helps them determine one’s lifestyle and habits. Ways those attackers can collect information about you is by either acting as a troll or employing social bots.\nTrolls are real human users on SMSs with fake accounts. Social bots are computer programs posing as real human users on SMSs with fake accounts. They can take advantage of algorithms on SMSs that advertisers use to help them figure out what advertisements to share with certain users that they think a specific user would most likely be interested in seeing. This is determined based on information that users share, post, like, click, and divulge. This helps trolls and social bots figure out patterns about a specific user’s lifestyle and habits. It also gives them opportunities to attempt swaying the outcome of elections during election season by spreading inflammatory political content to users who are most likely to believe it, true or not. Thus, creating “echo chambers” and “us versus them” confrontations on SMSs by preying on our cognitive biases. This can cause users to experience missed career opportunities and embarrass themselves towards either never getting hired for a job or fired from a job if such content pulls them into using harmful language on SMSs that they otherwise would never use in face-to-face interactions (Ziegeldorf, 2016, p. 226). Researchers have come up with different reasons why users overshare on SMSs and put themselves at risk.\nDespite privacy concerns they may have, users do not always observe privacy-protecting behaviors that reflect their concerns (Wisniewski, 2017, p. 95). This could be a result of limited motivation to control their privacy by taking the route that is good enough instead of one that is more optimal (Wisniewski, 2017, p. 95). Or biases may be playing a role against users’ better judgement because users may be experiencing information overload or feel they have nothing to hide (Bergram, 2020, p. 2). Bergram et al. (2020) pointed out that 62% of a sample of users surveyed in the U.S. believed that if a website had a privacy policy that it meant it could not share their data with other companies. Not surprising because it is hard to get a sense of any SMSs position on the problem of over-sharing since they are still businesses (Kroll, 2021, p. 2). They are still going to do what is necessary with whatever security system they put in place to increase user interaction and sharing (Kroll, 2021, p. 2). It is hard to know if the feeling of a secure and trustworthy platform they are promoting is all just security theater or a genuine attempt to toughen security measures (Kroll, 2021, p. 2). This is because the desire to share is influenced by trust in the SMS and a feeling of control (Kroll, 2021, p. 4). Even if privacy controls are available to users, they may also be either unwilling or unable to take the time to learn and educate themselves on the controls that they can use to their advantage (Wisniewski, 2017, p. 95).\nThis could be the result of users having trouble navigating elaborate privacy options that are available to them (Wisniewski, 2017, p. 95). Wisniewski, et al. (2017) found that 48% of users on SMSs have expressed difficulty understanding how to manage their SMSs privacy settings. Most Facebook users do not seem to know the consequences of setting up their own privacy settings in a certain way based on the fact common in most users, they share content that goes against the disclosure intentions they uphold (Wisniewski, 2017, p. 96). Because of these behaviors, users do not always use the privacy controls available to them to take control over their data. By staying with the permissive privacy control defaults and having a lack of knowledge or motivation to control their own privacy, oversharing becomes predominant among users (Wisniewski, 2017, p. 96). To curb the problem of oversharing, researchers should consider or continue considering some research possibilities."
  },
  {
    "objectID": "policy_paper.html#ii.-potential-research-ideas-for-solutions-to-limit-oversharing",
    "href": "policy_paper.html#ii.-potential-research-ideas-for-solutions-to-limit-oversharing",
    "title": "Policy Paper on Oversharing",
    "section": "",
    "text": "One research possibility could be to investigate how many users have added random strangers and by investigating further how often users add random strangers on a certain scale (i.e., all the time, most of the time, neutral, sometimes, never). However, to advance this further as it relates to the basis of why users overshare, researchers should consider understanding why users add random strangers as friends or request to be their friends in the first place (Kroll, 2021, p. 4). There is probably a psychological basis to this that involves the issue of low self-esteem and the feeling of being wanted and accepted by others. Involvement on social media for some users may be more of a popularity contest since there are users who have been successful as social media influencers and made a living out of that lifestyle. This could bring about potential privacy concerns regarding oversharing because a user may not be aware of whether a random stranger is a potential troll, social bot, hacker, or attacker who could prey upon that user’s oversharing behavior to commit crimes such as cyber-stalking or identity theft. Research in this area could help lead towards ways of designing privacy nudges, or better ones, to help users avoid or reconsider adding certain users to their circle of friends that they do not know. Yet, it could also reveal coping strategies users utilize that could be helpful towards the design of privacy nudges.\nResearch is needed to investigate what coping strategies users use to limit their own self-disclosure (Kroll, 2021, p. 4). Researchers could ask users if they have stopped disclosing information by not creating further posts, deleting past posts, deleting their account, changing the visibility of new and old posts, or deleting friendships with remote strangers (Kroll, 2021, p. 4). From there, researchers can glean some more coping strategies from users that users have used to deal with difficult users online that they do not know and have regretted adding as friends or have used after regretting sharing certain online content. This could help create a classification system of coping strategies to help design or improve designs of privacy nudges used to nudge users towards consideration of certain coping strategies to help limit oversharing.\nAnother research possibility could be to investigate if privacy campaigns lead to an increased awareness amongst users of privacy and the privacy settings available to them to help them make informed decisions on how to control their sharing (Kroll, 2021, p. 5). There is a need to discover which privacy settings are more desirable to users and which ones SMSs are creating are the desired or actual kinds of privacy settings that researchers and policymakers are looking for or are needed to be created and presented to users (Kroll, 2021, p. 5). However, it might be a good idea to investigate the role that attention plays on users’ privacy awareness in terms of their awareness of privacy settings and features.\nResearch is needed to investigate the role attention plays in most users displaying privacy behavior such as oversharing on SMSs that does not align with the privacy concerns that they have. The amount of information laid out on the screen of SMSs is so great. Each piece of information is competing for our attention constantly. It could be the reason why some users claim to not notice or remember seeing a privacy notification about their current privacy setting to control oversharing (Kroll, 2021, p.13). Eye tracking data of users through research could be used to provide insights into where on the screen of certain SMSs that users fixate their attention on the most. The data could serve as a guide towards better placement of privacy nudges to where they are in the correct area of the screen in a user’s visual focus of attention.\nAnother research possibility could be to continue investigating how well personalized privacy nudges tailored to a specific user to limit oversharing is better than privacy nudges that are designed to be standardized and a one-size-fits-all model (Wisniewski, 2017, p. 96). Further research in this area could help continue to create or build upon existing classification systems of specific user types. This could be done according to privacy management strategies based on a user’s privacy behavior and awareness of privacy features on a SMS and then grouping users from there into privacy proficiency levels (Wisniewski, 2017, p. 106). It needs to be investigated not only with one SMS such as Facebook but with other SMSs such as Twitter, Instagram, Reddit, etc. that exist. The reason is that privacy behaviors amongst users is going to be different with each SMS because they are all going to have different privacy features and settings. But let’s look at Facebook as an example, anyway.\nThe classification system for Facebook that Wisniewski et al. (2017) has created is very helpful for potentially creating machine learning applications and programs, specifically, for Facebook that can help Facebook sort out what type of users on their platform falls into a specific categorical group based on the privacy management strategies they utilize. That way Facebook can use that to help them create personalized privacy nudges for each specific user based on where a user falls in the classification system.\nTo understand how this works, the group of Facebook users that Wisniewski et al (2017) analyzed were grouped into the categories of “Privacy Minimalists,” “Self-Censors,” Time Savers/Consumers,” “Privacy Balancers,” and “Privacy Maximizers” based on which 11 privacy behaviors they commonly utilized, and which 6 privacy features they were commonly aware of. The privacy management strategies utilized by each group based on their privacy behavior of using Facebook and privacy feature awareness of utilizing Facebook’s privacy features were also used to further classify them as being either “Novice,” “Near-Novice,” “Mostly Novice,” “Some Expertise,” “Near-Experts,” and “Experts” (Wisniewski, 2017, p. 103). To see how this came together, Wisniewski et al (2017) classified “Privacy Maximizers” as either “Experts” or “Near-Experts;” “Privacy Balancers” as  either “Experts,” “Near-Experts,” “Some Expertise,” or “Novice;” “Privacy Minimalists” as either “Mostly Novices” or “Near-Novices;” “Selective Sharers” as either “Experts” or “Near Experts;” and both “Time Sharers/Consumers” and “Self-Censors” as either “Mostly Novice” or “Some Expertise.”\nAs one can see, such a classification system like this still needs more refinement through more research because there is not really a one-to-one linear relationship between these categories (Wisniewski, 2017, p. 103). However, it is a step in the right direction towards helping make privacy nudges more effective at limiting oversharing.Another idea that has shown promising results, but still needs more research to help refine it is a system devised by Ziegeldorf et al, (2020) called Comparison-based Privacy (CbP).\nIt is built on the same concept as the classification system created by Wisniewski et al (2017) in that the goal is not to create standardized, one-size-fits-all privacy nudges, but to personalized them for a specific type of user. The CbP approach is not really a classification system but more of a comparison-based system that seems to be designed towards being programmed into a computer application that utilizes aspects of machine learning. Basically, CbP compares a user’s sharing behavior to the sharing behavior of different groups of users within a SMS (Ziegeldorf, 2016, p. 227). That user’s sharing behavior is not just being compared to random groups of users but based on certain comparison metrics and comparison groups (Ziegeldorf, 2016, p. 227).\nHow this works is, first, the sharing behavior of a user is determined based upon certain comparison metrics such as amount of shared content, usage patterns, etc. (Ziegeldorf, 2016, p. 227). Second, the overall profile of the user such as family, friends, colleagues, profession, age, etc. is collected to determine certain user groups that the user can relate to that can be used as the comparison group (Ziegeldorf, 2016, p. 227). Third, the averaged overall sharing behavior of the comparison group is compared to the sharing behavior of the user (Ziegeldorf, 2016, p. 227). Finally, personalized privacy nudges are generated and provided to the user to limit any sharing behavior that may not have been commonly found in the comparison group (Ziegeldorf, 2016, p. 227). The CbP approach found some relative success when it was used with a collection of tweets from the SMS, Twitter (Ziegeldorf, 2016, p. 232).\nBased on half a million tweets from 1,839 Twitter users with 659 users being teachers, 542 users being nurses, 559 users being journalists, and 79 users being U.S. senators, it was discovered that all groups were very restrictive about their location by keeping their tweets from being tagged with a geo-location (Ziegeldorf, 2016, p. 232). Above 90% of users disclosed their location in less than 7.8% of their tweets (Ziegeldorf, 2016, p. 232). Ziegeldorf et al (2016) found that the CbP approach was able to reflect that fact. For the “abusive language” metric, it was discovered that journalists and politicians use very little abusive language, whereas nurses and teachers used quite a bit of it (Ziegeldorf, 2016, p. 232). However, Ziegeldorf et al (2016) found that the CbP approach would rather nudge the politician more so than the nurse due to the relative norm of privacy and lack of user-specific comparison groups because some amount of abusive language was tolerable in certain groups.\nFurthermore, with the collection of the top 300 job-haters from a site called “FireMe!”as a contrast group by using the metrics such as disclosing more locations than others, having significantly higher rates of abusive language, and tweeting with clearly more negative sentiment, the CbP approach was not able to reveal to individual users that they were at risk of losing their job, but it was still able to nudge them away from those harmful sharing behaviors. This shows that although this approach has the potential to effectively nudge users towards limiting their sharing behavior, its functionality still needs to be polished through more study.\nFinally, another research possibility could investigate which privacy features and settings from each SMS performs better in helping users display more privacy awareness. This could help researchers and policymakers figure out which types of features and settings could be used as privacy nudges to help improve the design of interfaces for privacy and security purposes across all SMSs. Now, let’s take what is known about the problem of oversharing, its research, and the research possibilities being considered and think of ways a possible applied cybersecurity policy could be implemented."
  },
  {
    "objectID": "policy_paper.html#iii.-potential-applied-cybersecurity-policy-to-help-limit-oversharing",
    "href": "policy_paper.html#iii.-potential-applied-cybersecurity-policy-to-help-limit-oversharing",
    "title": "Policy Paper on Oversharing",
    "section": "",
    "text": "One idea that can be implemented into an applied cybersecurity policy is to encourage users through privacy nudges to have them reconsider avoiding accepting friend requests from users that they have weak ties to. Users with weak ties are those who are random strangers or volatile acquaintances with low intimacy whereas users with strong ties are those who are good friends or relatives that a user is already familiar with and has a close friendship (Kroll, 2021, p. 4). Encouraging users to either reconsider being friends or deleting friendships with remote strangers could protect them from being victims of potential cyber-stalking or identity theft (Kroll, 2021, p. 4).\nA second idea would be to organize privacy controls and settings in such a way that they are more transparent, consistent, and user-friendly to the user because just the availability and presence of privacy settings, themselves, can help increase the willingness amongst users to protect their personal data (Kroll, 2021, p. 4). The reason for this is that we must consider the possibility that the reason most users’ privacy behavior does not reflect their privacy concerns is that privacy nudges designed to limit oversharing are not always placed in their visual focus of attention. This is because SMSs have such a high volume of content on the screen and it is hard to wade through all the pieces of information competing for our attention (Kroll, 2021, p. 13). With this in mind, an important privacy nudge that could be placed in the visual focus of attention of first-time SMS users would be redirecting users to set up their privacy preferences if they have not already done so. That way when they are making their first post, they are not staying with the default settings (Kroll, 2021, p. 5).\nA third idea would be informing the user of where their post is going and who it is going to reach to help users be aware of where the content being shared is going. This may help prevent users from sharing unwanted posts (Kroll, 2021, p. 5). Mainly, because experiments with Facebook users according to Kroll, et al (2021) have shown that showing the post’s reach or delaying it to give the user a chance to reconsider sending it was valued by users and reduced regret over sharing content, especially amongst users who have employed certain coping mechanisms to counter the feelings of regret after sharing content they felt they should not have shared. Reducing regret over sharing content amongst users can help make their social networking experience more enjoyable.   \nA fourth and final idea would be to provide privacy nudges that are personalized for a specific user, not a standardized, one-size-fits-all nudge (Wisniewski, 2017, p. 95). Mainly, because research has shown personalized nudges may be more effective since if a nudge is contrary to a user’s own privacy management strategy, then it could be viewed as a hindrance (Wisniewski, 2017, p. 96). Every default setting, feedback, or optimal hint may need to be different for each user (Wisniewski, 2017, p. 96). It is important to remember that privacy education designed to help users learn how to use an elaborate set of privacy controls fails to take a user’s existing proficiency into account most of the time (Wisniewski, 2017, p. 96). Every user is different and complex.\nThe classification system laid out by Wisniewski et al (2017), although it may need some refinement, is a good framework from which to work from to help design and provide personalized privacy nudges to limit oversharing. The CbP approach by Ziegeldorf et al (2016), which also needs some refinement, is also a good system to work from to help design and provide personalized privacy nudges for limiting oversharing. However, the best practice recommendation in employing an application that utilizes the CbP approach is to leave it out of the hands of third-party operators. This avoids requiring the user to trust an additional entity for privacy purposes (Ziegeldorf, 2016, p. 231).\nTwo ways to do this is to, first, allow the site operator of the SMS, itself, to run the application or, second, allow the user to run the application as a browser plugin (Ziegeldorf, 2016, p. 231). A second-best practice recommendation is to avoid information leaks to keep a user’s information private by applying differential privacy (Ziegeldorf, 2016, p. 231). What this means is that the user’s information is withheld from the site operator when publicly sharing information about a dataset used to describe the behavioral patterns of groups within it (Ziegeldorf, 2016, p. 231). The final best practice recommendation, considering the site operator of an SMS may not always be trustworthy, is to ensure the application with the CbP system filters out outliers. This is to either prevent an attacker from manipulating the aggregated behavior of a comparison group or prevent the same data, itself, from unintentionally steering the user towards the wrong direction of making ill-advised privacy decisions.\nOverall, it is the hope that delineating the problem of oversharing on SMSs in this era helps show how it was begotten by the concept of what SMSs are all about. Moreover, understanding this, hopefully, has brought about an awareness of the potential consequences of engaging in this behavior on SMSs. Further understanding of the psychological underpinnings of oversharing, hopefully, makes it clear why we as users need to take heed of those potential consequences because anyone could be at risk of becoming victims of cybercrime from engaging in this behavior. Finally, it is the hope that presenting potential research and applied cybersecurity policy ideas can help advance our understanding of oversharing on SMSs and continue developing better solutions."
  },
  {
    "objectID": "policy_paper.html#references",
    "href": "policy_paper.html#references",
    "title": "Policy Paper on Oversharing",
    "section": "",
    "text": "Bergram, K., Gjerlufsen, T., Maingot, P., Bezençon, V., Holzer, A. (2020). Digital nudges for\nprivacy awareness: From consent to informed consent?. Twenty-Eighth European\nConference on Information Systems. 1-13.\nKroll, T. & Stieglitz, S. (2021). Digital nudging and privacy: Improving decisions about self-\ndisclosure in social networks. Behaviour & Information Technology, 40(1), 1-19.\nhttps://doi.org/10.1080/0144929X.2019.1584644\nWisniewski, P. J.,  Knijnenburg, B. P., & Lipford, H. R. (2017). Making privacy personal:\nProfilingsocial network users to inform privacy education and nudging. Int. J. Human-\nComputer Studies, 98, 95-108. http://dx.doi.org/10.1016/j.ijhcs.2016.09.006\nZiegeldorf, J. H., Henze, M., Hummen, R., & Wehrle, K. (2016). Comparison-based privacy:\nNudging privacy in social media. Springer International Publishing Switzerland, 226-\n234. 10.1007/978-3-319-29883-2.15"
  },
  {
    "objectID": "posts/2024-19-3 Reflection: Selective Attention in Cybersecurity/index.html",
    "href": "posts/2024-19-3 Reflection: Selective Attention in Cybersecurity/index.html",
    "title": "Reflection No. 1: Cognitive Essentials for Cybersecurity Topic on Attention",
    "section": "",
    "text": "Cognitive Psychology Essentials for Cybersecurity Reflection No. 1\nThe Cognitive Psychology Essentials for Cybersecurity course lecture on attention helped generate some scenarios in my mind on how security failures could be created that takes advantage of the pitfalls of selective attention. This looks at how multiple pieces of information on a computer interface is competing for the user’s attention, but the user decides to attend to only one of those pieces of information while simultaneously suppressing other irrelevant ones. A potential scenario that comes to mind that could portray as a security failure revolves around phishing attempts created around the presentation of open-source software.\n\nA novice investor could go to an open-source software on the internet to receive tips on how to become a savvy investor to achieve financial gains. People who use open-source software are those who don’t want to pay more money for a service if they can avoid it and selective attention can play a part in looking for these. Open-source software is not always well-designed and may present security vulnerabilities. An individual in cyberspace could obtain a user’s email and IP address through a user signing up for a newsletter on investing tips and track the user’s internet activity. This individual could track which website the user goes for banking and send a phishing email that preys on the investor’s desire to take risks to achieve financial gains. By sending an email that pretends to be the investor’s banking institution, a ruse can be created about how there’s a great investing opportunity offered through that bank with a link to a login page. This is where the pitfall of selective attention comes into play. The investment opportunity appears attractive with a link to a login page to seize this opportunity that looks almost exactly like the user’s bank login page while ignoring other small details about the email or URL address of the login page that gives it away as being fake. Before the user knows it, the user’s log-in and personal information is stolen after logging in."
  },
  {
    "objectID": "hotel_website_redesign.html#website-redesign-final-deliverable-presentation",
    "href": "hotel_website_redesign.html#website-redesign-final-deliverable-presentation",
    "title": "Hotel Website Redesign",
    "section": "",
    "text": "This is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "hotel_website_redesign.html#website-redesign-prototype",
    "href": "hotel_website_redesign.html#website-redesign-prototype",
    "title": "Hotel Website Redesign",
    "section": "Website Redesign Prototype",
    "text": "Website Redesign Prototype"
  },
  {
    "objectID": "easy_internet_app.html#easy-internet-app-final-deliverable-presentation",
    "href": "easy_internet_app.html#easy-internet-app-final-deliverable-presentation",
    "title": "Easy Internet App",
    "section": "",
    "text": "This is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "easy_internet_app.html#easy-internet-app-design-prototype",
    "href": "easy_internet_app.html#easy-internet-app-design-prototype",
    "title": "Easy Internet App",
    "section": "Easy Internet App Design Prototype",
    "text": "Easy Internet App Design Prototype"
  },
  {
    "objectID": "posts/2024-26-3 Reflection: Confirmation Bias in Cybersecurity/confirmation_bias.html",
    "href": "posts/2024-26-3 Reflection: Confirmation Bias in Cybersecurity/confirmation_bias.html",
    "title": "Reflection No. 2: Cognitive Essentials for Cybersecurity Topic on Confirmation bias",
    "section": "",
    "text": "The topic of confirmation bias has shown that solutions to tackling confirmation bias by technology companies, especially through social media platforms where echo chambers are easily created, can be provided externally. However, solving the problem of echo chambers due to the influence of confirmation bias will still have to come with a great deal of personal responsibility through the user’s willingness to change his/her cognitive behavior. In doing so, an external solution is still going to be needed to create environmental conditions that help the user make that behavior change.\n\nOne solution could be to have an algorithm that focuses on categories of what a user likes based on his/her user profile instead of how many likes or views of content or responses to content he/she has completed. If all content presented was only based on content a user has liked, viewed, or responded to then he/she will get inundated with the same content of a specific category repeatedly, especially if it is all political news sources with a heavy bias to one side filled with misinformation. This ensures the user does not get worked up over repeated hits of the same political content through an echo chamber. And that he/she gets more diversification of content he/she likes to ensure the user comes back to the platform and the company does not lose users and money.\n\nWhile it would be user-friendly to give the user the freedom to set his/her filters, he/she cannot be expected to change his/her behavior on his/her own if extreme cognitive bias takes root. Because many people rely on social media for informing others of events such as political rallies, from a cybersecurity perspective there is an obligation to society to ensure these events do not lead to riots, insurrections, and violence or the creation of hate groups that could be harmful to communities and civilizations if the creation of echo chambers get out of hand."
  },
  {
    "objectID": "posts/2024-26-3 Reflection: \"Nudging\" in Cybersecurity/index.html",
    "href": "posts/2024-26-3 Reflection: \"Nudging\" in Cybersecurity/index.html",
    "title": "Reflection No. 3: Cognitive Essentials for Cybersecurity Topic on Nudging",
    "section": "",
    "text": "The example of a “nudge” of presenting healthy foods at the forefront of our visual focus of attention and unhealthy foods in our peripheral unfocused area of attention helped me brainstorm and investigate some ideas of an example of a “nudge” that could be used to help users make safe choices as it relates to cybersecurity. The most common one that comes to mind is helping users create stronger passwords by informing them if the password that was created was either a weak or strong one. Another one, although has been ethically abused frequently, is offering a user a free trial to use a service for a certain number of days instead of making an immediate decision to pay for any one of the subscription offerings. But one that I would like to see implemented, or more often if it has, is either the option to lock payment information or not to save it brought to the forefront of a user’s visual focus of attention when entering payment information on an ordering app.\n\nWith more online ordering on the rise over the past few years and due to the Covid-19 pandemic, there is a good chance more people are saving their payment information on accounts created through apps and online shopping websites because it is convenient. After entering payment information, the user should be given the option of choosing either locking the card and payment information first, not saving it second, or saving it third. If they click the first visible option, users can then select dual-factor authentication, security questions, or fingerprint ID, etc. to secure it. Because there used to be a time when it was advised to enter only our credit card information instead of our debit card information and to reconsider saving payment information online for safe online shopping practices, presenting options in that way will give more users, or most of us, who may not observe these practices an opportunity to safeguard their payment information from potential unforeseen security breaches."
  },
  {
    "objectID": "perception_proposal.html",
    "href": "perception_proposal.html",
    "title": "Perception Research Proposal",
    "section": "",
    "text": "By Grant Powell\nSchool of Behavioral and Brain Sciences, University of Texas at Dallas\nACN 6332 Perception\nProfessor Rachna Raman\nMay 4, 2023\n\n\nThe focus of this research proposal is to answer the following research question: Where on the body is there the most sensitivity to soundwave vibrations as it hits the skin not when they are introduced through a medium attached to the skin, but when it is unattached and away near the skin’s surface? This idea is similarly based on a perceptual measurement concept from the 1800s called the Just Noticeable Difference (JND) according to Weber’s Law by German psycho-physicist Ernst Weber. It is centered around the question of what is the smallest detectable change in the intensity of a stimulus needed for a person to notice that either the stimulus is being perceived or there are two or more stimuli being perceived instead of one? Basically, a mean average relative threshold of noticing whether a stimulus or stimuli are being felt, heard, smelt, tasted, or viewed is being searched for. Although there have been experiments in the past that have utilized this concept to answer research questions like the one being proposed here, they were all done in a slightly different manner.\n\n\n\nIn a comprehensive bibliography by Proctor (1984), the first experiment on testing sensitivity to soundwave vibrations through touch to determine if they can be heard this way was in 1924 by Robert H. Gault. Gault transmitted his own speech soundwave vibrations by speaking through a tube’s 14-foot-long air column that contacted the surface of the palm of a normal-hearing subject’s hand. The subject whose hearing was completely muted in the lab was able to correctly match the vibrations with 34 English words in non-sentence form and comprehend the meaning of those words in sentence form in various combinations (Gault, R.H., 1924 & 1926). Through a Gault-teletactor in an experiment by Goodfellow (1934), it was found, though not statistically significant, that at the lowest possible decibel the highest sensitivity amongst 13 body areas of 2 totally deaf and 4 normal hearing subjects to speech and music soundwave vibrations at 64 Hz was in the left index finger, at 256 Hz was in the palm of the right hand, and at 1,024 Hz was in the fingernail of the right index finger. Although these experiments and later experiments that expanded on these findings only used measurement apparatuses that contacted the skin and put a higher focus on touch sensitivity to speech soundwave vibrations instead of music soundwave vibrations (Proctor, A, 1984), the goal of the proposed research is still going to be like the experiment by Goodfellow.\n\n\n\nTherefore, the following research hypothesis will be posited: When introduced to music soundwave vibrations through a medium unattached and away near the skin’s surface, any part of the hand will be considered the body area most sensitive to soundwave vibrations in comparison to the other body areas.\n\n\n\nThe proposed research will use an apparatus of some kind involving a sound speaker fitted with a cone over it to help filter the sound through the hole of the cone onto a specific body area without touching the skin much in the same way that the pinna of the human ear helps filter soundwaves directly into the ear. The same frequencies of soundwaves in Goodfellow (1934) will be introduced through the apparatus onto the skin region of a certain body area. First, it will make direct contact onto the skin to develop a baseline decibel range that can be felt. Second, the apparatus will be moved back .5 inches away from the skin’s surface and the decibel knob turned down until the frequency of the soundwave vibrations cannot be felt. Third, continue repeating step two until the frequency, itself, cannot be felt and measure the distance from the body area.\n\n\n\nGault, R.H. (1924). Progress in experiments on tactual interpretation of oral speech. The Journal of Abnormal Psychology and Social Psychology, 19(2), 155-159. https://doi.org/10.1037/h00657 52 (/doi/10.1037/h0065752)\nGault, R.H. (1926). Tactual interpretation of speech. The Scientific Monthly, 22(2), 126-131. https://www.jstor.org/stable/7438\nGoodfellow, L.D. (1934). The sensitivity of various areas of the body to vibratory stimuli. The Journal of General Psychology, 11(2), 435-440, https://doi.org/10.1080/00221309.1934.9917848\nProctor, A. (1984). Tactile aids for the deaf: A comprehensive bibliography. American Annals of the Deaf, 129(5), 409-416. https://www.jstor.org/stable/44399523"
  },
  {
    "objectID": "posts/2024-26-3 Reflection: Confirmation Bias in Cybersecurity/index.html",
    "href": "posts/2024-26-3 Reflection: Confirmation Bias in Cybersecurity/index.html",
    "title": "Reflection No. 2: Cognitive Essentials for Cybersecurity Topic on Confirmation Bias",
    "section": "",
    "text": "The topic of confirmation bias has shown that solutions to tackling confirmation bias by technology companies, especially through social media platforms where echo chambers are easily created, can be provided externally. However, solving the problem of echo chambers due to the influence of confirmation bias will still have to come with a great deal of personal responsibility through the user’s willingness to change his/her cognitive behavior. In doing so, an external solution is still going to be needed to create environmental conditions that help the user make that behavior change.\n\nOne solution could be to have an algorithm that focuses on categories of what a user likes based on his/her user profile instead of how many likes or views of content or responses to content he/she has completed. If all content presented was only based on content a user has liked, viewed, or responded to then he/she will get inundated with the same content of a specific category repeatedly, especially if it is all political news sources with a heavy bias to one side filled with misinformation. This ensures the user does not get worked up over repeated hits of the same political content through an echo chamber. And that he/she gets more diversification of content he/she likes to ensure the user comes back to the platform and the company does not lose users and money.\n\nWhile it would be user-friendly to give the user the freedom to set his/her filters, he/she cannot be expected to change his/her behavior on his/her own if extreme cognitive bias takes root. Because many people rely on social media for informing others of events such as political rallies, from a cybersecurity perspective there is an obligation to society to ensure these events do not lead to riots, insurrections, and violence or the creation of hate groups that could be harmful to communities and civilizations if the creation of echo chambers get out of hand."
  },
  {
    "objectID": "perception_proposal.html#purpose",
    "href": "perception_proposal.html#purpose",
    "title": "Perception Research Proposal",
    "section": "",
    "text": "The focus of this research proposal is to answer the following research question: Where on the body is there the most sensitivity to soundwave vibrations as it hits the skin not when they are introduced through a medium attached to the skin, but when it is unattached and away near the skin’s surface? This idea is similarly based on a perceptual measurement concept from the 1800s called the Just Noticeable Difference (JND) according to Weber’s Law by German psycho-physicist Ernst Weber. It is centered around the question of what is the smallest detectable change in the intensity of a stimulus needed for a person to notice that either the stimulus is being perceived or there are two or more stimuli being perceived instead of one? Basically, a mean average relative threshold of noticing whether a stimulus or stimuli are being felt, heard, smelt, tasted, or viewed is being searched for. Although there have been experiments in the past that have utilized this concept to answer research questions like the one being proposed here, they were all done in a slightly different manner."
  },
  {
    "objectID": "perception_proposal.html#background",
    "href": "perception_proposal.html#background",
    "title": "Perception Research Proposal",
    "section": "",
    "text": "In a comprehensive bibliography by Proctor (1984), the first experiment on testing sensitivity to soundwave vibrations through touch to determine if they can be heard this way was in 1924 by Robert H. Gault. Gault transmitted his own speech soundwave vibrations by speaking through a tube’s 14-foot-long air column that contacted the surface of the palm of a normal-hearing subject’s hand. The subject whose hearing was completely muted in the lab was able to correctly match the vibrations with 34 English words in non-sentence form and comprehend the meaning of those words in sentence form in various combinations (Gault, R.H., 1924 & 1926). Through a Gault-teletactor in an experiment by Goodfellow (1934), it was found, though not statistically significant, that at the lowest possible decibel the highest sensitivity amongst 13 body areas of 2 totally deaf and 4 normal hearing subjects to speech and music soundwave vibrations at 64 Hz was in the left index finger, at 256 Hz was in the palm of the right hand, and at 1,024 Hz was in the fingernail of the right index finger. Although these experiments and later experiments that expanded on these findings only used measurement apparatuses that contacted the skin and put a higher focus on touch sensitivity to speech soundwave vibrations instead of music soundwave vibrations (Proctor, A, 1984), the goal of the proposed research is still going to be like the experiment by Goodfellow."
  },
  {
    "objectID": "perception_proposal.html#hypothesis",
    "href": "perception_proposal.html#hypothesis",
    "title": "Perception Research Proposal",
    "section": "",
    "text": "Therefore, the following research hypothesis will be posited: When introduced to music soundwave vibrations through a medium unattached and away near the skin’s surface, any part of the hand will be considered the body area most sensitive to soundwave vibrations in comparison to the other body areas."
  },
  {
    "objectID": "perception_proposal.html#method",
    "href": "perception_proposal.html#method",
    "title": "Perception Research Proposal",
    "section": "",
    "text": "The proposed research will use an apparatus of some kind involving a sound speaker fitted with a cone over it to help filter the sound through the hole of the cone onto a specific body area without touching the skin much in the same way that the pinna of the human ear helps filter soundwaves directly into the ear. The same frequencies of soundwaves in Goodfellow (1934) will be introduced through the apparatus onto the skin region of a certain body area. First, it will make direct contact onto the skin to develop a baseline decibel range that can be felt. Second, the apparatus will be moved back .5 inches away from the skin’s surface and the decibel knob turned down until the frequency of the soundwave vibrations cannot be felt. Third, continue repeating step two until the frequency, itself, cannot be felt and measure the distance from the body area."
  },
  {
    "objectID": "perception_proposal.html#reference",
    "href": "perception_proposal.html#reference",
    "title": "Perception Research Proposal",
    "section": "",
    "text": "Gault, R.H. (1924). Progress in experiments on tactual interpretation of oral speech. The Journal of Abnormal Psychology and Social Psychology, 19(2), 155-159. https://doi.org/10.1037/h00657 52 (/doi/10.1037/h0065752)\nGault, R.H. (1926). Tactual interpretation of speech. The Scientific Monthly, 22(2), 126-131. https://www.jstor.org/stable/7438\nGoodfellow, L.D. (1934). The sensitivity of various areas of the body to vibratory stimuli. The Journal of General Psychology, 11(2), 435-440, https://doi.org/10.1080/00221309.1934.9917848\nProctor, A. (1984). Tactile aids for the deaf: A comprehensive bibliography. American Annals of the Deaf, 129(5), 409-416. https://www.jstor.org/stable/44399523"
  },
  {
    "objectID": "clear_speech.html",
    "href": "clear_speech.html",
    "title": "Clear Speech",
    "section": "",
    "text": "Grant Powell\nSchool of Behavioral and Brain Sciences, University of Texas at Dallas\nACN 6763 Speech Perception\nProfessor Peter Assmann\nOctober 27, 2022\n\n\nIn everyday verbal communication, the fundamental goal amongst us, humans, is and has always been to find a way to communicate with each other in the clearest way possible. We try to achieve this goal from both the perspective as a listener and speaker. From the perspective of the speaker, we may try to communicate our message through our physical capabilities of how we manipulate the vocal tract or how we apply our knowledge of the overall structure of the language we speak with the utmost confidence that our listener is able to speak and understand the same language. From the perspective of the listener with that same confidence, we may try to communicate with the speaker by listening for certain vowel sounds, consonant sounds, pauses, spacing between words, sentence structure, or changes in pitch to indicate whether a question is being asked. However, no matter what language we speak and how much we improve it to communicate clearly, clear communication, overall, is still fundamentally dependent on our physical faculties of how we utilize our vocal tract and how our auditory system is functioning.\nResearchers have identified this goal of achieving clear communication through how we use our vocal tract and auditory system as “clear speech.” This type of speech is very much necessary in everyday communication settings where there is background noise and conversations involving listeners who are deaf or hard-of-hearing (Bradlow & Smiljanic, 2009). Since the focus of clear speech that is being investigated is based, primarily, on how the English language is used, the modifications that speakers have been observed of making are speaking slowly, speaking loudly, articulating in a more exaggerated manner, speaking with a higher voice pitch, and employing a more variable voice pitch (Bradlow & Smiljanic, 2009; Ferguson & Quene, 2014). The way that researchers have determined to measure clear speech is through global and segmental measurements.\nGlobal measurements involve measuring speaking rate, pause frequency and duration, fundamental frequency average and range, long term spectra (i.e., how the spectral energy is distributed over the course of what was spoken), and temporal envelope modulations (i.e., the changes in the amplitude and frequency of sound perceived by the listener over time) (Bradlow & Smiljanic, 2009). Segmental measurements involve measuring vowel formant changes (i.e., the transitions from the first formant frequency, F1, to the second formant frequency, F2, of a spoken vowel), vowel space (i.e., the area between vowel categories as defined by a two-dimensional area bounded by lines connecting F1 and F2 coordinates of vowels), segment duration, consonant-vowel ratio, voice onset time, short-term spectra (i.e., the spectrum of the speech signal at a particular point in time), sound insertion, and stop consonant burst elimination (Bradlow & Smiljanic, 2008; Berisha, et al, 2013). Based on these various measurements for analysis, researchers have determined certain features that make clear speech clear.\nThe features of clear speech that makes it clear are a wide range of acoustic and articulatory adjustments such as a decrease in speaking rate involving longer segments and longer and more frequent pauses, a wider dynamic pitch range, greater sound-pressure levels, more noticeably clear stop releases, greater root mean square intensity of the non-silent portions of obstruent consonants (i.e., release burst, frication, or aspiration), increased energy in the 1000-3000 Hz range of long-term spectra, and higher-voice intensity. But of all the acoustic features that make up clear speech, the one feature that stands out the most on a consistent basis is “vowel space expansion.”\n\n\n\nAccording to the research literature, it is believed that vowel space expansion, or certain elements of it, may play a role in helping listeners with hearing loss improve their speech intelligibility. Global clear speech acoustic changes such as those mentioned earlier involving speaking more slowly, more loudly, with a higher voice pitch, and with a more variable voice pitch in clear speech are also accompanied by vowel modifications such as an expanded vowel space, greater dynamic formant movement, and longer vowel durations (Ferguson & Quene, 2014). In studies involving having speakers speak clearly in a clear speech condition by speaking as if they were speaking to someone who has difficulty understanding them, it has been found in listeners such as adults with sensorineural hearing loss and who wear cochlear implants that they benefitted from clearer speech for speech identification purposes (Ferguson & Quene, 2014). But as is the case in some other studies, every speaker’s idea of speaking clearly varies from speaker to speaker, especially when adding in multiple speakers.\nThere is not a consensus on how much vowel space expansion and certain characteristics of it improves speech intelligibility in listeners with hearing loss even though the benefit exists. For example, in a study by Ferguson (2012) involving listening to multiple individual speakers produce clear speech, it was found with a better signal-to-noise ratio (SNR) (i.e., -3 dB SNR for elderly hearing-impaired, EHI, listeners vs. -10 dB SNR for young normal hearing, YNH, listeners) that vowel intelligibility improved for EHI listeners. However, having only one speaker produce clear speech in a study by Ferguson and Kewley-Port (2002) provided no benefit for EHI listeners. There are a few reasons for this.\nOne reason is that of the three traditional vowel acoustic cues involving steady-state formant frequencies, dynamic formant movement, and vowel duration when identifying vowels, the relative importance of the cues from the perspective of the listener was different between the EHI and YNH groups (Ferguson & Quene, 2014). This is because the clear speech acoustic changes that benefit YNH listeners may not benefit EHI listeners, or most listeners with hearing loss, in general, since hearing loss alters the way in which acoustic cues are used to identify vowels (Ferguson & Quene, 2014). A second reason is that in comparison to the single speaker in the study by Ferguson and Kewley-Port (2002), 3 out of the 41 speakers from the database in the study by Ferguson (2012) had the same clear speech acoustic cues as the single speaker that only improved the vowel intelligibility of YNH listeners. This shows that it is important to be mindful through continued research and knowledge of which clear speaking style utilizing certain acoustic cues benefits people with hearing loss.\nThe study by Ferguson and Quene (2014) emphasizes that point by discovering that their results confirmed their first hypothesis, which is that the relationship between acoustic characteristics and vowel intelligibility in clear and conversational speech will differ between YNH and EHI listeners. This means that the hypothesis had expected that EHI listeners would depend more heavily on vowel duration acoustic cues than YNH listeners (Ferguson & Quene, 2014). This was not expected to be the case for all listeners with hearing loss, but mostly for older adults with hearing loss due to well-documented temporal processing deficits in older adults including age-related decline in cognitive processing of information that may make increased vowel duration a more helpful clear speech acoustic change for vowel intelligibility (Ferguson & Quene, 2014). The results of the Generalized Linear Mixed Modeling that confirmed the first hypothesis in the study by Ferguson and Quene (2014) showed that vowel duration, along with F1, had a stronger effect than F2 on vowel intelligibility in EHI listeners than for YNH listeners.\nLonger duration was associated with better intelligibility in EHI listeners, mainly, for tense vowels (Ferguson & Quene, 2014). For high vowels, lower F1 values led to improved intelligibility in EHI listeners (Ferguson & Quene, 2014). Because hearing loss would make hearing vowel formants less audible and age was expected to play a factor in causing poor temporal processing and slower cognitive processing, the results for vowel duration improving intelligibility, specifically, for tense vowels confirmed that expectation (Ferguson & Quene, 2014). Mainly, because longer vowel duration helped EHI listeners by allowing more processing time for vowel identification and by increasing the temporal contrast between spectrally similar tense-lax pairs such as /i/-/ɪ/ and /u/-/ʊ/ (Ferguson & Quene, 2014). Moreover, EHI listeners’ intelligibility performance was better when high vowels had lower F1 values (Ferguson & Quene, 2014). With low vowels involving F1 values between 550 to 750 Hz, higher F1 values led to better intelligibility to the same degree for both EHI and YNH listeners (Ferguson & Quene, 2014). Along with F1 information and vowel duration when identifying vowels, EHI listeners did use F2 information, but in a different way than YNH listeners (Ferguson & Quene, 2014). Although it is generally agreed upon in English that F1 and F2 frequencies at steady-state, dynamic movement of these formants, and duration are three acoustic cues that mainly determine the identity of vowels, there were other indications from the analysis by Ferguson and Quene (2014) that other acoustic changes not captured by their measurements also contributed to influencing vowel intelligibility in EHI listeners.\nPossible acoustic changes that may have played a role in influencing vowel intelligibility are voice quality, fundamental frequency, formant bandwidth, or other aspects of the spectral envelope. This calls for continued research on these changes to improve the overall knowledge and understanding of vowel perception and what makes vowels more intelligible in clear speech, in general, and to what extent, if at all, they play a role in improving vowel intelligibility in listeners with hearing loss. While the study by Ferguson and Quene (2014) focus more on older adults with hearing loss, let us look at listeners with hearing loss who are not older adults.\n\n\n\nBerguson et al. (2015) examined vowel characteristics and the clear speech attribute of vowel space expansion in infant-directed (ID) and adult-directed (AD) speech by mothers on listeners with hearing loss who are children to determine which was clearer and beneficial. They found that ID speech demonstrated a more expanded vowel space area and dispersion when used for children with and without hearing loss than with AD speech (Berguson et al, 2015). The reason is that mothers produced more distinctive point vowels in ID speech for children with hearing loss more so than in AD speech (Berguson et al, 2015). This supports the researchers’ idea in terms of their research predictions that mothers produce vowels clearly when speaking to children with and without hearing loss (Berguson et al, 2015). This knowledge and the continued research in this area is important to know to determine the clear speaking style and acoustic cues that may be beneficial in the overall growth and development of a child with hearing loss.\nGathering the evidence found from these studies and others in the research literature by applying their findings to help prevent and treat language delays in children with hearing loss with continued research, would be a good next step forward (Berguson et al, 2015). This is needed because speech-language delays in this population are common, and the speech-language outcomes vary significantly especially for children with hearing loss who use cochlear implants (Berguson et al, 2015). The high variability in the deviations of speech and language outcomes could also be ameliorated by having speech-language pathologists use the findings from this type of research to also close the gaps in the quality of maternal speech input to children with hearing loss (Berguson et al, 2015). While it is important to expand research into this area to aid in improving treatment delivery for language delays in children with hearing loss, it is best to conduct research in the manner of the studies by Berguson et al (2015) by examining relatively large sample sizes of children with actual hearing loss instead of simulated hearing loss to help make the findings more generalizable to the population.\nThe applicability of the studies by Berguson et al (2015) also generated results that partially supports the hypothesis that point vowels in ID speech are produced in more phonologically contrastive articulatory positions than in AD speech, thus, supporting the hyper-articulation hypothesis of speech directed at children with hearing loss. This means that the way that mothers are providing ID speech may be facilitating language acquisition for children with hearing loss through the production of an expanded acoustic vowel space area and increased vowel space dispersion (Berguson et al, 2015). This is beneficial to children with hearing loss because vowel space modification, according to the research literature, has been linked to enhanced speech sound discrimination and word recognition. This finding from Berguson et al (2015), along with what they found as mentioned earlier, indicates potential benefits of applying therapeutic clinical interventions designed towards shaping vowel space characteristics of mothers’ speech towards children with hearing loss. However, more research is still needed in this area because most of the evidence from the research literature points out that the nature of phonetic changes in ID speech is complex considering that there is not enough consistent evidence purporting that such changes would benefit the learning of phonetic categories (Berguson et al, 2015). Berguson et al (2015) also found that for speech directed towards children with cochlear implants that there was an increase in F2 frequencies for /i/ and /a/ vowels relative to speech in groups matched on chronological age for the /a/ vowel and hearing experience for the /i/ vowel. But with hearing aids, speech directed towards children was represented by an increase in only the F1 frequency values for the /i/ vowels relative to the group matched on hearing experience (Berguson et al, 2015). This means that prosodic differences in prosody such as slowed rate and prosodic position could be playing a role between ID and AD speech and that more research assessing the influence of these factors on language acquisition in children with hearing loss, based on differences in the degree of hearing loss and types of assistive devices being used, should be investigated for future research (Berguson et al, 2015). However, the results of all these findings by Berguson et al (2015) would not have been evident without the help of assistive hearing devices such as cochlear implants (CI) and hearing aids (HA) that are designed for listeners with hearing loss to hear speech clearly.\n\n\n\nThe improved performance in speech intelligibility amongst listeners with HA and CI that is evident in the studies, as previously mentioned, very well could not have been possible without these assistive hearing devices that are designed to capture and send the temporal fine structure (TFS) and temporal envelope (ENV) of the speech signal’s temporal and spectral properties to the cochlea along its basilar membrane (Hong & Moon, 2014). Mainly, because the multiple acoustic cues that are used to interpret and understand speech in the human auditory system that have been pointed out, so far, are classified based on their temporal and spectral properties (Hong & Moon, 2014). Hong and Moon (2014) define the ENV as being characterized by the slow variation in the amplitude of the speech signal over time and the TFS as being represented as the rapid oscillations with a rate close to the center frequency of the band. Both pieces of information from the speech signal are represented to the human auditory system by the timing of neural discharges (Hong & Moon, 2014). They are also vital for speech perception in quiet and noisy backgrounds, especially the TFS, because it has been identified as being most important for pitch perception and sound localization based on experiments that conducted “chimeras,” which involves presenting the first sound representing the ENV and then the second sound representing the TFS to determine the relative perceptual importance of the ENV and TFS in different acoustic settings (Hong & Moon, 2014).\nAnother reason the TFS is important for pitch perception is because listeners with cochlear hearing loss, or who use CI, have poor pitch perception because they have trouble separating simultaneous sounds from each other in noisy environments based on their overall pitch qualities (Hong & Moon, 2014). This means that these types of listeners may run into situations where the fundamental frequency, F0, and its perceptual correlate, pitch, of simultaneous sounds may be perceived as being the same, or as a single entity (Hong & Moon, 2014). What separates the TFS from the ENV, performance-wise, from the perspective of the listener, is that, while the ENV is enough to improve speech intelligibility in quiet environments, it is not enough to fully perceive pitches where there is background noise (Hong & Moon, 2014).\nThis is because the ENV, by itself, is not enough to perceptually separate mixtures of sound and, as a result, the TFS is necessary for speech perception in noisy environments, especially fluctuating noise, where there are multiple speakers speaking at once (Hong & Moon, 2014). For example, in a study from the research literature, listeners with normal hearing showed improvement in speech perception with a simulated speech reception threshold of about 15 dB in a noisy environment when more of the TFS information was added (Hong & Moon, 2014). Thus, confirming the significant role that the TFS plays in a listener’s ability to identify speech in a noisy, fluctuating background (Hong & Moon, 2014). With this knowledge, researchers have found strategies of delivering the TFS information to listeners with cochlear hearing loss who wear either a CI or HA\nTo overcome the limitations such as reduced sensitivity to temporal modulation in electric hearing and changes in the repetition rate of the electric waveform above approximately 300 Hz that could not be processed in most CI users, especially considering that the TFS typically oscillates at a much higher rate, CI specialists have employed what is called a HiRes strategy that uses a relatively high envelope cutoff frequency and pulse rate to improve the delivery of the TFS information (Hong & Moon, 2014). They have also implemented a strategy that adds a frequency modulation (FM) signal by transforming the rapidly varying TFS information into a slowly varying FM signal and this has been found to improve sentence recognition performance in CI users by as much as 71% when exposed to babble noise (Hong & Moon, 2014). Now, with HA the TFS information is delivered through non-linear compression, which means that the gain applied to a signal is inversely related with the signal input level by making intense sounds less amplified than weak sounds (Hong & Moon, 2014). Although studies from the research literature suggested that slow compression was better, based on measures of listening comfort, and fast compression was better, based on measures of speech intelligibility, it has been found in HA users with good sensitivity to the TFS information that they may benefit more from fast compressions (Hong & Moon, 2014). This is because the TFS information may be better for listening in the dips of a noisy, fluctuating background and, in the dips, fast compression increases the audibility of signals (Hong & Moon, 2014). However, it has been found that speech intelligibility improved significantly with fast compressions than slow compressions, regardless of conditions, when testing normal-hearing listeners using vocoded signals (Hong & Moon, 2014). This means that the availability of the TFS information does not affect the optimal compression speed and, thus, should be confirmed, if it has not already, by testing listeners with actual cochlear hearing loss (Hong & Moon, 2014). \n\n\n\nOverall, qualities of vowel space expansion that are consistently found in clear speech such as vowel duration have been found to be an important acoustic cue for speakers to emphasize when speaking to older adult listeners with hearing loss. This is especially so when considering the cognitive and temporal processing declines that begin to occur as listeners with hearing loss enter older adulthood. Since there were other clear speech acoustic cues besides vowel duration that may have improved vowel intelligibility that were not measured, more continued research is needed to improve the overall understanding and knowledge of vowel perception. This can help determine exactly those unmeasured acoustic qualities that makes vowels more intelligible in clear speech that may benefit listeners with hearing loss. Further research in this area would also be a great benefit for young listeners with hearing loss.\nUnderstanding the clear speech acoustic cues associated with vowel space expansion that are beneficial to young listeners with hearing loss can allow speech-language pathologists to better guide a mother’s speech towards her child to improve her child’s overall growth and development in speech and language. Finally, better understanding and knowledge gained from continued research in the clear speech acoustic cues such as vowel space expansion that benefits listeners with hearing loss can help CI and HA specialists continue to develop better strategies for delivering the TFS information to assistive hearing devices. This can allow listeners with hearing loss hear speech clearly by looking for those acoustic cues that make clear speech clear to improve their speech intelligibility.\n\n\n\nBradlow, A. R., & Smiljanic, R. (2009). Speaking and hearing clearly: talker and listener factors\nin speaking style changes. Language and Linguistics Compass, 3(1), 236-264,\nhttps://doi.org/10.1111/j.1749-818x.2008.00112x\nBeresha, V., Liss, J., M., Utianski, R., L., Sandoval, S., & Spanias, A. (2013). Automatic\nassessment of vowel space area. Journal of the Acoustical Society of America, 134(5),\nhttps://doi.org/10.1121/1.4826150\nBerguson, T. R., Burnham, E. B., Kondaurova, M., & Wieland, E. A. (2015). Vowel space\ncharacteristics of speech directed to children with and without hearing loss. Journal of\nSpeech, Language, and Hearing Research, 58, 254-267.\nFerguson, S. H., & Quene, H. (2014). Acoustic correlates of vowel intelligibility in clear and\nconversational speech for young normal-hearing and elderly hearing-impaired listeners.\nJournal of the Acoustical Society of America, 135(6).\nHong, S. H., & Moon, I. J. (2014). What is temporal fine structure and why is it important?\nKorean Journal of Audiology, 18(1), 1-7, http://dx.doi.org/10.7874/kja.2014.18.1.1"
  },
  {
    "objectID": "clear_speech.html#clear-speech-considerations-for-the-deaf-and-hard-of-hearing",
    "href": "clear_speech.html#clear-speech-considerations-for-the-deaf-and-hard-of-hearing",
    "title": "Clear Speech",
    "section": "",
    "text": "Grant Powell\nSchool of Behavioral and Brain Sciences, University of Texas at Dallas\nACN 6763 Speech Perception\nProfessor Peter Assmann\nOctober 27, 2022\n\n\nIn everyday verbal communication, the fundamental goal amongst us, humans, is and has always been to find a way to communicate with each other in the clearest way possible. We try to achieve this goal from both the perspective as a listener and speaker. From the perspective of the speaker, we may try to communicate our message through our physical capabilities of how we manipulate the vocal tract or how we apply our knowledge of the overall structure of the language we speak with the utmost confidence that our listener is able to speak and understand the same language. From the perspective of the listener with that same confidence, we may try to communicate with the speaker by listening for certain vowel sounds, consonant sounds, pauses, spacing between words, sentence structure, or changes in pitch to indicate whether a question is being asked. However, no matter what language we speak and how much we improve it to communicate clearly, clear communication, overall, is still fundamentally dependent on our physical faculties of how we utilize our vocal tract and how our auditory system is functioning.\nResearchers have identified this goal of achieving clear communication through how we use our vocal tract and auditory system as “clear speech.” This type of speech is very much necessary in everyday communication settings where there is background noise and conversations involving listeners who are deaf or hard-of-hearing (Bradlow & Smiljanic, 2009). Since the focus of clear speech that is being investigated is based, primarily, on how the English language is used, the modifications that speakers have been observed of making are speaking slowly, speaking loudly, articulating in a more exaggerated manner, speaking with a higher voice pitch, and employing a more variable voice pitch (Bradlow & Smiljanic, 2009; Ferguson & Quene, 2014). The way that researchers have determined to measure clear speech is through global and segmental measurements.\nGlobal measurements involve measuring speaking rate, pause frequency and duration, fundamental frequency average and range, long term spectra (i.e., how the spectral energy is distributed over the course of what was spoken), and temporal envelope modulations (i.e., the changes in the amplitude and frequency of sound perceived by the listener over time) (Bradlow & Smiljanic, 2009). Segmental measurements involve measuring vowel formant changes (i.e., the transitions from the first formant frequency, F1, to the second formant frequency, F2, of a spoken vowel), vowel space (i.e., the area between vowel categories as defined by a two-dimensional area bounded by lines connecting F1 and F2 coordinates of vowels), segment duration, consonant-vowel ratio, voice onset time, short-term spectra (i.e., the spectrum of the speech signal at a particular point in time), sound insertion, and stop consonant burst elimination (Bradlow & Smiljanic, 2008; Berisha, et al, 2013). Based on these various measurements for analysis, researchers have determined certain features that make clear speech clear.\nThe features of clear speech that makes it clear are a wide range of acoustic and articulatory adjustments such as a decrease in speaking rate involving longer segments and longer and more frequent pauses, a wider dynamic pitch range, greater sound-pressure levels, more noticeably clear stop releases, greater root mean square intensity of the non-silent portions of obstruent consonants (i.e., release burst, frication, or aspiration), increased energy in the 1000-3000 Hz range of long-term spectra, and higher-voice intensity. But of all the acoustic features that make up clear speech, the one feature that stands out the most on a consistent basis is “vowel space expansion.”\n\n\n\nAccording to the research literature, it is believed that vowel space expansion, or certain elements of it, may play a role in helping listeners with hearing loss improve their speech intelligibility. Global clear speech acoustic changes such as those mentioned earlier involving speaking more slowly, more loudly, with a higher voice pitch, and with a more variable voice pitch in clear speech are also accompanied by vowel modifications such as an expanded vowel space, greater dynamic formant movement, and longer vowel durations (Ferguson & Quene, 2014). In studies involving having speakers speak clearly in a clear speech condition by speaking as if they were speaking to someone who has difficulty understanding them, it has been found in listeners such as adults with sensorineural hearing loss and who wear cochlear implants that they benefitted from clearer speech for speech identification purposes (Ferguson & Quene, 2014). But as is the case in some other studies, every speaker’s idea of speaking clearly varies from speaker to speaker, especially when adding in multiple speakers.\nThere is not a consensus on how much vowel space expansion and certain characteristics of it improves speech intelligibility in listeners with hearing loss even though the benefit exists. For example, in a study by Ferguson (2012) involving listening to multiple individual speakers produce clear speech, it was found with a better signal-to-noise ratio (SNR) (i.e., -3 dB SNR for elderly hearing-impaired, EHI, listeners vs. -10 dB SNR for young normal hearing, YNH, listeners) that vowel intelligibility improved for EHI listeners. However, having only one speaker produce clear speech in a study by Ferguson and Kewley-Port (2002) provided no benefit for EHI listeners. There are a few reasons for this.\nOne reason is that of the three traditional vowel acoustic cues involving steady-state formant frequencies, dynamic formant movement, and vowel duration when identifying vowels, the relative importance of the cues from the perspective of the listener was different between the EHI and YNH groups (Ferguson & Quene, 2014). This is because the clear speech acoustic changes that benefit YNH listeners may not benefit EHI listeners, or most listeners with hearing loss, in general, since hearing loss alters the way in which acoustic cues are used to identify vowels (Ferguson & Quene, 2014). A second reason is that in comparison to the single speaker in the study by Ferguson and Kewley-Port (2002), 3 out of the 41 speakers from the database in the study by Ferguson (2012) had the same clear speech acoustic cues as the single speaker that only improved the vowel intelligibility of YNH listeners. This shows that it is important to be mindful through continued research and knowledge of which clear speaking style utilizing certain acoustic cues benefits people with hearing loss.\nThe study by Ferguson and Quene (2014) emphasizes that point by discovering that their results confirmed their first hypothesis, which is that the relationship between acoustic characteristics and vowel intelligibility in clear and conversational speech will differ between YNH and EHI listeners. This means that the hypothesis had expected that EHI listeners would depend more heavily on vowel duration acoustic cues than YNH listeners (Ferguson & Quene, 2014). This was not expected to be the case for all listeners with hearing loss, but mostly for older adults with hearing loss due to well-documented temporal processing deficits in older adults including age-related decline in cognitive processing of information that may make increased vowel duration a more helpful clear speech acoustic change for vowel intelligibility (Ferguson & Quene, 2014). The results of the Generalized Linear Mixed Modeling that confirmed the first hypothesis in the study by Ferguson and Quene (2014) showed that vowel duration, along with F1, had a stronger effect than F2 on vowel intelligibility in EHI listeners than for YNH listeners.\nLonger duration was associated with better intelligibility in EHI listeners, mainly, for tense vowels (Ferguson & Quene, 2014). For high vowels, lower F1 values led to improved intelligibility in EHI listeners (Ferguson & Quene, 2014). Because hearing loss would make hearing vowel formants less audible and age was expected to play a factor in causing poor temporal processing and slower cognitive processing, the results for vowel duration improving intelligibility, specifically, for tense vowels confirmed that expectation (Ferguson & Quene, 2014). Mainly, because longer vowel duration helped EHI listeners by allowing more processing time for vowel identification and by increasing the temporal contrast between spectrally similar tense-lax pairs such as /i/-/ɪ/ and /u/-/ʊ/ (Ferguson & Quene, 2014). Moreover, EHI listeners’ intelligibility performance was better when high vowels had lower F1 values (Ferguson & Quene, 2014). With low vowels involving F1 values between 550 to 750 Hz, higher F1 values led to better intelligibility to the same degree for both EHI and YNH listeners (Ferguson & Quene, 2014). Along with F1 information and vowel duration when identifying vowels, EHI listeners did use F2 information, but in a different way than YNH listeners (Ferguson & Quene, 2014). Although it is generally agreed upon in English that F1 and F2 frequencies at steady-state, dynamic movement of these formants, and duration are three acoustic cues that mainly determine the identity of vowels, there were other indications from the analysis by Ferguson and Quene (2014) that other acoustic changes not captured by their measurements also contributed to influencing vowel intelligibility in EHI listeners.\nPossible acoustic changes that may have played a role in influencing vowel intelligibility are voice quality, fundamental frequency, formant bandwidth, or other aspects of the spectral envelope. This calls for continued research on these changes to improve the overall knowledge and understanding of vowel perception and what makes vowels more intelligible in clear speech, in general, and to what extent, if at all, they play a role in improving vowel intelligibility in listeners with hearing loss. While the study by Ferguson and Quene (2014) focus more on older adults with hearing loss, let us look at listeners with hearing loss who are not older adults.\n\n\n\nBerguson et al. (2015) examined vowel characteristics and the clear speech attribute of vowel space expansion in infant-directed (ID) and adult-directed (AD) speech by mothers on listeners with hearing loss who are children to determine which was clearer and beneficial. They found that ID speech demonstrated a more expanded vowel space area and dispersion when used for children with and without hearing loss than with AD speech (Berguson et al, 2015). The reason is that mothers produced more distinctive point vowels in ID speech for children with hearing loss more so than in AD speech (Berguson et al, 2015). This supports the researchers’ idea in terms of their research predictions that mothers produce vowels clearly when speaking to children with and without hearing loss (Berguson et al, 2015). This knowledge and the continued research in this area is important to know to determine the clear speaking style and acoustic cues that may be beneficial in the overall growth and development of a child with hearing loss.\nGathering the evidence found from these studies and others in the research literature by applying their findings to help prevent and treat language delays in children with hearing loss with continued research, would be a good next step forward (Berguson et al, 2015). This is needed because speech-language delays in this population are common, and the speech-language outcomes vary significantly especially for children with hearing loss who use cochlear implants (Berguson et al, 2015). The high variability in the deviations of speech and language outcomes could also be ameliorated by having speech-language pathologists use the findings from this type of research to also close the gaps in the quality of maternal speech input to children with hearing loss (Berguson et al, 2015). While it is important to expand research into this area to aid in improving treatment delivery for language delays in children with hearing loss, it is best to conduct research in the manner of the studies by Berguson et al (2015) by examining relatively large sample sizes of children with actual hearing loss instead of simulated hearing loss to help make the findings more generalizable to the population.\nThe applicability of the studies by Berguson et al (2015) also generated results that partially supports the hypothesis that point vowels in ID speech are produced in more phonologically contrastive articulatory positions than in AD speech, thus, supporting the hyper-articulation hypothesis of speech directed at children with hearing loss. This means that the way that mothers are providing ID speech may be facilitating language acquisition for children with hearing loss through the production of an expanded acoustic vowel space area and increased vowel space dispersion (Berguson et al, 2015). This is beneficial to children with hearing loss because vowel space modification, according to the research literature, has been linked to enhanced speech sound discrimination and word recognition. This finding from Berguson et al (2015), along with what they found as mentioned earlier, indicates potential benefits of applying therapeutic clinical interventions designed towards shaping vowel space characteristics of mothers’ speech towards children with hearing loss. However, more research is still needed in this area because most of the evidence from the research literature points out that the nature of phonetic changes in ID speech is complex considering that there is not enough consistent evidence purporting that such changes would benefit the learning of phonetic categories (Berguson et al, 2015). Berguson et al (2015) also found that for speech directed towards children with cochlear implants that there was an increase in F2 frequencies for /i/ and /a/ vowels relative to speech in groups matched on chronological age for the /a/ vowel and hearing experience for the /i/ vowel. But with hearing aids, speech directed towards children was represented by an increase in only the F1 frequency values for the /i/ vowels relative to the group matched on hearing experience (Berguson et al, 2015). This means that prosodic differences in prosody such as slowed rate and prosodic position could be playing a role between ID and AD speech and that more research assessing the influence of these factors on language acquisition in children with hearing loss, based on differences in the degree of hearing loss and types of assistive devices being used, should be investigated for future research (Berguson et al, 2015). However, the results of all these findings by Berguson et al (2015) would not have been evident without the help of assistive hearing devices such as cochlear implants (CI) and hearing aids (HA) that are designed for listeners with hearing loss to hear speech clearly.\n\n\n\nThe improved performance in speech intelligibility amongst listeners with HA and CI that is evident in the studies, as previously mentioned, very well could not have been possible without these assistive hearing devices that are designed to capture and send the temporal fine structure (TFS) and temporal envelope (ENV) of the speech signal’s temporal and spectral properties to the cochlea along its basilar membrane (Hong & Moon, 2014). Mainly, because the multiple acoustic cues that are used to interpret and understand speech in the human auditory system that have been pointed out, so far, are classified based on their temporal and spectral properties (Hong & Moon, 2014). Hong and Moon (2014) define the ENV as being characterized by the slow variation in the amplitude of the speech signal over time and the TFS as being represented as the rapid oscillations with a rate close to the center frequency of the band. Both pieces of information from the speech signal are represented to the human auditory system by the timing of neural discharges (Hong & Moon, 2014). They are also vital for speech perception in quiet and noisy backgrounds, especially the TFS, because it has been identified as being most important for pitch perception and sound localization based on experiments that conducted “chimeras,” which involves presenting the first sound representing the ENV and then the second sound representing the TFS to determine the relative perceptual importance of the ENV and TFS in different acoustic settings (Hong & Moon, 2014).\nAnother reason the TFS is important for pitch perception is because listeners with cochlear hearing loss, or who use CI, have poor pitch perception because they have trouble separating simultaneous sounds from each other in noisy environments based on their overall pitch qualities (Hong & Moon, 2014). This means that these types of listeners may run into situations where the fundamental frequency, F0, and its perceptual correlate, pitch, of simultaneous sounds may be perceived as being the same, or as a single entity (Hong & Moon, 2014). What separates the TFS from the ENV, performance-wise, from the perspective of the listener, is that, while the ENV is enough to improve speech intelligibility in quiet environments, it is not enough to fully perceive pitches where there is background noise (Hong & Moon, 2014).\nThis is because the ENV, by itself, is not enough to perceptually separate mixtures of sound and, as a result, the TFS is necessary for speech perception in noisy environments, especially fluctuating noise, where there are multiple speakers speaking at once (Hong & Moon, 2014). For example, in a study from the research literature, listeners with normal hearing showed improvement in speech perception with a simulated speech reception threshold of about 15 dB in a noisy environment when more of the TFS information was added (Hong & Moon, 2014). Thus, confirming the significant role that the TFS plays in a listener’s ability to identify speech in a noisy, fluctuating background (Hong & Moon, 2014). With this knowledge, researchers have found strategies of delivering the TFS information to listeners with cochlear hearing loss who wear either a CI or HA\nTo overcome the limitations such as reduced sensitivity to temporal modulation in electric hearing and changes in the repetition rate of the electric waveform above approximately 300 Hz that could not be processed in most CI users, especially considering that the TFS typically oscillates at a much higher rate, CI specialists have employed what is called a HiRes strategy that uses a relatively high envelope cutoff frequency and pulse rate to improve the delivery of the TFS information (Hong & Moon, 2014). They have also implemented a strategy that adds a frequency modulation (FM) signal by transforming the rapidly varying TFS information into a slowly varying FM signal and this has been found to improve sentence recognition performance in CI users by as much as 71% when exposed to babble noise (Hong & Moon, 2014). Now, with HA the TFS information is delivered through non-linear compression, which means that the gain applied to a signal is inversely related with the signal input level by making intense sounds less amplified than weak sounds (Hong & Moon, 2014). Although studies from the research literature suggested that slow compression was better, based on measures of listening comfort, and fast compression was better, based on measures of speech intelligibility, it has been found in HA users with good sensitivity to the TFS information that they may benefit more from fast compressions (Hong & Moon, 2014). This is because the TFS information may be better for listening in the dips of a noisy, fluctuating background and, in the dips, fast compression increases the audibility of signals (Hong & Moon, 2014). However, it has been found that speech intelligibility improved significantly with fast compressions than slow compressions, regardless of conditions, when testing normal-hearing listeners using vocoded signals (Hong & Moon, 2014). This means that the availability of the TFS information does not affect the optimal compression speed and, thus, should be confirmed, if it has not already, by testing listeners with actual cochlear hearing loss (Hong & Moon, 2014). \n\n\n\nOverall, qualities of vowel space expansion that are consistently found in clear speech such as vowel duration have been found to be an important acoustic cue for speakers to emphasize when speaking to older adult listeners with hearing loss. This is especially so when considering the cognitive and temporal processing declines that begin to occur as listeners with hearing loss enter older adulthood. Since there were other clear speech acoustic cues besides vowel duration that may have improved vowel intelligibility that were not measured, more continued research is needed to improve the overall understanding and knowledge of vowel perception. This can help determine exactly those unmeasured acoustic qualities that makes vowels more intelligible in clear speech that may benefit listeners with hearing loss. Further research in this area would also be a great benefit for young listeners with hearing loss.\nUnderstanding the clear speech acoustic cues associated with vowel space expansion that are beneficial to young listeners with hearing loss can allow speech-language pathologists to better guide a mother’s speech towards her child to improve her child’s overall growth and development in speech and language. Finally, better understanding and knowledge gained from continued research in the clear speech acoustic cues such as vowel space expansion that benefits listeners with hearing loss can help CI and HA specialists continue to develop better strategies for delivering the TFS information to assistive hearing devices. This can allow listeners with hearing loss hear speech clearly by looking for those acoustic cues that make clear speech clear to improve their speech intelligibility.\n\n\n\nBradlow, A. R., & Smiljanic, R. (2009). Speaking and hearing clearly: talker and listener factors\nin speaking style changes. Language and Linguistics Compass, 3(1), 236-264,\nhttps://doi.org/10.1111/j.1749-818x.2008.00112x\nBeresha, V., Liss, J., M., Utianski, R., L., Sandoval, S., & Spanias, A. (2013). Automatic\nassessment of vowel space area. Journal of the Acoustical Society of America, 134(5),\nhttps://doi.org/10.1121/1.4826150\nBerguson, T. R., Burnham, E. B., Kondaurova, M., & Wieland, E. A. (2015). Vowel space\ncharacteristics of speech directed to children with and without hearing loss. Journal of\nSpeech, Language, and Hearing Research, 58, 254-267.\nFerguson, S. H., & Quene, H. (2014). Acoustic correlates of vowel intelligibility in clear and\nconversational speech for young normal-hearing and elderly hearing-impaired listeners.\nJournal of the Acoustical Society of America, 135(6).\nHong, S. H., & Moon, I. J. (2014). What is temporal fine structure and why is it important?\nKorean Journal of Audiology, 18(1), 1-7, http://dx.doi.org/10.7874/kja.2014.18.1.1"
  },
  {
    "objectID": "posts/2024-28-4 Reflection: \"Thoughts on the Perception of the Human Eye\"/index copy.html",
    "href": "posts/2024-28-4 Reflection: \"Thoughts on the Perception of the Human Eye\"/index copy.html",
    "title": "Reflection No. 1: Thoughts on the Perception of the Human Eye",
    "section": "",
    "text": "The most striking feature of the human eye and its perception are the Gestalt principles involving grouping. The reason this stands out is because, when I was in high school taking a printmaking course, I found M.C. Escher’s works of art to be incredible.I realized, from his artwork, the seemingly limitless potential that the brain and eyes hadin working together to be able to perceive, as both the perceiver and creator, ways to seamlessly sequence groupings such as birds into fishes to manipulate both our personal and general perspective of what is called the figure and ground. As the birds come forward into the foreground as the figure, the fishes dissipate into the background as the ground and vice versa. In doing this, Escher would also sometimes utilize some concepts of ambiguity where you arelooking at his artwork one way, but from another perspective you are able to look at it from an alternative viewpoint. In this manner, he is playing with the viewers’ visual acuity.\nHe is manipulating the spatial frequency of both our vision from the viewer’s viewpoint and the artwork from the creator’s viewpoint through his vision. Higher spatial frequency utilizessharp features and fine details to bring certain objects to the forefront, the figure, and less detailsand blurry features to push certain objects into the background, the ground. Higher and lowerspatial frequency is achieved by increasing the number of cycles of gratings per unit of what iscalled a sine wave, or sinusoidal grating, through the number of times a pattern of, say, linesrepeat per unit area or where the bars of light and dark lines gradually change their intensitywithin that area (R. Blake & R. Sekular, 2001). The number of lines repeating per unit area cangive us either a higher or lower spatial frequency depending on the angle and distance we are viewing from (R. Blake & R. Sekular, 2001).\nReference\nR. Blake & R, Sekular (2001). Tutorial on spatial frequency analysis. Perception, 3rd Ed., http://www.psy.vanderbilt.edu/courses/hon185/SpatialFrequency/SpatialFrequency.html"
  },
  {
    "objectID": "posts/2024-28-4 Reflection: \"Thoughts on the Perception of the Human Eye\"/index.html",
    "href": "posts/2024-28-4 Reflection: \"Thoughts on the Perception of the Human Eye\"/index.html",
    "title": "Reflection No. 1: Thoughts on the Perception of the Human Eye",
    "section": "",
    "text": "The most striking feature of the human eye and its perception are the Gestalt principles involving grouping. The reason this stands out is because, when I was in high school taking a printmaking course, I found M.C. Escher’s works of art to be incredible.I realized, from his artwork, the seemingly limitless potential that the brain and eyes hadin working together to be able to perceive, as both the perceiver and creator, ways to seamlessly sequence groupings such as birds into fishes to manipulate both our personal and general perspective of what is called the figure and ground. As the birds come forward into the foreground as the figure, the fishes dissipate into the background as the ground and vice versa. In doing this, Escher would also sometimes utilize some concepts of ambiguity where you arelooking at his artwork one way, but from another perspective you are able to look at it from an alternative viewpoint. In this manner, he is playing with the viewers’ visual acuity.\nHe is manipulating the spatial frequency of both our vision from the viewer’s viewpoint and the artwork from the creator’s viewpoint through his vision. Higher spatial frequency utilizessharp features and fine details to bring certain objects to the forefront, the figure, and less detailsand blurry features to push certain objects into the background, the ground. Higher and lowerspatial frequency is achieved by increasing the number of cycles of gratings per unit of what iscalled a sine wave, or sinusoidal grating, through the number of times a pattern of, say, linesrepeat per unit area or where the bars of light and dark lines gradually change their intensitywithin that area (R. Blake & R. Sekular, 2001). The number of lines repeating per unit area cangive us either a higher or lower spatial frequency depending on the angle and distance we are viewing from (R. Blake & R. Sekular, 2001).\nReference\nR. Blake & R, Sekular (2001). Tutorial on spatial frequency analysis. Perception, 3rd Ed., http://www.psy.vanderbilt.edu/courses/hon185/SpatialFrequency/SpatialFrequency.html"
  },
  {
    "objectID": "posts/2024-28-4 Reflection: Color Perception and Survival Needs/index.html",
    "href": "posts/2024-28-4 Reflection: Color Perception and Survival Needs/index.html",
    "title": "Reflection No. 2: The Use of Color Perception for Survival Needs in Modern-day Society",
    "section": "",
    "text": "Color perception has evolved in human beings. Color perception has changed as our survival needs have changed. Instead of using color for determining if a dangerous animal is camouflaging itself to lure us into a trap or if food looks rotten, our color perception has evolved to help keep us safe on the roads when driving a car. For example, when I drive, I need to see the color of the cars driving by in my blind spot after I look in my side mirror and take that quick glance to either my left or right depending on which side mirror I am using.\nBecause I use my peripheral vision in this instance, the color of a moving object appearing will catch my attention in comparison to the blurriness of the backdrop. When it does, then I can focus on that object quickly and realize it is a car and remind myself not to change lanes, yet. Otherwise, I will get into a wreck and either, accidentally, hurt or kill myself. Now, this becomes challenging when checking for gray, silver, dark blue, and black cars in the evening and at night.\nIn the evening and at night, the problem of univariance occurs where the lack of lightmakes it difficult to utilize my color perception, effectively. This is where my rods are active andmy cones, used to detect color when exposed to light, are inactive. Since I and everybody elseonly have one type of rod, the only color we see is mostly blackness even though there is still alittle bit of light to help us make out shapes and objects, hence, street, city, and car lights and thenight sky being helpful guides of light in the dark. If we had a second set of rods, then maybe wecould see something else besides blackness with the little light received at night. This is also whywe, typically, have three cones and not one. There is just not enough light to activate those cones for color perception and oversaturate the rods to make the rods inactive."
  },
  {
    "objectID": "posts/2024-28-4 Reflection: Music Perception/index.html",
    "href": "posts/2024-28-4 Reflection: Music Perception/index.html",
    "title": "Reflection No. 4: Why Our Brains Might Light Up Like a Christmas Tree on an fMRI?",
    "section": "",
    "text": "The aspects of human behavior that plays the greatest role in causing our brains to light up like a Christmas tree through a brain imaging scan such as an fMRI are emotions and memories. Yes, the motivation to move through singing and dancing will cause areas of our brain associated with motor control to light up, but not without the motivating factors of emotion and memory.\nWhen sitting in our cars singing along to a song, we are motivated to sing by how we feel about the song we are listening to. We could be singing either because it just makes us happy, or it stirs up pleasant memories that elicits a pleasurable emotion that motivates us to want to sing or dance. This is why it is very important to use songs or music with older adults with dementia that come from their childhood or young adult years because those types of songs they associate with the most can tap into their long-term memory store and elicit an emotional response to an old memory. So much so, that it may be pleasant enough for them to want to speak again, or more than they usually have been, through either singing or conversation.\nFor example, my grandmother had Alzheimer’s Disease back when I was studying music therapy and she was staring off into space and not speaking, but when I sang “Amazing Grace” one time her affect lit up and she started crying. Then she was talking coherently for the first time in a long time about the song and, while the conversation was short, it was amazing. Music has this ability to tap into our emotions and memories that other activities are unable to. This is also why it is important to use a person’s preferred music they like or have a personal or cultural association with to elicit a specific response from them that benefits their overall well-being from a health and wellness perspective. This is especially true if you are wanting to induce meaningful behavioral modification via mood through the “iso-principle.”"
  },
  {
    "objectID": "posts/2024-28-4 Reflection: Pleasant Touch/index.html",
    "href": "posts/2024-28-4 Reflection: Pleasant Touch/index.html",
    "title": "Reflection No. 5: What Purpose Does Pleasant Touch Play?",
    "section": "",
    "text": "One purpose that “pleasant touch” may serve in humans and animals is developing a sense of trust as it pertains to our survival needs. For example, I have a Boston Terrier and she tends to stay very close to my wife and I when we take her with us to see family members on trips because she is not sure how others, especially little kids who might be a little rough touching her, will be when physically interacting with her. She stays close to us because she knows we will gently care for her based on how we touch her. I think the same can be said for most people when they were little.\nWhen I was a child, the person I felt the most comfortable having touch me was my mother because she was very nurturing, and I knew my overall daily living needs would be met. For example, I remember liking it when she combed my hair when she helped me get ready for school. In fact, it feels great when the hair stylist gives me a haircut, too, and that helps build trust with them. Otherwise, I would not want them to cut my hair. Because there are C-tactile afferent nerve fibers, a type of C-fiber commonly associated with generating the response signals that create a pleasurable sensation, wrapped around the roots of our hair fibers, the preceding examples may explain why when hair is touched in people in a certain way that it can sometimes generate a pleasurable sensation. Yet, I think this also explains how “pleasant touch” has evolved.\nGoing to the hair stylist or having my wife stroke my hair and massage my head feels great and is a fantastic way for men and women to deal with stress. Even my wife loves it when I stroke her hair. “Pleasant touch” seems to be a necessary de-stressor in an age where we still have more responsibilities and stress to deal with despite the proliferation of technological devices designed to lessen the daily load life throws at us."
  },
  {
    "objectID": "posts/2024-28-4 Reflection: 3 Optical Illusions/index.html",
    "href": "posts/2024-28-4 Reflection: 3 Optical Illusions/index.html",
    "title": "Reflection No. 3: Three Optical Illusions That Stand Out to Me",
    "section": "",
    "text": "The three optical illusions that I have experienced in real-life that have stood out to me are the ‘wagon wheel,’ ‘apparent motion,’ and ‘inattentional blindness’ illusions. The ‘wagonwheel’ illusion has intrigued me the most ever since it captured my attention as a little boy.\nI thought it was cool to look at the wheels moving on another car passing by while sitting in the backseat as my parents drove me around places. I would play a game to deal with my boredom to see if I could try to see the wheels moving forward 100% of the time by blocking out the illusion of them moving backwards. To no avail, my eyes would not cooperate because, no matter how much my natural saccadic eye movements tried to keep up with the fast motion of the wheels, my brain would continue filling in the gaps it couldn’t keep up with to allow me to not be able to completely avoid seeing this illusion. Now, the second illusion that stood out to me when I enjoyed watching Disney shows as a child was ‘apparent motion.’\nI remember being so excited to receive a small pocket-sized book either from McDonald’s happy meals or boxes of breakfast cereal. I would flip through it and watch as a character moved in artificial, real-time motion. It blew my mind away. I don’t know how many times I spent flipping through those books, but it was countless. It was just neat to watch “the rapid alternation of objects on the page moving in different locations in rapid succession.” Finally, the last illusion ‘inattentional blindness’ recently happened when I was driving.\nWhile driving through a neighborhood, I saw in the distance the stoplight as a sign that I was finally about to get out of a specific neighborhood I was lost in. Well, lo and behold, a stop sign appeared out of nowhere as I got closer and realized what I missed, and I slammed on my brakes the moment I saw it."
  },
  {
    "objectID": "posts/2024-28-4 Reflection Favorite Perception Topic/index.html",
    "href": "posts/2024-28-4 Reflection Favorite Perception Topic/index.html",
    "title": "Reflection No. 6: My Favorite Perception Topic",
    "section": "",
    "text": "The most striking topic I learned in Perception is how the brain maps itself as it relates to tonotopic, retinotopic and somatotopic mapping. Of those forms of mapping, somatotopic mapping to me stands out the most.\nI have always had a fascination with touch physiology when it comes to the subject of perception because, as a musician and board certified-music therapist, I not only play instruments, but I have severe to profound hearing loss, binaurally. Wearing hearing aids every day helps me hear some of the high frequency speech and music sounds I am missing from when I was a premature infant, but I think some aspects of touch plays a role in working in tandem with the assistance that my hearing devices provide.\nFor example, when I play guitar my hearing devices, along with the residual hearing I still have, plays a role in letting me know whether the tone quality of a string I am plucking or pressing down on the fingerboard will sound musically palatable or buzz, but my touch sensitivity in my fingers when plucking and pressing down the strings on the fingerboard also plays a role in providing the necessary feedback. I do not necessarily have to hear whether the string I am plucking or pressing down is going to sound good or buzz. I can already tell sometimes based on how I am plucking the string or pressing it down through my finger’s touch sensitivity whether the notes are going to sound good or buzz.\nIn the area of the homunculus of the somatotopic map that is responsible for receiving input on the sensation felt in my fingers, that there might be a set of neurons that have been strengthened from years of playing guitar since I was eleven that can relay to my brain whether the sensation in my fingers is just right enough to inform me whether the sound I am making will have a nice tone."
  }
]